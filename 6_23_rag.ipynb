{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 처리\n",
    "! pip install -U docling\n",
    "\n",
    "! pip install qwen-vl-utils\n",
    "! pip install accelerate\n",
    "! pip install packaging ninja\n",
    "! pip install flash-attn==2.6.3 --no-build-isolation\n",
    "! pip install \"transformers==4.51.3\"\n",
    "\n",
    "# 나머지는 동일\n",
    "! pip install nvidia-ml-py3\n",
    "! pip install -q FlagEmbedding\n",
    "! pip install selenium webdriver-manager requests tqdm beautifulsoup4 lxml\n",
    "! pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 현재 설치된 버전 확인 ===\n",
      "PyTorch 버전: 2.4.1+cu124\n",
      "Transformers 버전: 4.51.3\n",
      "CUDA 사용 가능: True\n",
      "CUDA 버전: 12.4\n",
      "사용 가능한 GPU 개수: 1\n",
      "Accelerate 버전: 1.8.1\n",
      "Flash Attention 버전: 2.6.3\n"
     ]
    }
   ],
   "source": [
    "# 필수 import들\n",
    "import torch\n",
    "import transformers\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from contextlib import contextmanager\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor,AutoModelForCausalLM\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import huggingface_hub\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pynvml\n",
    "\n",
    "# 버전 확인\n",
    "print(\"=== 현재 설치된 버전 확인 ===\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"Transformers 버전: {transformers.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "    print(f\"사용 가능한 GPU 개수: {torch.cuda.device_count()}\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"Accelerate 버전: {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Accelerate 패키지가 설치되지 않음\")\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"Flash Attention 버전: {flash_attn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Flash Attention 패키지가 설치되지 않음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iwuNlhfCWVe"
   },
   "source": [
    "# 1. Crwaling Data\n",
    "## URL, PDF_URL, 출판일\n",
    "## PDF_URL 다운로드 받아 ex_pdf_file에 저장.\n",
    "## pdf 파일명: 년도_제목.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# 시스템 업데이트\n",
    "apt update\n",
    "\n",
    "# Chrome 브라우저 설치\n",
    "wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add -\n",
    "echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list\n",
    "apt update\n",
    "apt install -y google-chrome-stable\n",
    "apt install -y wget curl unzip xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# Chrome 설치 확인\n",
    "google-chrome --version\n",
    "\n",
    "# Python 패키지 확인\n",
    "python -c \"import selenium; print('Selenium 설치됨')\"\n",
    "python -c \"import webdriver_manager; print('WebDriver Manager 설치됨')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HzdfE_s788NZ",
    "outputId": "83aa5c1d-58ba-49a8-9c60-21447e414daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:22:32,271 - INFO - 저장 디렉토리 생성: ./data/ex_pdf\n",
      "2025-06-23 06:22:32,271 - INFO - ====== WebDriver manager ======\n",
      "2025-06-23 06:22:32,329 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:32,389 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:32,452 - INFO - There is no [linux64] chromedriver \"137.0.7151.119\" for browser google-chrome \"137.0.7151\" in cache\n",
      "2025-06-23 06:22:32,453 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:32,613 - INFO - WebDriver version 137.0.7151.119 selected\n",
      "2025-06-23 06:22:32,615 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/137.0.7151.119/linux64/chromedriver-linux64.zip\n",
      "2025-06-23 06:22:32,615 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/137.0.7151.119/linux64/chromedriver-linux64.zip\n",
      "2025-06-23 06:22:32,700 - INFO - Driver downloading response is 200\n",
      "2025-06-23 06:22:32,887 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:33,034 - INFO - Driver has been saved in cache [/root/.wdm/drivers/chromedriver/linux64/137.0.7151.119]\n",
      "2025-06-23 06:22:33,370 - INFO - Chrome 드라이버 설정 완료\n",
      "2025-06-23 06:22:33,370 - INFO - \n",
      "==================================================\n",
      "2025-06-23 06:22:33,371 - INFO - 페이지 1 처리 시작\n",
      "2025-06-23 06:22:33,371 - INFO - ==================================================\n",
      "2025-06-23 06:22:33,371 - INFO - 페이지 1 뉴스 리스트 가져오는 중: https://www.bok.or.kr/portal/singl/newsData/list.do?pageIndex=1&targetDepth=3&menuNo=200081&syncMenuChekKey=1&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&date=&sdate=&edate=&sort=1&pageUnit=10\n",
      "2025-06-23 06:22:44,816 - INFO - 총 10개의 뉴스 항목 발견\n",
      "2025-06-23 06:22:44,881 - INFO -   1. [현지정보] 2025_6월 FOMC 시장반응... (등록일\n",
      "2025.06.19)\n",
      "2025-06-23 06:22:44,915 - INFO -   2. [현지정보] 뉴욕사무소 이전 기념행사 개최... (등록일\n",
      "2025.06.17)\n",
      "2025-06-23 06:22:44,971 - INFO -   3. [현지정보] 美 2025.5월 소비자물가 동향 및 금융시장 반응... (등록일\n",
      "2025.06.12)\n",
      "2025-06-23 06:22:45,026 - INFO -   4. [현지정보] 美 2025.5월 고용지표 내용 및 뉴욕 금융시장 반응... (등록일\n",
      "2025.06.09)\n",
      "2025-06-23 06:22:45,082 - INFO -   5. [현지정보] 25년 6월 캐나다 중앙은행 정책회의 결과 및 시장 반응... (등록일\n",
      "2025.06.04)\n",
      "2025-06-23 06:22:45,143 - INFO -   6. [현지정보] 미국 신용등급 하향 조정에 대한 시장참가자 평가... (등록일\n",
      "2025.05.20)\n",
      "2025-06-23 06:22:45,206 - INFO -   7. [현지정보] Moody’s社, 미국 신용등급 하향조정... (등록일\n",
      "2025.05.19)\n",
      "2025-06-23 06:22:45,261 - INFO -   8. [현지정보] 美 2025.4월 소비자물가 동향 및 금융시장 반응... (등록일\n",
      "2025.05.14)\n",
      "2025-06-23 06:22:45,309 - INFO -   9. [현지정보] 2025년 5월 FOMC 시장반응... (등록일\n",
      "2025.05.08)\n",
      "2025-06-23 06:22:45,339 - INFO -   10. [현지정보] 최근(2025.4월)의 미국경제 상황과 평가... (등록일\n",
      "2025.05.07)\n",
      "페이지 1 처리:   0%|          | 0/10 [00:00<?, ?it/s]2025-06-23 06:22:45,347 - INFO - \n",
      "[1/10] 처리 중: [현지정보] 2025_6월 FOMC 시장반응...\n",
      "2025-06-23 06:22:45,348 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091991&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:22:52,637 - INFO -   PDF 파일 발견: (현지정보 250618) 2025_6월 FOMC 시장반응_f.pdf\n",
      "2025-06-23 06:22:52,654 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:22:52,655 - INFO - PDF 다운로드 중: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf\n",
      "2025-06-23 06:22:59,248 - INFO - PDF 다운로드 완료: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf (307,388 bytes)\n",
      "페이지 1 처리:  10%|█         | 1/10 [00:14<02:14, 14.90s/it]2025-06-23 06:23:00,252 - INFO - \n",
      "[2/10] 처리 중: [현지정보] 뉴욕사무소 이전 기념행사 개최...\n",
      "2025-06-23 06:23:00,253 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091946&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:04,939 - INFO -   PDF가 아닌 파일은 건너뜀: [현지정보] 뉴욕사무소 이전 기념행사 개최.hwp (hwp)\n",
      "2025-06-23 06:23:04,961 - INFO - PDF 파일을 찾을 수 없음\n",
      "2025-06-23 06:23:04,962 - INFO - 이 뉴스에는 PDF 파일이 없음\n",
      "페이지 1 처리:  20%|██        | 2/10 [00:19<01:11,  8.91s/it]2025-06-23 06:23:04,963 - INFO - \n",
      "[3/10] 처리 중: [현지정보] 美 2025.5월 소비자물가 동향 및 금융시장 반응...\n",
      "2025-06-23 06:23:04,964 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091882&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:10,267 - INFO -   PDF 파일 발견: [현지정보] 美 2025.5월 소비자물가 동향 및 금융시장 반응_F.pdf\n",
      "2025-06-23 06:23:10,294 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:23:10,294 - INFO - PDF 다운로드 중: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf\n",
      "2025-06-23 06:23:17,215 - INFO - PDF 다운로드 완료: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf (325,603 bytes)\n",
      "페이지 1 처리:  30%|███       | 3/10 [00:32<01:16, 10.89s/it]2025-06-23 06:23:18,218 - INFO - \n",
      "[4/10] 처리 중: [현지정보] 美 2025.5월 고용지표 내용 및 뉴욕 금융시장 반응...\n",
      "2025-06-23 06:23:18,220 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091787&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:23,505 - INFO -   PDF 파일 발견: (현지정보) 美 2025.5월 고용지표 내용 및 뉴욕 금융시장 반응_f.pdf\n",
      "2025-06-23 06:23:23,535 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:23:23,536 - INFO - PDF 다운로드 중: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf\n",
      "2025-06-23 06:23:27,954 - INFO - PDF 다운로드 완료: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf (268,806 bytes)\n",
      "페이지 1 처리:  40%|████      | 4/10 [00:43<01:04, 10.83s/it]2025-06-23 06:23:28,957 - INFO - \n",
      "[5/10] 처리 중: [현지정보] 25년 6월 캐나다 중앙은행 정책회의 결과 및 시장 반응...\n",
      "2025-06-23 06:23:28,958 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091760&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:34,235 - INFO -   PDF 파일 발견: [현지정보] 25년 6월 캐나다 중앙은행 정책회의 결과 및 시장 반응.pdf\n",
      "2025-06-23 06:23:34,264 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:23:34,265 - INFO - PDF 다운로드 중: 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf\n",
      "2025-06-23 06:23:40,863 - INFO - PDF 다운로드 완료: 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf (471,063 bytes)\n",
      "페이지 1 처리:  50%|█████     | 5/10 [00:56<00:57, 11.58s/it]2025-06-23 06:23:41,867 - INFO - \n",
      "[6/10] 처리 중: [현지정보] 미국 신용등급 하향 조정에 대한 시장참가자 평가...\n",
      "2025-06-23 06:23:41,868 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091484&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:47,883 - INFO -   PDF 파일 발견: [현지정보] 미국 신용등급 하향 조정에 대한 시장참가자 평가.pdf\n",
      "2025-06-23 06:23:47,906 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:23:47,907 - INFO - PDF 다운로드 중: 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf\n",
      "2025-06-23 06:23:54,650 - INFO - PDF 다운로드 완료: 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf (304,873 bytes)\n",
      "페이지 1 처리:  60%|██████    | 6/10 [01:10<00:49, 12.33s/it]2025-06-23 06:23:55,651 - INFO - \n",
      "[7/10] 처리 중: [현지정보] Moody’s社, 미국 신용등급 하향조정...\n",
      "2025-06-23 06:23:55,651 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091459&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:00,334 - INFO -   PDF 파일 발견: (현지정보 20250516) Moody’s社, 미국 신용등급 하향조정_f.pdf\n",
      "2025-06-23 06:24:00,359 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:24:00,360 - INFO - PDF 다운로드 중: 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf\n",
      "2025-06-23 06:24:04,424 - INFO - PDF 다운로드 완료: 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf (104,921 bytes)\n",
      "페이지 1 처리:  70%|███████   | 7/10 [01:20<00:34, 11.50s/it]2025-06-23 06:24:05,428 - INFO - \n",
      "[8/10] 처리 중: [현지정보] 美 2025.4월 소비자물가 동향 및 금융시장 반응...\n",
      "2025-06-23 06:24:05,429 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091387&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:11,294 - INFO -   PDF 파일 발견: [현지정보] 美 2025.4월 소비자물가 동향 및 금융시장 반응.pdf\n",
      "2025-06-23 06:24:11,323 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:24:11,324 - INFO - PDF 다운로드 중: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf\n",
      "2025-06-23 06:24:16,758 - INFO - PDF 다운로드 완료: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf (319,471 bytes)\n",
      "페이지 1 처리:  80%|████████  | 8/10 [01:32<00:23, 11.76s/it]2025-06-23 06:24:17,763 - INFO - \n",
      "[9/10] 처리 중: [현지정보] 2025년 5월 FOMC 시장반응...\n",
      "2025-06-23 06:24:17,764 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091313&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:22,503 - INFO -   PDF 파일 발견: (현지정보 250507) 2025_5월 FOMC 시장반응_f.pdf\n",
      "2025-06-23 06:24:22,531 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:24:22,532 - INFO - PDF 다운로드 중: 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf\n",
      "2025-06-23 06:24:27,004 - INFO - PDF 다운로드 완료: 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf (305,178 bytes)\n",
      "페이지 1 처리:  90%|█████████ | 9/10 [01:42<00:11, 11.29s/it]2025-06-23 06:24:28,007 - INFO - \n",
      "[10/10] 처리 중: [현지정보] 최근(2025.4월)의 미국경제 상황과 평가...\n",
      "2025-06-23 06:24:28,008 - INFO - PDF 링크 추출 중: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091256&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:33,006 - INFO -   PDF 파일 발견: 최근(2025.4월)의 미국경제 상황과 평가.pdf\n",
      "2025-06-23 06:24:33,014 - INFO - 총 1개의 PDF 파일 발견\n",
      "2025-06-23 06:24:33,014 - INFO - PDF 다운로드 중: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf\n",
      "2025-06-23 06:24:37,754 - INFO - PDF 다운로드 완료: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf (1,006,696 bytes)\n",
      "페이지 1 처리: 100%|██████████| 10/10 [01:53<00:00, 11.34s/it]\n",
      "2025-06-23 06:24:38,756 - INFO - \n",
      "============================================================\n",
      "2025-06-23 06:24:38,756 - INFO - PDF 다운로드 결과 요약\n",
      "2025-06-23 06:24:38,756 - INFO - ============================================================\n",
      "2025-06-23 06:24:38,756 - INFO - 총 뉴스 항목: 10\n",
      "2025-06-23 06:24:38,757 - INFO - 총 PDF 파일: 9\n",
      "2025-06-23 06:24:38,757 - INFO - 성공한 다운로드: 9\n",
      "2025-06-23 06:24:38,757 - INFO - 실패한 다운로드: 0\n",
      "2025-06-23 06:24:38,757 - INFO - 저장된 PDF 파일 수: 9\n",
      "2025-06-23 06:24:38,757 - INFO - \n",
      "저장된 PDF 파일 목록:\n",
      "2025-06-23 06:24:38,757 - INFO -   - 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf (307,388 bytes)\n",
      "2025-06-23 06:24:38,757 - INFO -   - 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf (325,603 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf (268,806 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf (471,063 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf (304,873 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf (104,921 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf (319,471 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf (305,178 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf (1,006,696 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO - \n",
      "PDF가 없는 뉴스 항목: 1개\n",
      "2025-06-23 06:24:38,822 - INFO - 드라이버 종료 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class BOKNewsScraper:\n",
    "    def __init__(self, save_dir='./data/ex_pdf_file', headless=True):\n",
    "        self.BASE_URL = 'https://www.bok.or.kr/portal/singl/newsData/list.do?pageIndex={page}&targetDepth=3&menuNo=200081&syncMenuChekKey=1&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&date=&sdate=&edate=&sort=1&pageUnit=10'\n",
    "        self.DETAIL_BASE = 'https://www.bok.or.kr'\n",
    "        self.save_dir = save_dir\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "        \n",
    "        # 저장 디렉토리 생성\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        logger.info(f\"저장 디렉토리 생성: {self.save_dir}\")\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"RunPod 환경에 최적화된 Chrome 드라이버 설정\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            \n",
    "            # RunPod 환경을 위한 Chrome 옵션들\n",
    "            if self.headless:\n",
    "                options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--disable-web-security\")\n",
    "            options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "            options.add_argument(\"--window-size=1920,1080\")\n",
    "            options.add_argument(\"--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "            \n",
    "            # ChromeDriverManager를 사용해 자동으로 드라이버 관리\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            # 페이지 로드 타임아웃 설정\n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.implicitly_wait(10)\n",
    "            \n",
    "            logger.info(\"Chrome 드라이버 설정 완료\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"드라이버 설정 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_news_list(self, page=1, max_retries=3):\n",
    "        \"\"\"뉴스 리스트 가져오기\"\"\"\n",
    "        url = self.BASE_URL.format(page=page)\n",
    "        logger.info(f\"페이지 {page} 뉴스 리스트 가져오는 중: {url}\")\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                # 페이지 로딩 대기\n",
    "                WebDriverWait(self.driver, 15).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.bd-line\"))\n",
    "                )\n",
    "                \n",
    "                # 추가 대기 시간\n",
    "                time.sleep(2)\n",
    "                \n",
    "                bd_line_div = self.driver.find_element(By.CSS_SELECTOR, \"div.bd-line\")\n",
    "                news_items = bd_line_div.find_elements(By.CSS_SELECTOR, \"li.bbsRowCls\")\n",
    "                \n",
    "                if not news_items:\n",
    "                    logger.warning(f\"페이지 {page}에서 뉴스 항목을 찾을 수 없음\")\n",
    "                    return []\n",
    "                \n",
    "                results = []\n",
    "                logger.info(f\"총 {len(news_items)}개의 뉴스 항목 발견\")\n",
    "                \n",
    "                for idx, item in enumerate(news_items):\n",
    "                    try:\n",
    "                        # 링크 추출\n",
    "                        set_div = item.find_element(By.CSS_SELECTOR, \"div.set\")\n",
    "                        a_tag = set_div.find_element(By.TAG_NAME, \"a\")\n",
    "                        href = a_tag.get_attribute('href')\n",
    "                        detail_url = urljoin(self.DETAIL_BASE, href)\n",
    "                        \n",
    "                        # 제목 추출\n",
    "                        title_text = a_tag.text.strip()\n",
    "                        \n",
    "                        # 날짜 추출\n",
    "                        try:\n",
    "                            date_span = item.find_element(By.CSS_SELECTOR, \"span.date\")\n",
    "                            date_text = date_span.text.strip()\n",
    "                        except NoSuchElementException:\n",
    "                            date_text = \"날짜없음\"\n",
    "                        \n",
    "                        results.append({\n",
    "                            'title': title_text,\n",
    "                            'detail_url': detail_url,\n",
    "                            'date': date_text,\n",
    "                        })\n",
    "                        \n",
    "                        logger.info(f\"  {idx + 1}. {title_text[:50]}... ({date_text})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"뉴스 항목 {idx + 1} 처리 실패: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                return results\n",
    "                \n",
    "            except TimeoutException:\n",
    "                logger.warning(f\"페이지 로딩 타임아웃 (시도 {attempt + 1}/{max_retries})\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.error(\"페이지 로딩 최대 재시도 횟수 초과\")\n",
    "                    return []\n",
    "            except Exception as e:\n",
    "                logger.error(f\"뉴스 리스트 가져오기 실패 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    return []\n",
    "    \n",
    "    def get_pdf_links(self, detail_url, max_retries=3):\n",
    "        \"\"\"상세 페이지에서 PDF 링크만 추출\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"PDF 링크 추출 중: {detail_url}\")\n",
    "                self.driver.get(detail_url)\n",
    "                time.sleep(3)  # 페이지 로딩 대기\n",
    "                \n",
    "                # 모든 파일 링크 찾기\n",
    "                file_elements = self.driver.find_elements(By.XPATH, \"//a[starts-with(@href, '/fileSrc/')]\")\n",
    "                \n",
    "                if not file_elements:\n",
    "                    logger.info(\"첨부 파일을 찾을 수 없음\")\n",
    "                    return []\n",
    "                \n",
    "                pdf_links = []\n",
    "                for file_element in file_elements:\n",
    "                    try:\n",
    "                        file_href = file_element.get_attribute(\"href\")\n",
    "                        file_title = file_element.get_attribute(\"title\") or file_element.text.strip()\n",
    "                        \n",
    "                        if file_title:\n",
    "                            # 파일 확장자 확인\n",
    "                            file_ext = self.get_file_extension_from_url(file_href) or self.get_file_extension_from_title(file_title)\n",
    "                            \n",
    "                            # PDF 파일만 필터링\n",
    "                            if file_ext and file_ext.lower() == 'pdf':\n",
    "                                pdf_links.append({\n",
    "                                    'title': file_title,\n",
    "                                    'file_url': urljoin(self.DETAIL_BASE, file_href),\n",
    "                                    'file_type': 'pdf'\n",
    "                                })\n",
    "                                logger.info(f\"  PDF 파일 발견: {file_title}\")\n",
    "                            else:\n",
    "                                logger.info(f\"  PDF가 아닌 파일은 건너뜀: {file_title} ({file_ext})\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"파일 링크 처리 실패: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if pdf_links:\n",
    "                    logger.info(f\"총 {len(pdf_links)}개의 PDF 파일 발견\")\n",
    "                else:\n",
    "                    logger.info(\"PDF 파일을 찾을 수 없음\")\n",
    "                \n",
    "                return pdf_links\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"PDF 링크 추출 실패 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    return []\n",
    "    \n",
    "    def get_file_extension_from_url(self, url):\n",
    "        \"\"\"URL에서 파일 확장자 추출\"\"\"\n",
    "        try:\n",
    "            # URL에서 파일명 부분 추출\n",
    "            filename = url.split('/')[-1]\n",
    "            if '.' in filename:\n",
    "                return filename.split('.')[-1]\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def get_file_extension_from_title(self, title):\n",
    "        \"\"\"제목에서 파일 확장자 추출\"\"\"\n",
    "        try:\n",
    "            if '.' in title:\n",
    "                return title.split('.')[-1]\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def download_pdf(self, file_url, save_path, max_retries=3):\n",
    "        \"\"\"PDF 파일 다운로드\"\"\"\n",
    "        if os.path.exists(save_path):\n",
    "            logger.info(f\"이미 존재함, 건너뜀: {os.path.basename(save_path)}\")\n",
    "            return True\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"PDF 다운로드 중: {os.path.basename(save_path)}\")\n",
    "                \n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(file_url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # PDF 파일인지 확인 (Content-Type 체크)\n",
    "                content_type = response.headers.get('content-type', '').lower()\n",
    "                if 'pdf' not in content_type and not save_path.lower().endswith('.pdf'):\n",
    "                    logger.warning(f\"PDF가 아닌 파일로 보임: {content_type}\")\n",
    "                \n",
    "                with open(save_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                file_size = os.path.getsize(save_path)\n",
    "                logger.info(f\"PDF 다운로드 완료: {os.path.basename(save_path)} ({file_size:,} bytes)\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"PDF 다운로드 실패 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    return False\n",
    "    \n",
    "    def sanitize_filename(self, filename, max_length=100):\n",
    "        \"\"\"파일명 정리\"\"\"\n",
    "        # 불법 문자 제거\n",
    "        filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename.strip())\n",
    "        # 연속 공백을 하나로\n",
    "        filename = re.sub(r'\\s+', '_', filename)\n",
    "        # 콜론을 대시로\n",
    "        filename = filename.replace(\":\", \"-\")\n",
    "        # 길이 제한\n",
    "        if len(filename) > max_length:\n",
    "            filename = filename[:max_length]\n",
    "        return filename\n",
    "    \n",
    "    def run_scraper(self, start_page=1, end_page=1):\n",
    "        \"\"\"메인 스크래핑 실행 (PDF 전용)\"\"\"\n",
    "        if not self.setup_driver():\n",
    "            logger.error(\"드라이버 설정 실패로 종료\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            all_results = []\n",
    "            \n",
    "            for page in range(start_page, end_page + 1):\n",
    "                logger.info(f\"\\n{'='*50}\")\n",
    "                logger.info(f\"페이지 {page} 처리 시작\")\n",
    "                logger.info(f\"{'='*50}\")\n",
    "                \n",
    "                news_list = self.get_news_list(page)\n",
    "                if not news_list:\n",
    "                    logger.warning(f\"페이지 {page}에서 뉴스를 찾을 수 없음\")\n",
    "                    continue\n",
    "                \n",
    "                # 각 뉴스 항목 처리\n",
    "                for i, news in enumerate(tqdm(news_list, desc=f\"페이지 {page} 처리\")):\n",
    "                    logger.info(f\"\\n[{i+1}/{len(news_list)}] 처리 중: {news['title'][:50]}...\")\n",
    "                    \n",
    "                    # PDF 링크만 추출\n",
    "                    pdf_links = self.get_pdf_links(news['detail_url'])\n",
    "                    news['pdf_files'] = pdf_links\n",
    "                    \n",
    "                    if not pdf_links:\n",
    "                        logger.info(\"이 뉴스에는 PDF 파일이 없음\")\n",
    "                        all_results.append(news)\n",
    "                        continue\n",
    "                    \n",
    "                    # PDF 파일 다운로드\n",
    "                    for j, pdf_info in enumerate(pdf_links):\n",
    "                        file_title = pdf_info['title']\n",
    "                        file_url = pdf_info['file_url']\n",
    "                        \n",
    "                        # 파일명 생성\n",
    "                        safe_date = self.sanitize_filename(news['date']) if news['date'] else 'no_date'\n",
    "                        safe_title = self.sanitize_filename(file_title, 50)\n",
    "                        \n",
    "                        # PDF 확장자 확인 및 추가\n",
    "                        if not safe_title.lower().endswith('.pdf'):\n",
    "                            safe_title += '.pdf'\n",
    "                        \n",
    "                        filename = f\"{safe_date}_{safe_title}\"\n",
    "                        save_path = os.path.join(self.save_dir, filename)\n",
    "                        \n",
    "                        # PDF 다운로드\n",
    "                        success = self.download_pdf(file_url, save_path)\n",
    "                        pdf_info['download_success'] = success\n",
    "                        pdf_info['local_path'] = save_path if success else None\n",
    "                    \n",
    "                    all_results.append(news)\n",
    "                    time.sleep(1)  # 서버 부하 방지\n",
    "            \n",
    "            # 결과 요약 출력\n",
    "            self.print_summary(all_results)\n",
    "            return all_results\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"\\n사용자에 의해 중단됨\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"스크래핑 중 오류 발생: {e}\")\n",
    "        finally:\n",
    "            self.cleanup()\n",
    "    \n",
    "    def print_summary(self, results):\n",
    "        \"\"\"결과 요약 출력 (PDF 전용)\"\"\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(\"PDF 다운로드 결과 요약\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        total_news = len(results)\n",
    "        total_pdfs = sum(len(news.get('pdf_files', [])) for news in results)\n",
    "        successful_downloads = sum(\n",
    "            sum(1 for pdf_info in news.get('pdf_files', []) if pdf_info.get('download_success', False))\n",
    "            for news in results\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"총 뉴스 항목: {total_news}\")\n",
    "        logger.info(f\"총 PDF 파일: {total_pdfs}\")\n",
    "        logger.info(f\"성공한 다운로드: {successful_downloads}\")\n",
    "        logger.info(f\"실패한 다운로드: {total_pdfs - successful_downloads}\")\n",
    "        \n",
    "        # 저장된 PDF 파일 목록\n",
    "        saved_pdfs = [f for f in os.listdir(self.save_dir) if f.endswith('.pdf')]\n",
    "        logger.info(f\"저장된 PDF 파일 수: {len(saved_pdfs)}\")\n",
    "        \n",
    "        if saved_pdfs:\n",
    "            logger.info(\"\\n저장된 PDF 파일 목록:\")\n",
    "            for pdf_file in saved_pdfs:\n",
    "                file_path = os.path.join(self.save_dir, pdf_file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                logger.info(f\"  - {pdf_file} ({file_size:,} bytes)\")\n",
    "        \n",
    "        # PDF가 없는 뉴스 항목 통계\n",
    "        news_without_pdf = sum(1 for news in results if not news.get('pdf_files'))\n",
    "        if news_without_pdf > 0:\n",
    "            logger.info(f\"\\nPDF가 없는 뉴스 항목: {news_without_pdf}개\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"리소스 정리\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"드라이버 종료 완료\")\n",
    "\n",
    "# 사용 예시\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    # PDF 전용 스크래퍼 인스턴스 생성\n",
    "    scraper = BOKNewsScraper(\n",
    "        save_dir='./data/ex_pdf',  # 저장 경로\n",
    "        headless=True  # GUI 없이 실행 (RunPod에서는 True 권장)\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 1페이지만 스크래핑 (테스트용)\n",
    "        results = scraper.run_scraper(start_page=1, end_page=1)\n",
    "        \n",
    "        # 여러 페이지를 원할 경우:\n",
    "        # results = scraper.run_scraper(start_page=1, end_page=3)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"메인 실행 중 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqFB8ceBGWvt"
   },
   "source": [
    "# 2. Extracted Data\n",
    "#### pdf(raw data) -> Docling -> Table, Text, Image(graph, chart etc..)\n",
    "\n",
    "\n",
    "#### 1) ex_images(이미지만 모음.)\n",
    "#### 2) ex_text (텍스트 마크다운 형태)\n",
    "#### 3_ ex_table (테이블 마크다운 형태태)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b03gnccU99Mk"
   },
   "source": [
    "## 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZTb_fG-xGzey"
   },
   "outputs": [],
   "source": [
    "# JSON 모듈 가져오기 - 데이터 직렬화 및 역직렬화에 사용\n",
    "import json\n",
    "# 로깅 모듈 가져오기 - 디버깅 및 정보 기록에 사용\n",
    "import logging\n",
    "# 시간 측정 모듈 가져오기 - 실행 시간 측정에 사용\n",
    "import time\n",
    "# 파일 경로 처리 모듈 가져오기 - 파일 및 디렉토리 경로 관리에 사용\n",
    "from pathlib import Path\n",
    "# 로거 인스턴스 생성 - 현재 모듈의 로깅을 위해 사용\n",
    "import os\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o9QApKB5Gzhn"
   },
   "outputs": [],
   "source": [
    "# 기본 데이터 모델 가져오기 - 입력 형식 정의에 사용\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "# PDF 파이프라인 옵션 가져오기 - PDF 처리 설정에 사용\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "# 문서 변환기 및 PDF 형식 옵션 가져오기 - 문서 변환에 사용\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# PDF 파이프라인 옵션 설정\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "\n",
    "pipeline_options.do_ocr = False  # OCR 기능 비활성화 (이미 텍스트가 있는 PDF 사용)\n",
    "pipeline_options.ocr_options.lang = [\"ko\"]  # OCR 언어를 한국어로 설정 (OCR 사용 시)\n",
    "\n",
    "pipeline_options.do_table_structure = True  # 표 구조 인식 활성화 --> docling의 tableformer 활용해 표 상세 사항 파악\n",
    "pipeline_options.table_structure_options.do_cell_matching = True  # 표 셀 매칭 활성화\n",
    "\n",
    "# 문서 변환기 생성 및 PDF 형식 옵션 설정\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCwUd2vhGzkt",
    "outputId": "82e3ae8f-3476-49d3-c9e9-94781dab01bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:40,983 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:40,983 - INFO - Initializing pipeline for StandardPdfPipeline with options hash ab7aa2351bda7a3639289f49ddf570b8\n",
      "2025-06-23 06:24:40,987 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-06-23 06:24:40,987 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-06-23 06:24:40,988 - INFO - Accelerator device: 'cuda:0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 변환 시작: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:47,791 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-23 06:24:48,050 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-06-23 06:24:48,051 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-06-23 06:24:48,051 - INFO - Processing document 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf\n",
      "2025-06-23 06:24:50,178 - INFO - Finished converting document 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf in 9.20 sec.\n",
      "2025-06-23 06:24:50,191 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:50,192 - INFO - Processing document 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.pdf (9.20초)\n",
      "🔄 변환 시작: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:51,390 - INFO - Finished converting document 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf in 1.20 sec.\n",
      "2025-06-23 06:24:51,400 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:51,400 - INFO - Processing document 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.pdf (1.20초)\n",
      "🔄 변환 시작: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:52,285 - INFO - Finished converting document 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf in 0.89 sec.\n",
      "2025-06-23 06:24:52,294 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:52,295 - INFO - Processing document 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.pdf (0.89초)\n",
      "🔄 변환 시작: 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:53,555 - INFO - Finished converting document 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf in 1.26 sec.\n",
      "2025-06-23 06:24:53,562 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:53,563 - INFO - Processing document 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.pdf (1.27초)\n",
      "🔄 변환 시작: 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:54,633 - INFO - Finished converting document 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf in 1.07 sec.\n",
      "2025-06-23 06:24:54,644 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:54,644 - INFO - Processing document 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.pdf (1.08초)\n",
      "🔄 변환 시작: 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:54,957 - INFO - Finished converting document 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf in 0.31 sec.\n",
      "2025-06-23 06:24:54,966 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:54,966 - INFO - Processing document 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.pdf (0.32초)\n",
      "🔄 변환 시작: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:55,981 - INFO - Finished converting document 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf in 1.02 sec.\n",
      "2025-06-23 06:24:55,987 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:55,987 - INFO - Processing document 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.pdf (1.02초)\n",
      "🔄 변환 시작: 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:56,815 - INFO - Finished converting document 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf in 0.83 sec.\n",
      "2025-06-23 06:24:56,823 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:56,824 - INFO - Processing document 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.pdf (0.83초)\n",
      "🔄 변환 시작: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:25:01,390 - INFO - Finished converting document 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf in 4.57 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 변환 완료: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.pdf (4.57초)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_dir = Path(\"./data/ex_pdf\")\n",
    "output_base_dir = Path(\"./data/ex_text\")\n",
    "output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# PDF 파일 리스트 가져오기\n",
    "pdf_files = [f for f in os.listdir(input_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "if not pdf_files:\n",
    "    raise FileNotFoundError(\"입력 디렉토리에 PDF 파일이 없습니다.\")\n",
    "\n",
    "# 각 PDF 파일 처리\n",
    "for pdf_file in pdf_files:\n",
    "    input_doc_path = input_dir / pdf_file\n",
    "\n",
    "    try:\n",
    "        print(f\"🔄 변환 시작: {pdf_file}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 문서 변환 실행\n",
    "        conv_result = doc_converter.convert(str(input_doc_path))\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"✅ 변환 완료: {pdf_file} ({end_time:.2f}초)\")\n",
    "\n",
    "        # 파일 이름(확장자 제외)\n",
    "        doc_filename = conv_result.input.file.stem\n",
    "\n",
    "        # PDF별 출력 디렉토리 생성\n",
    "        pdf_output_dir = output_base_dir / doc_filename\n",
    "        pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "        # 마크다운 저장\n",
    "        with (pdf_output_dir / f\"{doc_filename}.md\").open(\"w\", encoding=\"utf-8\") as fp:\n",
    "            fp.write(conv_result.document.export_to_markdown())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 변환 실패: {pdf_file} - {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테이블 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 모듈 가져오기 - 데이터 직렬화 및 역직렬화에 사용\n",
    "import json\n",
    "# 로깅 모듈 가져오기 - 디버깅 및 정보 기록에 사용\n",
    "import logging\n",
    "# 시간 측정 모듈 가져오기 - 실행 시간 측정에 사용\n",
    "import time\n",
    "# 파일 경로 처리 모듈 가져오기 - 파일 및 디렉토리 경로 관리에 사용\n",
    "from pathlib import Path\n",
    "# 로거 인스턴스 생성 - 현재 모듈의 로깅을 위해 사용\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "_log = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(\"./data/ex_pdf\")\n",
    "    output_dir = Path(\"./data/ex_table\")\n",
    "\n",
    "    # PDF 파일 리스트 가져오기\n",
    "    pdf_files = [f for f in os.listdir(input_doc_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "    doc_converter = DocumentConverter()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 각 PDF 파일을 개별적으로 변환\n",
    "    for pdf_file in pdf_files:\n",
    "        full_pdf_path = input_doc_path / pdf_file  # 파일 경로 조합\n",
    "        doc_filename = Path(pdf_file).stem # 파일 이름 가져오기 (확장자 제외)\n",
    "\n",
    "        # 파일 이름으로 디렉토리 생성\n",
    "        file_output_dir = output_dir / doc_filename\n",
    "        file_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        conv_res = doc_converter.convert(full_pdf_path)  # 개별 파일 전달\n",
    "\n",
    "\n",
    "        # Export tables\n",
    "        for table_ix, table in enumerate(conv_res.document.tables):\n",
    "            table_df: pd.DataFrame = table.export_to_dataframe()\n",
    "            # 파일 이름으로 구분하여 출력\n",
    "            print(f\"## {doc_filename} - Table {table_ix}\")\n",
    "            print(table_df.to_markdown())\n",
    "\n",
    "            # Save the table as md in the dedicated directory\n",
    "            element_md_filename = file_output_dir / f\"{doc_filename}-table-{table_ix + 1}.md\"\n",
    "            _log.info(f\"Saving md table to {element_md_filename}\")\n",
    "            with element_md_filename.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(table.export_to_html(doc=conv_res.document))\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(f\"Documents converted and tables exported in {end_time:.2f} seconds.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0ZUs6p6-AeT"
   },
   "source": [
    "## 이미지 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C49gYsp-Gz1g"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2IY7FGR9Gz5F"
   },
   "outputs": [],
   "source": [
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ap9RYpsRGz73"
   },
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IaTB9h5QG0CD"
   },
   "outputs": [],
   "source": [
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s0FxMAZb89-j"
   },
   "outputs": [],
   "source": [
    "IMAGE_RESOLUTION_SCALE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aiOEIYPM8-BD"
   },
   "outputs": [],
   "source": [
    "# 이미지 해상도 스케일 설정 (예: 2 = 144 DPI)\n",
    "IMAGE_RESOLUTION_SCALE = 2\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    _log = logging.getLogger(__name__)\n",
    "\n",
    "    input_dir = Path(\"./data/ex_pdf\")\n",
    "    output_base_dir = Path(\"./data/images\")\n",
    "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = [f for f in input_dir.iterdir() if f.suffix.lower() == \".pdf\"]\n",
    "\n",
    "    if not pdf_files:\n",
    "        _log.warning(\"No PDF files found.\")\n",
    "        return\n",
    "\n",
    "    # 변환 파이프라인 설정\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            _log.info(f\"Processing: {pdf_file.name}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            conv_res = doc_converter.convert(str(pdf_file))\n",
    "            doc_filename = pdf_file.stem\n",
    "\n",
    "            # PDF별 디렉토리 생성\n",
    "            pdf_output_dir = output_base_dir / doc_filename\n",
    "            pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # 표 및 이미지 저장\n",
    "            table_counter = 0\n",
    "            picture_counter = 0\n",
    "            for element, _ in conv_res.document.iterate_items():\n",
    "                if isinstance(element, TableItem):\n",
    "                    table_counter += 1\n",
    "                    img_path = pdf_output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "                    with img_path.open(\"wb\") as fp:\n",
    "                        element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "                if isinstance(element, PictureItem):\n",
    "                    picture_counter += 1\n",
    "                    img_path = pdf_output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "                    with img_path.open(\"wb\") as fp:\n",
    "                        element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "            elapsed = time.time() - start_time\n",
    "            _log.info(f\"{pdf_file.name} processed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            _log.error(f\"❌ Failed to process {pdf_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9qEt7wV8-De",
    "outputId": "be46dd2c-76d8-4f41-e5a8-9c2a16e96845"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqh49TqoG0OX"
   },
   "source": [
    "# 3. VLM을 이용해서 각 Image에 대한 요약 생성\n",
    "## qwen 2.5 VL 7B-awq사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM 모델 로딩 디버깅을 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLM 모델 로딩 디버깅을 위한 개선된 코드\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def debug_load_model():\n",
    "    \"\"\"VLM 모델 로딩 디버깅 함수\"\"\"\n",
    "    print(\"🔍 VLM 모델 로딩 디버깅 시작\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. 기본 환경 확인\n",
    "        print(\"1️⃣ 기본 환경 확인:\")\n",
    "        print(f\"   🐍 Python 버전: {sys.version}\")\n",
    "        print(f\"   🔥 PyTorch 버전: {torch.__version__}\")\n",
    "        print(f\"   🖥️ CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   🎯 CUDA 버전: {torch.version.cuda}\")\n",
    "            print(f\"   🔢 GPU 개수: {torch.cuda.device_count()}\")\n",
    "            print(f\"   📛 GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. 필요한 라이브러리 임포트 테스트\n",
    "        print(\"2️⃣ 필요한 라이브러리 임포트 테스트:\")\n",
    "        \n",
    "        try:\n",
    "            from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "            print(\"   ✅ transformers 임포트 성공\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   ❌ transformers 임포트 실패: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            from PIL import Image\n",
    "            print(\"   ✅ PIL 임포트 성공\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   ❌ PIL 임포트 실패: {e}\")\n",
    "            return None, None\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        # 3. 모델 이름 확인\n",
    "        model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # 또는 사용하려는 모델명\n",
    "        print(f\"3️⃣ 로딩할 모델: {model_name}\")\n",
    "        print()\n",
    "        \n",
    "        # 4. 프로세서 로딩 테스트\n",
    "        print(\"4️⃣ 프로세서 로딩 중...\")\n",
    "        try:\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            print(\"   ✅ 프로세서 로딩 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 프로세서 로딩 실패: {e}\")\n",
    "            print(f\"   📋 상세 오류:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # 5. 모델 로딩 테스트\n",
    "        print(\"5️⃣ 모델 로딩 중...\")\n",
    "        try:\n",
    "            model = AutoModelForVision2Seq.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "            )\n",
    "            print(\"   ✅ 모델 로딩 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 모델 로딩 실패: {e}\")\n",
    "            print(f\"   📋 상세 오류:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # 6. GPU 메모리 상태 확인\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"6️⃣ GPU 메모리 상태:\")\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"   📊 할당된 메모리: {memory_allocated:.2f}GB\")\n",
    "            print(f\"   📦 예약된 메모리: {memory_reserved:.2f}GB\")\n",
    "            print(f\"   💾 총 메모리: {total_memory:.2f}GB\")\n",
    "            print(f\"   🆓 사용 가능: {total_memory - memory_reserved:.2f}GB\")\n",
    "        \n",
    "        print()\n",
    "        print(\"✅ 모든 테스트 통과! 모델 로딩 성공\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 예상치 못한 오류 발생: {e}\")\n",
    "        print(f\"📋 전체 오류 스택:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 VLM 모델 로딩 디버깅 시작\n",
      "============================================================\n",
      "1️⃣ 기본 환경 확인:\n",
      "   🐍 Python 버전: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]\n",
      "   🔥 PyTorch 버전: 2.4.1+cu124\n",
      "   🖥️ CUDA 사용 가능: True\n",
      "   🎯 CUDA 버전: 12.4\n",
      "   🔢 GPU 개수: 1\n",
      "   📛 GPU 이름: NVIDIA GeForce RTX 3090\n",
      "\n",
      "2️⃣ 필요한 라이브러리 임포트 테스트:\n",
      "   ✅ transformers 임포트 성공\n",
      "   ✅ PIL 임포트 성공\n",
      "\n",
      "3️⃣ 로딩할 모델: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "\n",
      "4️⃣ 프로세서 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ 프로세서 로딩 성공\n",
      "\n",
      "5️⃣ 모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:28:01,250 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ea577e42104676a2428de67b3f81b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ 모델 로딩 성공\n",
      "\n",
      "6️⃣ GPU 메모리 상태:\n",
      "   📊 할당된 메모리: 15.98GB\n",
      "   📦 예약된 메모리: 17.59GB\n",
      "   💾 총 메모리: 23.68GB\n",
      "   🆓 사용 가능: 6.09GB\n",
      "\n",
      "✅ 모든 테스트 통과! 모델 로딩 성공\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Qwen2_5_VLForConditionalGeneration(\n",
       "   (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "     (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "       (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "     )\n",
       "     (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "     (blocks): ModuleList(\n",
       "       (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "         (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "         (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "         (attn): Qwen2_5_VLVisionFlashAttention2(\n",
       "           (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "           (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         )\n",
       "         (mlp): Qwen2_5_VLMLP(\n",
       "           (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "           (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "           (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (merger): Qwen2_5_VLPatchMerger(\n",
       "       (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "       (mlp): Sequential(\n",
       "         (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "         (1): GELU(approximate='none')\n",
       "         (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (model): Qwen2_5_VLModel(\n",
       "     (embed_tokens): Embedding(152064, 3584)\n",
       "     (layers): ModuleList(\n",
       "       (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
       "         (self_attn): Qwen2_5_VLFlashAttention2(\n",
       "           (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "           (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "           (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "           (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "           (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "           (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "           (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "         (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "     (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       " ),\n",
       " Qwen2_5_VLProcessor:\n",
       " - image_processor: Qwen2VLImageProcessor {\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"max_pixels\": 12845056,\n",
       "   \"merge_size\": 2,\n",
       "   \"min_pixels\": 3136,\n",
       "   \"patch_size\": 14,\n",
       "   \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"longest_edge\": 12845056,\n",
       "     \"shortest_edge\": 3136\n",
       "   },\n",
       "   \"temporal_patch_size\": 2\n",
       " }\n",
       " \n",
       " - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " }\n",
       " )\n",
       " \n",
       " {\n",
       "   \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
       " })"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델 단독테트트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 LLM 모델 로딩 디버깅을 실행합니다...\n",
      "🧪 LLM 모델 로딩 디버깅 시작\n",
      "============================================================\n",
      "1️⃣ 기본 환경 확인:\n",
      "   🐍 Python 버전: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]\n",
      "   🔥 PyTorch 버전: 2.4.1+cu124\n",
      "   🖥️ CUDA 사용 가능: True\n",
      "   🎯 CUDA 버전: 12.4\n",
      "   🔢 GPU 개수: 1\n",
      "   📛 GPU 이름: NVIDIA GeForce RTX 3090\n",
      "\n",
      "2️⃣ 필요한 라이브러리 임포트 테스트:\n",
      "   ✅ transformers 임포트 성공\n",
      "\n",
      "3️⃣ 로딩할 LLM 모델: google/gemma-3-1b-it\n",
      "\n",
      "4️⃣ 토크나이저 로딩 중...\n",
      "   ✅ 토크나이저 로딩 성공\n",
      "\n",
      "5️⃣ 모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:30:06,478 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ 모델 로딩 성공\n",
      "\n",
      "6️⃣ 간단한 추론 테스트:\n",
      "   ✅ 추론 성공\n",
      "   💬 출력 예시: AI가 무엇인가요?\n",
      "\n",
      "AI는 **인공지능(Artificial Intelligence)**의 약자로, 인간의 지능을 모방하여 컴퓨터가 스스로 학습하고 문제를 해결할 수 있도록 하는 ...\n",
      "\n",
      "7️⃣ GPU 메모리 상태:\n",
      "   📊 할당된 메모리: 17.85GB\n",
      "   📦 예약된 메모리: 20.02GB\n",
      "   💾 총 메모리: 23.68GB\n",
      "   🆓 사용 가능: 3.66GB\n",
      "\n",
      "✅ 모든 테스트 통과! LLM 모델 로딩 성공\n",
      "\n",
      "📋 최종 결과: 성공\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def debug_load_llm_model():\n",
    "    \"\"\"LLM 모델 로딩 디버깅 함수\"\"\"\n",
    "    print(\"🧪 LLM 모델 로딩 디버깅 시작\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        # 1. 기본 환경 확인\n",
    "        print(\"1️⃣ 기본 환경 확인:\")\n",
    "        print(f\"   🐍 Python 버전: {sys.version}\")\n",
    "        print(f\"   🔥 PyTorch 버전: {torch.__version__}\")\n",
    "        print(f\"   🖥️ CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   🎯 CUDA 버전: {torch.version.cuda}\")\n",
    "            print(f\"   🔢 GPU 개수: {torch.cuda.device_count()}\")\n",
    "            print(f\"   📛 GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "        print()\n",
    "\n",
    "        # 2. 라이브러리 임포트 테스트\n",
    "        print(\"2️⃣ 필요한 라이브러리 임포트 테스트:\")\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "            print(\"   ✅ transformers 임포트 성공\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   ❌ transformers 임포트 실패: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # 3. 모델 이름 지정\n",
    "        model_name = \"google/gemma-3-4b-it\"  # 원하는 LLM 이름으로 변경 가능\n",
    "        print(f\"3️⃣ 로딩할 LLM 모델: {model_name}\")\n",
    "        print()\n",
    "\n",
    "        # 4. 토크나이저 로딩 테스트\n",
    "        print(\"4️⃣ 토크나이저 로딩 중...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"   ✅ 토크나이저 로딩 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 토크나이저 로딩 실패: {e}\")\n",
    "            print(f\"   📋 상세 오류:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # 5. 모델 로딩 테스트\n",
    "        print(\"5️⃣ 모델 로딩 중...\")\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"   ✅ 모델 로딩 성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 모델 로딩 실패: {e}\")\n",
    "            print(f\"   📋 상세 오류:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "\n",
    "        print()\n",
    "\n",
    "        # 6. 간단한 추론 테스트\n",
    "        print(\"6️⃣ 간단한 추론 테스트:\")\n",
    "        try:\n",
    "            input_text = \"AI가 무엇인가요?\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(\"   ✅ 추론 성공\")\n",
    "            print(f\"   💬 출력 예시: {decoded[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 추론 실패: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # 7. GPU 메모리 상태 확인\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\n7️⃣ GPU 메모리 상태:\")\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"   📊 할당된 메모리: {memory_allocated:.2f}GB\")\n",
    "            print(f\"   📦 예약된 메모리: {memory_reserved:.2f}GB\")\n",
    "            print(f\"   💾 총 메모리: {total_memory:.2f}GB\")\n",
    "            print(f\"   🆓 사용 가능: {total_memory - memory_reserved:.2f}GB\")\n",
    "\n",
    "        print()\n",
    "        print(\"✅ 모든 테스트 통과! LLM 모델 로딩 성공\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 예상치 못한 오류 발생: {e}\")\n",
    "        print(f\"📋 전체 오류 스택:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# 실행\n",
    "print(\"🧪 LLM 모델 로딩 디버깅을 실행합니다...\")\n",
    "llm_model, llm_tokenizer = debug_load_llm_model()\n",
    "print(f\"\\n📋 최종 결과: {'성공' if llm_model and llm_tokenizer else '실패'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. gpu 메모리 관리 및 모니터링 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "이 섹션에서는 GPU 메모리 상태를 실시간으로 모니터링하고 관리하는 함수들을 정의합니다. \n",
    "주요 기능으로는:\n",
    "\n",
    "1) GPU 메모리 사용량 조회 (get_gpu_memory_info)\n",
    "\n",
    "2) 메모리 정리 (cleanup_gpu_memory)\n",
    "\n",
    "3) 메모리 정리 동작 방식:\n",
    " 1. gc.collect(): Python 가비지 컬렉션 실행\n",
    " 2. torch.cuda.empty_cache(): PyTorch GPU 캐시 비우기\n",
    " 3. torch.cuda.synchronize(): GPU 연산 완료 대기\n",
    "\n",
    "4)  메모리 임계값 확인 및 OOM(Out of Memory) 방지\n",
    "\n",
    "5)  안전한 모델 추론을 위한 재시도 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NVML 초기화 성공\n",
      "✅ 고급 GPU 관리자 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 관리를 위한 전역 설정 (클래스 정의 전에 추가)\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()  # 전역 초기화\n",
    "    PYNVML_AVAILABLE = True\n",
    "    print(\"✅ NVML 초기화 성공\")\n",
    "except Exception as e:\n",
    "    PYNVML_AVAILABLE = False\n",
    "    print(f\"⚠️ NVML 초기화 실패: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "class AdvancedGPUManager:\n",
    "    \"\"\"고급 GPU 활용률 최적화 및 OOM 방지 관리자\"\"\"\n",
    "    \n",
    "    def __init__(self, target_utilization=85.0, safety_margin=0.9):\n",
    "        self.target_utilization = target_utilization  # 목표 GPU 활용률 (%)\n",
    "        self.safety_margin = safety_margin  # 메모리 안전 마진 (90%)\n",
    "        self.memory_history = []\n",
    "        self.optimal_batch_size = 1\n",
    "        self.max_batch_size = 8\n",
    "        \n",
    "        # pynvml 초기화\n",
    "        if PYNVML_AVAILABLE:\n",
    "            try:\n",
    "                pynvml.nvmlInit()\n",
    "                self.nvml_enabled = True\n",
    "            except:\n",
    "                self.nvml_enabled = False\n",
    "        else:\n",
    "            self.nvml_enabled = False\n",
    "    \n",
    "    def get_gpu_metrics(self):\n",
    "        \"\"\"GPU 활용률과 메모리 정보를 종합적으로 수집\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return None\n",
    "            \n",
    "        if self.nvml_enabled:\n",
    "            return self._get_nvml_metrics()\n",
    "        else:\n",
    "            return self._get_torch_memory_info()\n",
    "    \n",
    "    def _get_nvml_metrics(self):\n",
    "        \"\"\"NVML을 사용한 상세한 GPU 메트릭 수집\"\"\"\n",
    "        try:\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            \n",
    "            # GPU 활용률 정보\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            \n",
    "            # 메모리 정보\n",
    "            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            total_memory = memory_info.total\n",
    "            used_memory = memory_info.used\n",
    "            free_memory = memory_info.free\n",
    "            \n",
    "            # PyTorch 메모리 정보\n",
    "            torch_allocated = torch.cuda.memory_allocated()\n",
    "            torch_reserved = torch.cuda.memory_reserved()\n",
    "            \n",
    "            return {\n",
    "                'gpu_utilization': utilization.gpu,  # GPU 코어 활용률\n",
    "                'memory_utilization': utilization.memory,  # 메모리 대역폭 활용률\n",
    "                'total_memory_gb': total_memory / (1024**3),\n",
    "                'used_memory_gb': used_memory / (1024**3),\n",
    "                'free_memory_gb': free_memory / (1024**3),\n",
    "                'memory_usage_percent': (used_memory / total_memory) * 100,\n",
    "                'torch_allocated_gb': torch_allocated / (1024**3),\n",
    "                'torch_reserved_gb': torch_reserved / (1024**3),\n",
    "                'available_memory_gb': (total_memory - torch_reserved) / (1024**3)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ NVML 메트릭 수집 실패: {e}\")\n",
    "            return self._get_torch_memory_info()\n",
    "    \n",
    "    def _get_torch_memory_info(self):\n",
    "        \"\"\"PyTorch만을 사용한 기본 메모리 정보\"\"\"\n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        reserved = torch.cuda.memory_reserved()\n",
    "        total = torch.cuda.get_device_properties(0).total_memory\n",
    "        \n",
    "        return {\n",
    "            'gpu_utilization': 0,  # 추정 불가\n",
    "            'memory_utilization': 0,\n",
    "            'total_memory_gb': total / (1024**3),\n",
    "            'used_memory_gb': reserved / (1024**3),\n",
    "            'free_memory_gb': (total - reserved) / (1024**3),\n",
    "            'memory_usage_percent': (reserved / total) * 100,\n",
    "            'torch_allocated_gb': allocated / (1024**3),\n",
    "            'torch_reserved_gb': reserved / (1024**3),\n",
    "            'available_memory_gb': (total - reserved) / (1024**3)\n",
    "        }\n",
    "    \n",
    "    def calculate_optimal_batch_size(self, current_metrics, base_memory_per_item=0.8):\n",
    "        \"\"\"현재 메모리 상황을 기반으로 최적 배치 크기 계산\"\"\"\n",
    "        if not current_metrics:\n",
    "            return 1\n",
    "            \n",
    "        # 사용 가능한 메모리 (안전 마진 적용)\n",
    "        available_memory = current_metrics['available_memory_gb'] * self.safety_margin\n",
    "        \n",
    "        # 예상 배치 크기 계산 (VLM은 메모리를 많이 사용)\n",
    "        estimated_batch_size = max(1, int(available_memory / base_memory_per_item))\n",
    "        \n",
    "        # 최대값 제한\n",
    "        optimal_batch_size = min(estimated_batch_size, self.max_batch_size)\n",
    "        \n",
    "        # GPU 활용률이 낮다면 배치 크기 증가 시도\n",
    "        if current_metrics['gpu_utilization'] < self.target_utilization and optimal_batch_size < self.max_batch_size:\n",
    "            optimal_batch_size = min(optimal_batch_size + 1, self.max_batch_size)\n",
    "        \n",
    "        return optimal_batch_size\n",
    "    \n",
    "    def should_process_batch(self, batch_size=1):\n",
    "        \"\"\"배치 처리 가능 여부 판단\"\"\"\n",
    "        metrics = self.get_gpu_metrics()\n",
    "        if not metrics:\n",
    "            return True, 1\n",
    "            \n",
    "        # 메모리 사용률이 안전 임계값을 초과하는 경우\n",
    "        if metrics['memory_usage_percent'] > (self.safety_margin * 100):\n",
    "            print(f\"⚠️ 메모리 사용률 높음 ({metrics['memory_usage_percent']:.1f}%) - 대기\")\n",
    "            return False, max(1, batch_size // 2)\n",
    "        \n",
    "        # 최적 배치 크기 계산\n",
    "        optimal_size = self.calculate_optimal_batch_size(metrics)\n",
    "        \n",
    "        return True, optimal_size\n",
    "    \n",
    "    def cleanup_memory(self, intensive=False):\n",
    "        \"\"\"메모리 정리\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            if intensive:\n",
    "                torch.cuda.synchronize()\n",
    "    \n",
    "    def print_detailed_status(self):\n",
    "        \"\"\"상세한 GPU 상태 출력\"\"\"\n",
    "        metrics = self.get_gpu_metrics()\n",
    "        if not metrics:\n",
    "            print(\"❌ GPU 메트릭을 가져올 수 없습니다\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🖥️ GPU 상태 모니터링\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"🎯 GPU 코어 활용률: {metrics['gpu_utilization']}%\")\n",
    "        print(f\"📊 메모리 대역폭 활용률: {metrics['memory_utilization']}%\")\n",
    "        print(f\"💾 메모리 사용량: {metrics['used_memory_gb']:.1f}GB / {metrics['total_memory_gb']:.1f}GB ({metrics['memory_usage_percent']:.1f}%)\")\n",
    "        print(f\"🆓 사용 가능 메모리: {metrics['available_memory_gb']:.1f}GB\")\n",
    "        print(f\"🔥 PyTorch 할당됨: {metrics['torch_allocated_gb']:.1f}GB\")\n",
    "        print(f\"📦 PyTorch 예약됨: {metrics['torch_reserved_gb']:.1f}GB\")\n",
    "        \n",
    "        # 상태 평가\n",
    "        if metrics['gpu_utilization'] < 50:\n",
    "            print(\"⚡ GPU 활용률이 낮습니다 - 배치 크기 증가 권장\")\n",
    "        elif metrics['gpu_utilization'] > 90:\n",
    "            print(\"🔥 GPU 활용률이 매우 높습니다!\")\n",
    "            \n",
    "        if metrics['memory_usage_percent'] > 85:\n",
    "            print(\"⚠️ 메모리 사용률 주의 - OOM 위험\")\n",
    "        elif metrics['memory_usage_percent'] < 50:\n",
    "            print(\"✅ 메모리 여유 충분\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "print(\"✅ 고급 GPU 관리자 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 배치 적응형 및 적응형 메모리 관리\n",
    "\n",
    "메모리 상황에 따라 동적으로 처리량을 조정하는 시스템입니다:\n",
    "\n",
    "1. 메모리 사용량이 90% 초과시 배치 크기를 절반으로 축소\n",
    "2. 메모리 여유가 60% 미만시 배치 크기를 증가시켜 효율성 향상\n",
    "3. 임계값 초과시 자동으로 처리를 일시정지하고 메모리 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 메모리 효율적 배치 처리기 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "class MemoryEfficientBatchProcessor:\n",
    "    \"\"\"메모리 효율적인 배치 처리 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_batch_size=1, max_batch_size=4):\n",
    "        self.gpu_manager = AdvancedGPUManager()\n",
    "        self.current_batch_size = initial_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.processing_stats = {\n",
    "            'total_processed': 0,\n",
    "            'successful_batches': 0,\n",
    "            'oom_events': 0,\n",
    "            'memory_cleanups': 0,\n",
    "            'batch_size_adjustments': 0\n",
    "        }\n",
    "    \n",
    "    def process_items_efficiently(self, items, process_function, **kwargs):\n",
    "        \"\"\"메모리 효율적인 아이템 처리\"\"\"\n",
    "        results = []\n",
    "        total_items = len(items)\n",
    "        processed_count = 0\n",
    "        \n",
    "        print(f\"🚀 효율적 배치 처리 시작: {total_items}개 아이템\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while processed_count < total_items:\n",
    "            # 현재 GPU 상태 확인\n",
    "            can_process, optimal_batch_size = self.gpu_manager.should_process_batch(self.current_batch_size)\n",
    "            \n",
    "            if not can_process:\n",
    "                print(\"⏸️ 메모리 부족으로 일시 정지 - 정리 중...\")\n",
    "                self._emergency_cleanup()\n",
    "                time.sleep(3)\n",
    "                continue\n",
    "            \n",
    "            # 배치 크기 조정\n",
    "            if optimal_batch_size != self.current_batch_size:\n",
    "                print(f\"🔧 배치 크기 조정: {self.current_batch_size} → {optimal_batch_size}\")\n",
    "                self.current_batch_size = optimal_batch_size\n",
    "                self.processing_stats['batch_size_adjustments'] += 1\n",
    "            \n",
    "            # 배치 생성\n",
    "            batch_start = processed_count\n",
    "            batch_end = min(processed_count + self.current_batch_size, total_items)\n",
    "            current_batch = items[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"📦 배치 처리 중: {batch_start+1}-{batch_end}/{total_items} (크기: {len(current_batch)})\")\n",
    "            \n",
    "            try:\n",
    "                # 배치 처리 실행\n",
    "                batch_results = self._process_batch_safe(current_batch, process_function, **kwargs)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "                processed_count = batch_end\n",
    "                self.processing_stats['successful_batches'] += 1\n",
    "                \n",
    "                # 진행률 및 GPU 상태 표시 (5배치마다)\n",
    "                if self.processing_stats['successful_batches'] % 5 == 0:\n",
    "                    progress = (processed_count / total_items) * 100\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = processed_count / (elapsed / 60)  # 분당 처리율\n",
    "                    \n",
    "                    print(f\"📊 진행률: {progress:.1f}% ({processed_count}/{total_items}) | 처리율: {rate:.1f}개/분\")\n",
    "                    self.gpu_manager.print_detailed_status()\n",
    "                \n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(f\"💥 OOM 발생! 배치 크기 {self.current_batch_size} → {max(1, self.current_batch_size // 2)}\")\n",
    "                self.processing_stats['oom_events'] += 1\n",
    "                self.current_batch_size = max(1, self.current_batch_size // 2)\n",
    "                self._emergency_cleanup()\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 배치 처리 오류: {e}\")\n",
    "                # 개별 처리로 폴백\n",
    "                individual_results = self._fallback_individual_processing(current_batch, process_function, **kwargs)\n",
    "                results.extend(individual_results)\n",
    "                processed_count = batch_end\n",
    "        \n",
    "        self._print_processing_summary(total_items, time.time() - start_time)\n",
    "        return results\n",
    "    \n",
    "    def _process_batch_safe(self, batch, process_function, **kwargs):\n",
    "        \"\"\"안전한 배치 처리 (메모리 모니터링 포함)\"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        for i, item in enumerate(batch):\n",
    "            # 메모리 임계값 체크 (배치 중간에도)\n",
    "            if i > 0 and i % 2 == 0:  # 2개마다 체크\n",
    "                current_metrics = self.gpu_manager.get_gpu_metrics()\n",
    "                if current_metrics and current_metrics['memory_usage_percent'] > 88:\n",
    "                    print(f\"⚠️ 배치 중간 메모리 임계값 초과 ({current_metrics['memory_usage_percent']:.1f}%) - 정리\")\n",
    "                    self._incremental_cleanup()\n",
    "            \n",
    "            # 아이템 처리\n",
    "            try:\n",
    "                result = process_function(item, **kwargs)\n",
    "                batch_results.append(result)\n",
    "                self.processing_stats['total_processed'] += 1\n",
    "                \n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                # 배치 중간 OOM 발생시 즉시 정리 후 재시도\n",
    "                print(f\"💥 배치 중간 OOM - 긴급 정리 후 재시도\")\n",
    "                self._emergency_cleanup()\n",
    "                result = process_function(item, **kwargs)\n",
    "                batch_results.append(result)\n",
    "                self.processing_stats['total_processed'] += 1\n",
    "        \n",
    "        # 배치 완료 후 메모리 정리\n",
    "        self._incremental_cleanup()\n",
    "        return batch_results\n",
    "    \n",
    "    def _fallback_individual_processing(self, batch, process_function, **kwargs):\n",
    "        \"\"\"개별 처리 폴백\"\"\"\n",
    "        print(f\"🔄 개별 처리 모드로 전환 ({len(batch)}개 아이템)\")\n",
    "        individual_results = []\n",
    "        \n",
    "        for item in batch:\n",
    "            try:\n",
    "                self._incremental_cleanup()  # 각 아이템 처리 전 정리\n",
    "                result = process_function(item, **kwargs)\n",
    "                individual_results.append(result)\n",
    "                self.processing_stats['total_processed'] += 1\n",
    "            except Exception as item_error:\n",
    "                print(f\"❌ 개별 아이템 처리 실패: {item_error}\")\n",
    "                individual_results.append(None)\n",
    "        \n",
    "        return individual_results\n",
    "    \n",
    "    def _incremental_cleanup(self):\n",
    "        \"\"\"점진적 메모리 정리\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        self.processing_stats['memory_cleanups'] += 1\n",
    "    \n",
    "    def _emergency_cleanup(self):\n",
    "        \"\"\"긴급 메모리 정리\"\"\"\n",
    "        print(\"🧹 긴급 메모리 정리 실행...\")\n",
    "        self.gpu_manager.cleanup_memory(intensive=True)\n",
    "        time.sleep(2)\n",
    "        self.processing_stats['memory_cleanups'] += 1\n",
    "    \n",
    "    def _print_processing_summary(self, total_items, total_time):\n",
    "        \"\"\"처리 결과 요약\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"📊 배치 처리 완료 요약\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"📷 총 처리 아이템: {self.processing_stats['total_processed']}/{total_items}\")\n",
    "        print(f\"📦 성공한 배치: {self.processing_stats['successful_batches']}\")\n",
    "        print(f\"💥 OOM 발생 횟수: {self.processing_stats['oom_events']}\")\n",
    "        print(f\"🔧 배치 크기 조정: {self.processing_stats['batch_size_adjustments']}회\")\n",
    "        print(f\"🧹 메모리 정리 횟수: {self.processing_stats['memory_cleanups']}\")\n",
    "        print(f\"⚡ 최종 배치 크기: {self.current_batch_size}\")\n",
    "        print(f\"⏱️ 총 처리 시간: {total_time/60:.1f}분\")\n",
    "        print(f\"📈 평균 처리율: {self.processing_stats['total_processed']/(total_time/60):.1f}개/분\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "print(\"✅ 메모리 효율적 배치 처리기 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. xml파싱및 마크다운 포맷팅 함수들\n",
    "1. VLM(Vision Language Model) 출력을 구조화된 형태로 변환하는 함수들입니다:\n",
    "2. XML 형태의 모델 출력을 파싱하여 제목, 요약, 엔티티, 질문으로 분리\n",
    "3. 마크다운 형태의 응답도 처리 가능\n",
    "4. 파싱 실패시 기본값 설정으로 안정성 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최적화된 출력 파서 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "class OptimizedOutputParser:\n",
    "    \"\"\"VLM 출력 파싱을 위한 최적화된 클래스\"\"\"\n",
    "    \n",
    "    # 정규표현식을 클래스 레벨에서 컴파일 (성능 최적화)\n",
    "    XML_PATTERNS = {\n",
    "        'title': re.compile(r'<title>(.*?)</title>', re.DOTALL | re.IGNORECASE),\n",
    "        'summary': re.compile(r'<summary>(.*?)</summary>', re.DOTALL | re.IGNORECASE),\n",
    "        'entities': re.compile(r'<entities>(.*?)</entities>', re.DOTALL | re.IGNORECASE),\n",
    "        'questions': re.compile(r'<hypothetical_questions>(.*?)</hypothetical_questions>', re.DOTALL | re.IGNORECASE)\n",
    "    }\n",
    "    \n",
    "    MARKDOWN_PATTERNS = {\n",
    "        'title': re.compile(r'###\\s*🖼️\\s*[이미지\\s]*제목[\\s\\S]*?\\n(.+?)\\n', re.IGNORECASE),\n",
    "        'summary': re.compile(r'###\\s*📋\\s*[이미지\\s]*요약[\\s\\S]*?\\n([\\s\\S]*?)(?=###|$)', re.IGNORECASE),\n",
    "        'entities': re.compile(r'###\\s*🏷️\\s*[핵심\\s]*엔티티[\\s\\S]*?\\n([\\s\\S]*?)(?=###|$)', re.IGNORECASE),\n",
    "        'questions': re.compile(r'###\\s*❓\\s*[가상질문|관련질문][\\s\\S]*?\\n([\\s\\S]*?)(?=###|$)', re.IGNORECASE)\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_image_summary(cls, raw_output):\n",
    "        \"\"\"통합 파싱 함수 (XML 우선, 마크다운 백업)\"\"\"\n",
    "        # 기본 구조\n",
    "        parsed_data = {\n",
    "            'title': '',\n",
    "            'summary': '',\n",
    "            'entities': '',\n",
    "            'hypothetical_questions': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # XML 파싱 시도 (컴파일된 패턴 사용)\n",
    "            for key, pattern in cls.XML_PATTERNS.items():\n",
    "                match = pattern.search(raw_output)\n",
    "                if match:\n",
    "                    parsed_data[key] = match.group(1).strip()\n",
    "            \n",
    "            # XML이 실패하면 마크다운 파싱\n",
    "            if not any(parsed_data.values()):\n",
    "                markdown_keys = {'title': 'title', 'summary': 'summary', 'entities': 'entities', 'questions': 'hypothetical_questions'}\n",
    "                for md_key, data_key in markdown_keys.items():\n",
    "                    if md_key in cls.MARKDOWN_PATTERNS:\n",
    "                        match = cls.MARKDOWN_PATTERNS[md_key].search(raw_output)\n",
    "                        if match:\n",
    "                            parsed_data[data_key] = match.group(1).strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 파싱 오류: {e}\")\n",
    "            # 오류 시 원본 텍스트를 summary에 저장\n",
    "            parsed_data['summary'] = raw_output[:500] + \"...\" if len(raw_output) > 500 else raw_output\n",
    "        \n",
    "        return parsed_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_formatted_markdown(parsed_data):\n",
    "        \"\"\"파싱된 데이터를 마크다운으로 변환 (최적화됨)\"\"\"\n",
    "        return f\"\"\"# 📊 이미지 분석 결과\n",
    "\n",
    "## 🖼️ {parsed_data.get('title', '제목 없음')}\n",
    "\n",
    "### 📋 요약\n",
    "{parsed_data.get('summary', '분석 내용 없음')}\n",
    "\n",
    "### 🏷️ 핵심 엔티티\n",
    "{parsed_data.get('entities', '엔티티 없음')}\n",
    "\n",
    "### ❓ 관련 질문들\n",
    "{parsed_data.get('hypothetical_questions', '질문 없음')}\n",
    "\n",
    "---\n",
    "*분석 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "\n",
    "# 기존 함수를 새 클래스로 대체\n",
    "def parse_image_summary_xml(raw_output):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    return OptimizedOutputParser.parse_image_summary(raw_output)\n",
    "\n",
    "def parse_markdown_response(raw_output):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    return OptimizedOutputParser.parse_image_summary(raw_output)\n",
    "\n",
    "def create_formatted_markdown_from_parsed(parsed_data):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    return OptimizedOutputParser.create_formatted_markdown(parsed_data)\n",
    "\n",
    "print(\"✅ 최적화된 출력 파서 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 파일 저장 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최적화된 파일 관리자 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "class OptimizedFileManager:\n",
    "    \"\"\"파일 저장을 위한 최적화된 클래스\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_output_directories(base_output_dir=\"./analysis_output\"):\n",
    "        \"\"\"출력용 디렉토리들을 생성 (pathlib 사용)\"\"\"\n",
    "        base_path = Path(base_output_dir)\n",
    "        directories = {\n",
    "            'base': base_path,\n",
    "            'xml': base_path / \"xml_results\",\n",
    "            'markdown': base_path / \"markdown_results\", \n",
    "            'json': base_path / \"json_results\"\n",
    "        }\n",
    "        \n",
    "        # 디렉토리 생성 (한 번에 처리)\n",
    "        for dir_name, dir_path in directories.items():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"📁 출력 디렉토리 생성 완료: {base_path}\")\n",
    "        return directories\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_results_as_markdown(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "        \"\"\"마크다운 형식으로 저장 (최적화됨)\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_{timestamp}.md\"\n",
    "            output_file = output_dirs['markdown'] / filename\n",
    "            \n",
    "            # 템플릿 문자열로 한 번에 생성\n",
    "            content_parts = [\n",
    "                \"# 📊 이미지 분석 결과\\n\",\n",
    "                f\"**생성 시간:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                f\"**총 분석 이미지:** {len(results)}개\\n\",\n",
    "                \"---\\n\"\n",
    "            ]\n",
    "            \n",
    "            # 각 이미지 결과 추가\n",
    "            for i, (image_path, analysis) in enumerate(results, 1):\n",
    "                content_parts.extend([\n",
    "                    f\"\\n## 🖼️ 이미지 {i}: {Path(image_path).name}\\n\",\n",
    "                    f\"**📁 파일 경로:** `{image_path}`\\n\",\n",
    "                    f\"**🔍 분석 결과:**\\n{analysis}\\n\",\n",
    "                    \"---\\n\"\n",
    "                ])\n",
    "            \n",
    "            # 한 번에 파일 쓰기\n",
    "            output_file.write_text(''.join(content_parts), encoding='utf-8')\n",
    "            \n",
    "            print(f\"✅ 마크다운 저장: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 마크다운 저장 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_results_as_json(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "        \"\"\"JSON 형식으로 저장 (최적화됨)\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_{timestamp}.json\"\n",
    "            output_file = output_dirs['json'] / filename\n",
    "            \n",
    "            # JSON 데이터 구조 생성 (list comprehension 사용)\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"generated_at\": datetime.now().isoformat(),\n",
    "                    \"total_images\": len(results),\n",
    "                    \"format_version\": \"1.0\"\n",
    "                },\n",
    "                \"analysis_results\": [\n",
    "                    {\n",
    "                        \"id\": i,\n",
    "                        \"file_path\": image_path,\n",
    "                        \"filename\": Path(image_path).name,\n",
    "                        \"analysis\": analysis,\n",
    "                        \"word_count\": len(analysis.split()),\n",
    "                        \"processed_at\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    for i, (image_path, analysis) in enumerate(results, 1)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # JSON 파일 저장\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"✅ JSON 저장: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ JSON 저장 실패: {e}\")\n",
    "            return None\n",
    "\n",
    "# 기존 함수들을 새 클래스로 대체\n",
    "def create_output_directories(base_output_dir=\"./output\"):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    return OptimizedFileManager.create_output_directories(base_output_dir)\n",
    "\n",
    "def save_results_as_markdown(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    return OptimizedFileManager.save_results_as_markdown(results, output_dirs, filename_prefix)\n",
    "\n",
    "def save_results_as_json(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    return OptimizedFileManager.save_results_as_json(results, output_dirs, filename_prefix)\n",
    "\n",
    "print(\"✅ 최적화된 파일 관리자 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. vlm 모델 로딩 및 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최적화된 VLM 관리자 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "class OptimizedVLMManager:\n",
    "    \"\"\"VLM 관리자\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "        self.gpu_manager = AdvancedGPUManager()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"🚀  GPU 최적화 Qwen2.5-VL 모델 로드\"\"\"\n",
    "        try:\n",
    "            print(f\"🔄 {self.model_name} 모델 로딩 중...\")\n",
    "            \n",
    "            # 로딩 전 메모리 정리\n",
    "            self.gpu_manager.cleanup_memory(intensive=True)\n",
    "            \n",
    "            # 모델 로딩 (AWQ 최적화)\n",
    "            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                attn_implementation=\"flash_attention_2\",  # 성능 최적화\n",
    "                device_map=\"auto\",  # 자동 GPU 배치\n",
    "                trust_remote_code=True,  # 원격 코드 신뢰\n",
    "                low_cpu_mem_usage=True,  # CPU 메모리 사용량 최소화\n",
    "            )\n",
    "            \n",
    "            # 프로세서 로딩\n",
    "            self.processor = AutoProcessor.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # AWQ 모델 최적화 설정 (half() 호출 제거)\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.eval()  # 평가 모드로 설정\n",
    "                print(f\"✅ 모델 최적화 완료 (dtype: {self.model.dtype})\")\n",
    "            \n",
    "            device_info = f\"CPU\" if not torch.cuda.is_available() else f\"GPU ({torch.cuda.get_device_name(0)})\"\n",
    "            print(f\"✅ 모델 로딩 완료! 디바이스: {device_info}\")\n",
    "            \n",
    "            # 메모리 상태 확인\n",
    "            self.gpu_manager.print_detailed_status()\n",
    "            \n",
    "            return self.model, self.processor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 모델 로딩 오류: {e}\")\n",
    "            print(\"💡 해결 방법:\")\n",
    "            print(\"   1. Hugging Face 로그인 확인: huggingface_hub.login()\")\n",
    "            print(\"   2. 네트워크 연결 상태 확인\")\n",
    "            print(\"   3. GPU 메모리 부족 시 다른 프로세스 종료\")\n",
    "            return None, None\n",
    "    \n",
    "    def get_optimized_prompt(self):\n",
    "        \"\"\"최적화된 프롬프트 템플릿 반환\"\"\"\n",
    "        return \"\"\"You are an expert in extracting useful information from IMAGE.\n",
    "Extract key entities, summarize them, and write useful information for retrieval.\n",
    "Provide five hypothetical questions based on the image.\n",
    "\n",
    "Please analyze this image and provide a structured summary in the following markdown format:\n",
    "\n",
    "### 🖼️ Image Title\n",
    "[Write a clear, descriptive title for the image in Korean]\n",
    "\n",
    "### 📋 Image Summary\n",
    "[Provide a comprehensive summary of the image content in Korean - describe what you see, key information, trends, patterns, etc.]\n",
    "\n",
    "### 🏷️ Key Entities\n",
    "[List the main entities, keywords, and important elements found in the image in Korean]\n",
    "\n",
    "### ❓ Hypothetical Questions\n",
    "1. [Question 1]\n",
    "2. [Question 2] \n",
    "3. [Question 3]\n",
    "4. [Question 4]\n",
    "5. [Question 5]\n",
    "\n",
    "Important: \n",
    "- All output must be in Korean\n",
    "- Focus on financial/business content if applicable\n",
    "- Include specific numbers, percentages, or data points if visible\n",
    "- Make the summary detailed and informative for retrieval purposes\"\"\"\n",
    "    \n",
    "    def process_single_image_safe(self, image_path, max_retries=3):\n",
    "        \"\"\"🛡️ OOM 재시도 메커니즘을 포함한 안전한 단일 이미지 처리\"\"\"\n",
    "        if not self.model or not self.processor:\n",
    "            print(\"❌ 모델이 로드되지 않았습니다. load_model()을 먼저 실행하세요.\")\n",
    "            return None\n",
    "        \n",
    "        prompt_text = self.get_optimized_prompt()\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # 🔍 처리 전 메모리 확인\n",
    "                memory_info = self.gpu_manager.get_gpu_metrics()\n",
    "                if memory_info and memory_info['available_memory_gb'] < 3.0:  # 3GB 임계값\n",
    "                    print(f\"⚠️ 메모리 부족 ({memory_info['available_memory_gb']:.1f}GB) - 정리 중...\")\n",
    "                    self.gpu_manager.cleanup_memory(intensive=True)\n",
    "                    time.sleep(2)\n",
    "                \n",
    "                # 📸 이미지 로드 (에러 처리 강화)\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                except Exception as img_error:\n",
    "                    print(f\"❌ 이미지 로드 실패 {image_path}: {img_error}\")\n",
    "                    return f\"이미지 로드 실패: {str(img_error)}\"\n",
    "                \n",
    "                # 메시지 구성\n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": prompt_text}\n",
    "                    ]\n",
    "                }]\n",
    "                \n",
    "                # 🔧 입력 준비 (메모리 효율적)\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                image_inputs, video_inputs = process_vision_info(messages)\n",
    "                inputs = self.processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # GPU로 이동\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.to(\"cuda\")\n",
    "                \n",
    "                # 🎯 응답 생성 (최적화된 설정)\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(\n",
    "                        **inputs, \n",
    "                        max_new_tokens=768,  # 더 긴 응답 허용\n",
    "                        do_sample=True,  # 샘플링 활성화\n",
    "                        temperature=0.7,  # 창의성 조절\n",
    "                        top_p=0.8,  # 토큰 선택 다양성\n",
    "                        pad_token_id=self.processor.tokenizer.eos_token_id\n",
    "                    )\n",
    "                    \n",
    "                    # 응답 디코딩\n",
    "                    generated_ids_trimmed = [\n",
    "                        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                    ]\n",
    "                    output_text = self.processor.batch_decode(\n",
    "                        generated_ids_trimmed, \n",
    "                        skip_special_tokens=True, \n",
    "                        clean_up_tokenization_spaces=False\n",
    "                    )\n",
    "                \n",
    "                # 🧹 즉시 메모리 정리\n",
    "                del inputs, generated_ids, generated_ids_trimmed, image\n",
    "                self.gpu_manager.cleanup_memory()\n",
    "                \n",
    "                result = output_text[0] if output_text else \"분석이 생성되지 않았습니다\"\n",
    "                \n",
    "                # 결과 검증\n",
    "                if len(result.strip()) < 100:\n",
    "                    print(f\"⚠️ 짧은 응답 감지 ({len(result)} 문자) - 재시도...\")\n",
    "                    continue\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                print(f\"💥 OOM 오류 발생 (시도 {attempt + 1}/{max_retries})\")\n",
    "                self.gpu_manager.cleanup_memory(intensive=True)\n",
    "                time.sleep(5)  # OOM 후 더 오래 대기\n",
    "                \n",
    "                if attempt == max_retries - 1:\n",
    "                    return f\"메모리 부족으로 인해 {Path(image_path).name} 처리 실패\"\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 처리 오류 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return f\"처리 오류: {str(e)}\"\n",
    "                time.sleep(2)\n",
    "        \n",
    "        return f\"{max_retries}번 시도 후 {Path(image_path).name} 처리 실패\"\n",
    "\n",
    "# 기존 함수들과 호환성 유지\n",
    "def load_model():\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    vlm_manager = OptimizedVLMManager()\n",
    "    return vlm_manager.load_model()\n",
    "\n",
    "def process_single_image_safe(model, processor, image_path, max_retries=3):\n",
    "    \"\"\"기존 함수 호환성 유지\"\"\"\n",
    "    vlm_manager = OptimizedVLMManager()\n",
    "    vlm_manager.model = model\n",
    "    vlm_manager.processor = processor\n",
    "    return vlm_manager.process_single_image_safe(image_path, max_retries)\n",
    "\n",
    "print(\"✅ 최적화된 VLM 관리자 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. GPU 최적화 메인 파이프라인 로딩 완료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU 최적화 메인 파이프라인 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "def gpu_optimized_main_pipeline(image_dir=\"./data/images\", output_dir=\"./analysis_output\"):\n",
    "    \"\"\"GPU 활용률 최적화된 메인 파이프라인\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"🚀 GPU 최적화 MultiModal RAG 시스템\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. 고급 GPU 관리자 및 배치 처리기 초기화\n",
    "    gpu_manager = AdvancedGPUManager(target_utilization=85.0)\n",
    "    batch_processor = MemoryEfficientBatchProcessor(initial_batch_size=2, max_batch_size=6)\n",
    "    \n",
    "    # 초기 GPU 상태 확인\n",
    "    print(\"🔍 초기 GPU 상태:\")\n",
    "    gpu_manager.print_detailed_status()\n",
    "    \n",
    "    # 2. 출력 디렉토리 초기화 \n",
    "    output_dirs = OptimizedFileManager.create_output_directories(output_dir)\n",
    "    \n",
    "    # 3. 이미지 파일 수집\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [\n",
    "        str(path) for path in Path(image_dir).rglob('*')\n",
    "        if path.suffix.lower() in image_extensions and path.is_file()\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"❌ 이미지 파일 없음: {image_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📷 발견된 이미지: {len(image_files)}개\")\n",
    "    \n",
    "    # 4. VLM 모델 로딩\n",
    "    try:\n",
    "        model, processor = load_model()\n",
    "        if not model:\n",
    "            print(\"❌ VLM 모델 로딩 실패\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로딩 오류: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 5. 메모리 효율적 이미지 처리 함수 정의\n",
    "    def process_single_image_optimized(image_path):\n",
    "        \"\"\"단일 이미지 처리 (최적화됨)\"\"\"\n",
    "        return process_single_image_safe(model, processor, image_path, max_retries=2)\n",
    "    \n",
    "    # 6. GPU 최적화 배치 처리 실행\n",
    "    print(f\"🔄 GPU 최적화 이미지 분석 시작...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 메모리 효율적 배치 처리 사용\n",
    "    analysis_results = batch_processor.process_items_efficiently(\n",
    "        image_files, \n",
    "        process_single_image_optimized\n",
    "    )\n",
    "    \n",
    "    # 7. 결과 필터링 및 정리\n",
    "    valid_results = []\n",
    "    for i, (image_path, analysis) in enumerate(zip(image_files, analysis_results)):\n",
    "        if analysis and len(str(analysis).strip()) > 50:\n",
    "            valid_results.append((image_path, analysis))\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"❌ 유효한 분석 결과 없음\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n✅ 분석 완료! 성공: {len(valid_results)}/{len(image_files)}개\")\n",
    "    \n",
    "    # 8. 결과 저장 \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename_prefix = f\"gpu_optimized_analysis_{timestamp}\"\n",
    "    \n",
    "    print(f\"💾 결과 저장 중...\")\n",
    "    \n",
    "    md_file = OptimizedFileManager.save_results_as_markdown(valid_results, output_dirs, filename_prefix)\n",
    "    json_file = OptimizedFileManager.save_results_as_json(valid_results, output_dirs, filename_prefix)\n",
    "    \n",
    "    # 9. 최종 정리 및 결과 요약\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n📊 GPU 최적화 처리 요약:\")\n",
    "    print(f\"   📷 처리된 이미지: {len(valid_results)}개\")\n",
    "    print(f\"   ⏱️ 총 소요 시간: {total_time/60:.1f}분\")\n",
    "    print(f\"   ⚡ 평균 처리 속도: {len(valid_results)/(total_time/60):.1f}개/분\")\n",
    "    print(f\"   📁 출력 경로: {output_dirs['base']}\")\n",
    "    \n",
    "    # 최종 GPU 상태 확인\n",
    "    print(\"\\n🔍 최종 GPU 상태:\")\n",
    "    gpu_manager.print_detailed_status()\n",
    "    \n",
    "    return {\n",
    "        'results': valid_results,\n",
    "        'output_dirs': output_dirs,\n",
    "        'files': {'markdown': md_file, 'json': json_file},\n",
    "        'processing_time': total_time,\n",
    "        'gpu_stats': batch_processor.processing_stats\n",
    "    }\n",
    "\n",
    "print(\"✅ GPU 최적화 메인 파이프라인 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 2. LLM (Large Language Model) 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 1단계: 필수 라이브러리 임포트 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM 파이프라인 라이브러리 로딩 완료!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "=======================================================================\n",
    "🤖 LLM 텍스트/테이블 분석 파이프라인 (VLM 파이프라인 스타일)\n",
    "=======================================================================\n",
    "VLM 이미지 분석 파이프라인의 구조를 따라 구현된 \n",
    "Gemma-3-1B-IT를 활용한 텍스트/테이블 분석 시스템\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "\n",
    "# 경고 메시지 억제\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ LLM 파이프라인 라이브러리 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 2단계: GPU 메모리 관리 함수들 (VLM 파이프라인에서 재사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM GPU 메모리 관리자 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "class LLMGPUMemoryManager:\n",
    "    \"\"\"LLM 파이프라인용 고급 GPU 메모리 관리자 (VLM 스타일)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleanup():\n",
    "        \"\"\"GPU 메모리 정리\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                print(\"🧹 GPU 메모리 정리 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ GPU 메모리 정리 실패: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_status():\n",
    "        \"\"\"GPU 메모리 상태 출력\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"❌ GPU를 사용할 수 없습니다\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            allocated = torch.cuda.memory_allocated()\n",
    "            reserved = torch.cuda.memory_reserved()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory\n",
    "            \n",
    "            allocated_gb = allocated / (1024**3)\n",
    "            reserved_gb = reserved / (1024**3)\n",
    "            total_gb = total / (1024**3)\n",
    "            free_gb = (total - reserved) / (1024**3)\n",
    "            usage_percent = (allocated / total) * 100\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"🖥️ LLM GPU 메모리 상태\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"📊 총 메모리: {total_gb:.2f} GB\")\n",
    "            print(f\"💾 사용 중: {allocated_gb:.2f} GB ({usage_percent:.1f}%)\")\n",
    "            print(f\"📦 예약됨: {reserved_gb:.2f} GB\")\n",
    "            print(f\"🆓 사용 가능: {free_gb:.2f} GB\")\n",
    "            \n",
    "            if usage_percent > 85:\n",
    "                print(\"⚠️ 메모리 사용률 높음!\")\n",
    "            else:\n",
    "                print(\"✅ GPU 메모리 상태 양호\")\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ GPU 메모리 상태 확인 실패: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_available_memory():\n",
    "        \"\"\"사용 가능한 GPU 메모리 반환 (GB)\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            reserved = torch.cuda.memory_reserved()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory\n",
    "            return (total - reserved) / (1024**3)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "print(\"✅ LLM GPU 메모리 관리자 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 3단계: 마크다운 텍스트/테이블 분리 전처리 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '📖' (U+1F4D6) (2547790868.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    📖 extract_clean_text()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '📖' (U+1F4D6)\n"
     ]
    }
   ],
   "source": [
    "📖 extract_clean_text()\n",
    "├── 테이블 라인 제거 (| 패턴 필터링)\n",
    "├── 공백 정규화\n",
    "└── 순수 텍스트 추출\n",
    "\n",
    "🧠 Kiwi 한국어 형태소 분석\n",
    "├── 문장 경계 탐지\n",
    "├── 문장별 분리 (start, end 위치 포함)\n",
    "└── 의미 단위 보존\n",
    "\n",
    "📝 텍스트 청킹\n",
    "├── 최대 청크 크기: 1200자\n",
    "├── 문장 경계 보존\n",
    "└── 예상 결과: 50-80개 청크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "🍲 BeautifulSoup HTML 파싱\n",
    "├── <table> 태그 감지\n",
    "├── <tr>, <th>, <td> 셀 추출\n",
    "└── 텍스트 내용 정리\n",
    "\n",
    "📊 마크다운 변환\n",
    "├── HTML → | 구분자 테이블\n",
    "├── 헤더 구분선 (---) 추가\n",
    "└── 예상 결과: 10-20개 테이블\n",
    "\n",
    "# 변환 예시:\n",
    "HTML: <table><tr><th>항목</th><th>값</th></tr><tr><td>GDP</td><td>2.1%</td></tr></table>\n",
    "↓\n",
    "마크다운: | 항목 | 값 |\n",
    "         |------|-----|\n",
    "         | GDP  | 2.1% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "🤖 Gemma-3-1B-IT 모델 로딩\n",
    "├── Eager 모드 (Dynamo 오류 방지)\n",
    "├── BFloat16 정밀도\n",
    "└── CUDA GPU 활용\n",
    "\n",
    "📦 배치 처리\n",
    "├── 총 아이템: 60-100개 (텍스트 + 테이블)\n",
    "├── 동적 배치 크기: 4-8개\n",
    "└── 메모리 효율적 처리\n",
    "\n",
    "📄 분석 결과\n",
    "├── 요약 생성\n",
    "├── 가설적 질문 생성\n",
    "└── 콘텐츠 타입 분류\n",
    "\n",
    "\n",
    "\n",
    "💾 파일 저장\n",
    "├── 마크다운 형식 (분석 결과)\n",
    "├── JSON 형식 (구조화된 데이터)\n",
    "└── 타입별 분리 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizedTextTableProcessor 전체 클래스 코드 (IndentationError 수정 포함)\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "class OptimizedTextTableProcessor:\n",
    "    \"\"\"개선된 텍스트와 테이블 분리 처리기 - Kiwi 형태소 분석기 기반 정확한 문장 분리 청킹\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.table_pattern = re.compile(r'^\\s*\\|.*\\|\\s*$', re.MULTILINE)\n",
    "        self.table_separator_pattern = re.compile(r'^\\s*\\|[\\s\\-:]*\\|\\s*$', re.MULTILINE)\n",
    "\n",
    "        print(\"🔄 Kiwi 형태소 분석기 초기화 중...\")\n",
    "        self.kiwi = Kiwi()\n",
    "        print(\"✅ Kiwi 초기화 완료\")\n",
    "\n",
    "        self.processing_stats = {\n",
    "            'total_files': 0,\n",
    "            'tables_extracted': 0,\n",
    "            'text_chunks_created': 0,\n",
    "            'processing_time': 0,\n",
    "            'kiwi_sentences_analyzed': 0,\n",
    "            'average_chunk_length': 0\n",
    "        }\n",
    "\n",
    "    def process_markdown_file(self, text_dir, table_dir, output_dir=\"./text_extraction_output\"):\n",
    "        start_time = time.time()\n",
    "\n",
    "        text_path = Path(text_dir)\n",
    "        table_path = Path(table_dir)\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"📖 텍스트 파일 처리 시작...\")\n",
    "        text_files = list(text_path.rglob(\"*.md\"))\n",
    "        all_text_chunks = []\n",
    "\n",
    "        if not text_files:\n",
    "            print(f\"❌ {text_dir}에서 마크다운 파일을 찾을 수 없습니다\")\n",
    "        else:\n",
    "            print(f\"📁 {len(text_files)}개의 텍스트 파일 발견\")\n",
    "\n",
    "            for i, text_file in enumerate(text_files, 1):\n",
    "                print(f\"🔄 텍스트 처리 중: {text_file.name} ({i}/{len(text_files)})\")\n",
    "\n",
    "                try:\n",
    "                    content = text_file.read_text(encoding='utf-8')\n",
    "                    clean_text = self.extract_clean_text(content)\n",
    "\n",
    "                    if clean_text.strip():\n",
    "                        text_chunks = self._intelligent_text_chunking(clean_text, text_file.name)\n",
    "                        all_text_chunks.extend(text_chunks)\n",
    "\n",
    "                        self.processing_stats['total_files'] += 1\n",
    "                        self.processing_stats['text_chunks_created'] += len(text_chunks)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ {text_file.name} 처리 오류: {e}\")\n",
    "\n",
    "        print(\"📊 테이블 파일 처리 시작...\")\n",
    "        all_tables = self.load_table_files(table_path)\n",
    "        self.processing_stats['tables_extracted'] = len(all_tables)\n",
    "\n",
    "        saved_files = {}\n",
    "        if all_text_chunks or all_tables:\n",
    "            saved_files = self._save_extraction_results(all_text_chunks, all_tables, output_path)\n",
    "\n",
    "        if all_text_chunks:\n",
    "            avg_length = sum(len(chunk['content']) for chunk in all_text_chunks) / len(all_text_chunks)\n",
    "            self.processing_stats['average_chunk_length'] = round(avg_length, 2)\n",
    "\n",
    "        self.processing_stats['processing_time'] = time.time() - start_time\n",
    "        self._print_processing_summary()\n",
    "\n",
    "        return {\n",
    "            'text_chunks': all_text_chunks,\n",
    "            'tables': all_tables,\n",
    "            'output_dir': output_path,\n",
    "            'saved_files': saved_files,\n",
    "            'stats': self.processing_stats\n",
    "        }\n",
    "\n",
    "    def extract_clean_text(self, content):\n",
    "        if not content or not isinstance(content, str):\n",
    "            return \"\"\n",
    "\n",
    "        lines = content.split('\\n')\n",
    "        clean_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped.startswith('|') and '|' not in line:\n",
    "                if line_stripped or len(clean_lines) == 0 or clean_lines[-1].strip():\n",
    "                    clean_lines.append(line)\n",
    "\n",
    "        return '\\n'.join(clean_lines)\n",
    "\n",
    "    def load_table_files(self, table_dir):\n",
    "        tables = []\n",
    "        table_path = Path(table_dir)\n",
    "\n",
    "        print(f\"📊 HTML 테이블 처리 시작: {table_dir}\")\n",
    "\n",
    "        for file_path in table_path.rglob(\"*.md\"):\n",
    "            try:\n",
    "                content = file_path.read_text(encoding='utf-8')\n",
    "\n",
    "                if '<table>' in content:\n",
    "                    print(f\"🍲 HTML 테이블 감지: {file_path.name}\")\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    html_tables = soup.find_all('table')\n",
    "\n",
    "                    for i, table in enumerate(html_tables):\n",
    "                        markdown_table = self._html_table_to_markdown(table)\n",
    "\n",
    "                        tables.append({\n",
    "                            'source_file': str(file_path),\n",
    "                            'filename': f\"{file_path.stem}-table-{i+1}.md\",\n",
    "                            'content': markdown_table,\n",
    "                            'table_type': 'html_converted',\n",
    "                            'original_html': str(table)\n",
    "                        })\n",
    "\n",
    "                        print(f\"   ✅ HTML 테이블 {i+1} 변환 완료\")\n",
    "\n",
    "                elif '|' in content and ('---' in content or '|-' in content):\n",
    "                    tables.append({\n",
    "                        'source_file': str(file_path),\n",
    "                        'filename': file_path.name,\n",
    "                        'content': content,\n",
    "                        'table_type': 'markdown_native'\n",
    "                    })\n",
    "                    print(f\"📋 마크다운 테이블 처리: {file_path.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 파일 처리 오류 {file_path}: {e}\")\n",
    "\n",
    "        print(f\"📊 총 {len(tables)}개 테이블 로드 완료\")\n",
    "        return tables\n",
    "\n",
    "    def _html_table_to_markdown(self, table):\n",
    "        rows = table.find_all('tr')\n",
    "        markdown_lines = []\n",
    "\n",
    "        for i, row in enumerate(rows):\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            cell_texts = [re.sub(r'\\s+', ' ', cell.get_text(strip=True)) for cell in cells]\n",
    "\n",
    "            if cell_texts:\n",
    "                markdown_line = '| ' + ' | '.join(cell_texts) + ' |'\n",
    "                markdown_lines.append(markdown_line)\n",
    "\n",
    "                if i == 0:\n",
    "                    separator = '|' + '|'.join(['---'] * len(cell_texts)) + '|'\n",
    "                    markdown_lines.append(separator)\n",
    "\n",
    "        return '\\n'.join(markdown_lines)\n",
    "\n",
    "    def _intelligent_text_chunking(self, text, source_file, max_chunk_size=1200, overlap=150):\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return []\n",
    "\n",
    "        print(f\"🧠 Kiwi 문장 분리 시작: {source_file}\")\n",
    "        sentences = self._extract_sentences_with_kiwi(text)\n",
    "        self.processing_stats['kiwi_sentences_analyzed'] += len(sentences)\n",
    "        print(f\"📝 {len(sentences)}개 문장 분리 완료\")\n",
    "\n",
    "        chunks = self._create_sentence_based_chunks(sentences, source_file, max_chunk_size, overlap)\n",
    "        print(f\"✅ {len(chunks)}개 청크 생성 완료\")\n",
    "        return chunks\n",
    "\n",
    "    def _extract_sentences_with_kiwi(self, text):\n",
    "        text = self._preprocess_text(text)\n",
    "        sentences = []\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.strip():\n",
    "                sent_results = self.kiwi.split_into_sents(paragraph.strip())\n",
    "                for sent in sent_results:\n",
    "                    if sent.text.strip() and len(sent.text.strip()) > 5:\n",
    "                        sentences.append({\n",
    "                            'text': sent.text.strip(),\n",
    "                            'start': sent.start,\n",
    "                            'end': sent.end,\n",
    "                            'word_count': len(sent.text.split())\n",
    "                        })\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[\"\"\\\"]', '\"', text)\n",
    "        text = re.sub(r'[\\'\\']', \"'\", text)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _create_sentence_based_chunks(self, sentences, source_file, max_chunk_size, overlap):\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = {\n",
    "            'sentences': [],\n",
    "            'content': '',\n",
    "            'char_count': 0,\n",
    "            'word_count': 0\n",
    "        }\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_text = sentence['text']\n",
    "            sentence_length = len(sentence_text)\n",
    "\n",
    "            if current_chunk['char_count'] + sentence_length + 1 <= max_chunk_size:\n",
    "                current_chunk['sentences'].append(sentence)\n",
    "                current_chunk['content'] += sentence_text + ' '\n",
    "                current_chunk['char_count'] += sentence_length + 1\n",
    "                current_chunk['word_count'] += sentence['word_count']\n",
    "            else:\n",
    "                if current_chunk['sentences']:\n",
    "                    chunk_data = self._finalize_chunk(current_chunk, source_file, len(chunks))\n",
    "                    chunks.append(chunk_data)\n",
    "\n",
    "                current_chunk = {\n",
    "                    'sentences': [sentence],\n",
    "                    'content': sentence_text + ' ',\n",
    "                    'char_count': sentence_length + 1,\n",
    "                    'word_count': sentence['word_count']\n",
    "                }\n",
    "\n",
    "        if current_chunk['sentences']:\n",
    "            chunk_data = self._finalize_chunk(current_chunk, source_file, len(chunks))\n",
    "            chunks.append(chunk_data)\n",
    "\n",
    "        return [chunk for chunk in chunks if len(chunk['content'].strip()) > 30]\n",
    "\n",
    "    def _finalize_chunk(self, chunk_data, source_file, chunk_index):\n",
    "        content = chunk_data['content'].strip()\n",
    "        return {\n",
    "            'content': content,\n",
    "            'source_file': source_file,\n",
    "            'type': 'text',\n",
    "            'chunk_type': 'sentence_based',\n",
    "            'chunk_index': chunk_index,\n",
    "            'word_count': chunk_data['word_count'],\n",
    "            'char_count': len(content),\n",
    "            'sentence_count': len(chunk_data['sentences']),\n",
    "            'analysis_method': 'kiwi_sentence_splitting'\n",
    "        }\n",
    "    def _save_extraction_results(self, text_chunks, tables, output_path):\n",
    "        \"\"\"추출된 텍스트 청크와 테이블을 파일로 저장\"\"\"\n",
    "        saved_files = {}\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        try:\n",
    "            # 텍스트 청크 저장\n",
    "            if text_chunks:\n",
    "                text_output_file = output_path / f\"extracted_text_chunks_{timestamp}.json\"\n",
    "                # ... JSON 저장 로직\n",
    "            \n",
    "            # 테이블 저장  \n",
    "            if tables:\n",
    "                table_output_file = output_path / f\"extracted_tables_{timestamp}.json\"\n",
    "                # ... JSON 저장 로직\n",
    "                \n",
    "            # 통합 결과 저장\n",
    "            combined_output_file = output_path / f\"extraction_results_{timestamp}.json\"\n",
    "            # ... 통합 JSON 저장 로직\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "            \n",
    "        return saved_files\n",
    "\n",
    "    def _print_processing_summary(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"📊 Kiwi 기반 문장 분리 텍스트/테이블 추출 완료\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"📁 처리된 파일: {self.processing_stats['total_files']}개\")\n",
    "        print(f\"📊 로드된 테이블: {self.processing_stats['tables_extracted']}개\")\n",
    "        print(f\"📝 생성된 텍스트 청크: {self.processing_stats['text_chunks_created']}개\")\n",
    "        print(f\"🧠 Kiwi 분석 문장: {self.processing_stats['kiwi_sentences_analyzed']}개\")\n",
    "        print(f\"📏 평균 청크 길이: {self.processing_stats['average_chunk_length']}자\")\n",
    "        print(f\"⏱️ 처리 시간: {self.processing_stats['processing_time']:.2f}초\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"🎯 Kiwi 형태소 분석기 전용 문장 분리 청킹\")\n",
    "        print(\"✅ 높은 정확도의 문장 경계 탐지\")\n",
    "        print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 4단계: Gemma-3-1B-IT 모델 로딩 및 LLM 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 수정된 OptimizedLLMManagerFixed 클래스 로딩 완료!\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ 수정된 OptimizedLLMManager (문제 해결 버전)\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "class OptimizedLLMManager:\n",
    "    \"\"\"수정된 LLM 매니저 - 프롬프트 및 파싱 로직 수정\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google/gemma-3-4b-it\", max_length=4156):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.generation_stats = {\n",
    "            'total_processed': 0,\n",
    "            'total_time': 0,\n",
    "            'avg_time_per_item': 0\n",
    "        }\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"모델 로딩 (기존 로직 유지)\"\"\"\n",
    "        try:\n",
    "            torch._dynamo.config.suppress_errors = True\n",
    "            \n",
    "            print(f\"🔄 {self.model_name} 모델 로딩 중...\")\n",
    "            \n",
    "            LLMGPUMemoryManager.cleanup()\n",
    "            \n",
    "            print(\"📝 토크나이저 로딩...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(\"🤖 모델 로딩...\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"eager\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            print(f\"✅ {self.model_name} 로딩 완료!\")\n",
    "            print(f\"🎯 사용 디바이스: {self.device}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 모델 로딩 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_content(self, text, source_file=\"\"):\n",
    "        \"\"\"🔧 수정된 콘텐츠 분석 - 프롬프트와 파싱 로직 일치\"\"\"\n",
    "        try:\n",
    "            if not self.model or not self.tokenizer:\n",
    "                raise ValueError(\"모델 또는 토크나이저가 로딩되지 않았습니다.\")\n",
    "            \n",
    "            # 텍스트 데이터 안전한 추출\n",
    "            if isinstance(text, dict):\n",
    "                actual_text = text.get('content', str(text))\n",
    "                if isinstance(source_file, dict):\n",
    "                    source_file = text.get('source_file', str(source_file))\n",
    "            else:\n",
    "                actual_text = str(text)\n",
    "            \n",
    "            # 🔧 텍스트 길이 제한 완화 (1000 → 2000)\n",
    "            text_preview = actual_text[:4000] if len(actual_text) > 4000 else actual_text\n",
    "            \n",
    "            # 🔧 수정된 프롬프트 (한국어 형식으로 일치)\n",
    "            prompt = f\"\"\"다음 텍스트/테이블을 분석하여 핵심 내용을 요약하고 관련 가설 질문을 생성하세요.\n",
    "\n",
    "중요한 해석 가이드라인:\n",
    "\n",
    "날짜 형식 해석:\n",
    "- \"25.2월\" = 2025년 2월\n",
    "- \"21.12월\" = 2021년 12월  \n",
    "- \"24.10월\" = 2024년 10월\n",
    "- 년도.월 형식은 해당 년도의 해당 월로 해석\n",
    "\n",
    "테이블 타입 구분:\n",
    "- 목차 테이블: 페이지 번호나 구조적 섹션/챕터 포함\n",
    "- 데이터 테이블: 실제 숫자, 통계, 백분율 포함\n",
    "\n",
    "목차 테이블의 경우 \"이 문서는 X개 챕터로 구성됨...\" 으로 요약 시작\n",
    "\n",
    "텍스트/테이블:\n",
    "{text_preview}\n",
    "\n",
    "다음 형식으로 응답하세요:\n",
    "\n",
    "## 콘텐츠 타입:\n",
    "[데이터 테이블 / 목차 테이블 / 일반 텍스트 중 선택]\n",
    "\n",
    "## 요약:\n",
    "[위 가이드라인에 따라 정확한 해석으로 핵심 내용 요약]\n",
    "\n",
    "## Hypothetical Questions:\n",
    "1. [질문 1]\n",
    "2. [질문 2] \n",
    "3. [질문 3]\n",
    "4. [질문 4]\n",
    "5. [질문 5]\n",
    "\n",
    "중요사항:\n",
    "- 모든 출력은 한국어로 작성\n",
    "- 금융/비즈니스 내용에 중점\n",
    "- 검색 목적을 위한 상세하고 유익한 요약 작성\n",
    "\"\"\"\n",
    "            \n",
    "            # 토큰화\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # 추론\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.5,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # 디코딩\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            # 🔧 수정된 응답 파싱 (한국어 형식에 맞춤)\n",
    "            content_type = \"일반 텍스트\"\n",
    "            summary_text = \"\"\n",
    "            questions = []\n",
    "            \n",
    "            try:\n",
    "                # 콘텐츠 타입 추출\n",
    "                if \"## 콘텐츠 타입:\" in response:\n",
    "                    sections = response.split(\"## 요약:\")\n",
    "                    type_section = sections[0].replace(\"## 콘텐츠 타입:\", \"\").strip()\n",
    "                    content_type = type_section if type_section else \"일반 텍스트\"\n",
    "                    \n",
    "                    if len(sections) > 1:\n",
    "                        remaining = \"## 요약:\" + sections[1]\n",
    "                    else:\n",
    "                        remaining = response\n",
    "                else:\n",
    "                    remaining = response\n",
    "                \n",
    "                # 요약과 질문 추출\n",
    "                if \"## 요약:\" in remaining and \"## Hypothetical Questions:\" in remaining:\n",
    "                    parts = remaining.split(\"## Hypothetical Questions:\")\n",
    "                    summary_text = parts[0].replace(\"## 요약:\", \"\").strip()\n",
    "                    \n",
    "                    # 질문 추출 (더 견고한 정규식)\n",
    "                    questions_section = parts[1] if len(parts) > 1 else \"\"\n",
    "                    import re\n",
    "                    question_matches = re.findall(r'\\d+\\.\\s*(.+?)(?=\\n\\d+\\.|$)', questions_section, re.DOTALL)\n",
    "                    questions = [q.strip() for q in question_matches if q.strip()]\n",
    "                    \n",
    "                    # 질문이 없으면 다른 방식으로 추출\n",
    "                    if not questions:\n",
    "                        lines = questions_section.strip().split('\\n')\n",
    "                        for line in lines:\n",
    "                            line = line.strip()\n",
    "                            if re.match(r'\\d+\\.', line):\n",
    "                                question = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "                                if question:\n",
    "                                    questions.append(question)\n",
    "                else:\n",
    "                    # 파싱 실패 시 전체를 요약으로 처리\n",
    "                    summary_text = remaining.replace(\"## 요약:\", \"\").strip()\n",
    "                    \n",
    "                # 🔧 질문이 여전히 없으면 기본 메시지 설정\n",
    "                if not questions:\n",
    "                    questions = [\"분석 결과에서 질문을 추출할 수 없었습니다.\"]\n",
    "                    \n",
    "            except Exception as parse_error:\n",
    "                print(f\"⚠️ 응답 파싱 오류: {parse_error}\")\n",
    "                summary_text = response[:500] + \"...\" if len(response) > 500 else response\n",
    "                questions = [f\"파싱 오류로 인한 질문 생성 실패: {str(parse_error)}\"]\n",
    "            \n",
    "            return {\n",
    "                'source_file': source_file,\n",
    "                'content_type': content_type,\n",
    "                'summary': summary_text,\n",
    "                'hypothetical_questions': questions,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 분석 오류: {e}\")\n",
    "            return {\n",
    "                'source_file': source_file,\n",
    "                'content_type': \"오류\",\n",
    "                'summary': f\"분석 실패: {str(e)}\",\n",
    "                'hypothetical_questions': [f\"오류로 인한 질문 생성 실패: {str(e)}\"],\n",
    "                'status': 'error'\n",
    "            }\n",
    "    \n",
    "    def process_batch(self, text_chunks, tables, batch_size=8):\n",
    "        \"\"\"🔧 수정된 배치 처리 - 타입 키 일치\"\"\"\n",
    "        \n",
    "        print(f\"\\n🚀 수정된 배치 처리 시작!\")\n",
    "        print(f\"📦 텍스트 청크: {len(text_chunks)}개\")\n",
    "        print(f\"📊 테이블: {len(tables)}개\") \n",
    "        print(f\"🔧 배치 크기: {batch_size}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        all_results = []\n",
    "        \n",
    "        # 1. 모든 아이템을 하나의 리스트로 통합\n",
    "        all_items = []\n",
    "        \n",
    "        # 텍스트 청크 추가\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            all_items.append({\n",
    "                'type': 'text',\n",
    "                'data': chunk,\n",
    "                'index': i,\n",
    "                'source_type': 'text_chunk'\n",
    "            })\n",
    "        \n",
    "        # 테이블 추가\n",
    "        for i, table in enumerate(tables):\n",
    "            all_items.append({\n",
    "                'type': 'table', \n",
    "                'data': table,\n",
    "                'index': i,\n",
    "                'source_type': 'table'\n",
    "            })\n",
    "        \n",
    "        total_items = len(all_items)\n",
    "        print(f\"📊 총 처리할 항목: {total_items}개\")\n",
    "        \n",
    "        # 2. 배치 단위로 처리\n",
    "        processed_count = 0\n",
    "        \n",
    "        for batch_start in range(0, total_items, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, total_items)\n",
    "            current_batch = all_items[batch_start:batch_end]\n",
    "            \n",
    "            batch_num = batch_start//batch_size + 1\n",
    "            total_batches = (total_items-1)//batch_size + 1\n",
    "            \n",
    "            print(f\"🔄 배치 {batch_num}/{total_batches}: {len(current_batch)}개 항목 처리 중...\")\n",
    "            \n",
    "            # 배치 내 각 아이템 처리\n",
    "            for item in current_batch:\n",
    "                try:\n",
    "                    if processed_count % 10 == 0:\n",
    "                        LLMGPUMemoryManager.cleanup()\n",
    "                    \n",
    "                    # 데이터 추출\n",
    "                    data = item['data']\n",
    "                    content = data.get('content', '')\n",
    "                    source_file = data.get('source_file', '')\n",
    "                    \n",
    "                    # LLM 분석 실행\n",
    "                    analysis_result = self.analyze_content(content, source_file)\n",
    "                    \n",
    "                    # 🔧 수정된 결과 구조화 - type 키 일치\n",
    "                    result = {\n",
    "                        'analysis_id': f\"{item['source_type']}_{item['index']}_{int(time.time())}\",\n",
    "                        'source_file': source_file,\n",
    "                        'type': item['type'],  # ← content_type 대신 type 사용\n",
    "                        'text_content': content[:1000],\n",
    "                        'analysis_result': analysis_result.get('summary', ''),\n",
    "                        'hypothetical_questions': analysis_result.get('hypothetical_questions', []),\n",
    "                        'word_count': len(content.split()) if content else 0,\n",
    "                        'char_count': len(content) if content else 0,\n",
    "                        'processed_at': datetime.now().isoformat(),\n",
    "                        'processing_batch': batch_num\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                    # 진행률 표시\n",
    "                    if processed_count % 5 == 0:\n",
    "                        progress = (processed_count / total_items) * 100\n",
    "                        print(f\"📈 진행률: {progress:.1f}% ({processed_count}/{total_items})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ 아이템 처리 오류: {e}\")\n",
    "                    # 오류 발생시에도 결과에 포함\n",
    "                    all_results.append({\n",
    "                        'analysis_id': f\"error_{item['index']}_{int(time.time())}\",\n",
    "                        'source_file': item['data'].get('source_file', ''),\n",
    "                        'type': item['type'],  # ← content_type 대신 type 사용\n",
    "                        'text_content': '',\n",
    "                        'analysis_result': f'처리 실패: {str(e)}',\n",
    "                        'hypothetical_questions': [f'오류로 인한 질문 생성 실패: {str(e)}'],\n",
    "                        'word_count': 0,\n",
    "                        'char_count': 0,\n",
    "                        'processed_at': datetime.now().isoformat(),\n",
    "                        'processing_batch': batch_num\n",
    "                    })\n",
    "                    processed_count += 1\n",
    "            \n",
    "            # 배치 완료 후 메모리 정리\n",
    "            LLMGPUMemoryManager.cleanup()\n",
    "            print(f\"✅ 배치 {batch_num} 완료\")\n",
    "        \n",
    "        # 3. 통계 업데이트\n",
    "        total_time = time.time() - start_time\n",
    "        self.generation_stats['total_processed'] = len(all_results)\n",
    "        self.generation_stats['total_time'] = total_time\n",
    "        self.generation_stats['avg_time_per_item'] = total_time / len(all_results) if all_results else 0\n",
    "        \n",
    "        print(f\"\\n🎉 수정된 배치 처리 완료!\")\n",
    "        print(f\"📊 처리 통계:\")\n",
    "        print(f\"   - 총 처리 항목: {len(all_results)}개\")\n",
    "        print(f\"   - 총 소요 시간: {total_time:.2f}초\")\n",
    "        print(f\"   - 평균 처리 시간: {self.generation_stats['avg_time_per_item']:.2f}초/항목\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"리소스 정리\"\"\"\n",
    "        try:\n",
    "            if self.model:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "            if self.tokenizer:\n",
    "                del self.tokenizer  \n",
    "                self.tokenizer = None\n",
    "            \n",
    "            LLMGPUMemoryManager.cleanup()\n",
    "            print(\"🧹 수정된 LLM 매니저 리소스 정리 완료\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 리소스 정리 오류: {e}\")\n",
    "\n",
    "print(\"✅ 수정된 OptimizedLLMManagerFixed 클래스 로딩 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🗄️ 5단계: 파일 저장관리자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM 타입별 분리 저장 시스템 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "class OptimizedLLMFileManager:\n",
    "    \"\"\"LLM 파이프라인용 파일 저장 관리자 (타입별 분리 저장)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_output_directories(base_output_dir=\"./llm_analysis_output\"):\n",
    "        \"\"\"출력 디렉토리 생성 (타입별 분리)\"\"\"\n",
    "        base_path = Path(base_output_dir)\n",
    "        directories = {\n",
    "            'base': base_path,\n",
    "            'markdown_text': base_path / \"markdown_text_result\",\n",
    "            'markdown_table': base_path / \"markdown_table_result\", \n",
    "            'json_text': base_path / \"json_text_result\",\n",
    "            'json_table': base_path / \"json_table_result\",\n",
    "            'analysis': base_path / \"analysis_results\",\n",
    "            'extraction': base_path / \"text_extraction_output\"\n",
    "        }\n",
    "        \n",
    "        # 디렉토리 생성\n",
    "        for dir_name, dir_path in directories.items():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"📁 LLM 출력 디렉토리 생성 완료: {base_path}\")\n",
    "        print(f\"   📝 텍스트 마크다운: {directories['markdown_text'].name}\")\n",
    "        print(f\"   📊 테이블 마크다운: {directories['markdown_table'].name}\")\n",
    "        print(f\"   📝 텍스트 JSON: {directories['json_text'].name}\")\n",
    "        print(f\"   📊 테이블 JSON: {directories['json_table'].name}\")\n",
    "        return directories\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_analysis_results_by_type(results, output_dirs, filename_prefix=\"llm_analysis\"):\n",
    "        \"\"\"타입별로 분리하여 저장 (메인 함수)\"\"\"\n",
    "        # 타입별로 결과 분리\n",
    "        text_results = [r for r in results if r['type'] == 'text']\n",
    "        table_results = [r for r in results if r['type'] == 'table']\n",
    "        \n",
    "        saved_files = {}\n",
    "        \n",
    "        # 텍스트 결과 저장\n",
    "        if text_results:\n",
    "            saved_files['text_markdown'] = OptimizedLLMFileManager.save_text_results_as_markdown(\n",
    "                text_results, output_dirs, filename_prefix\n",
    "            )\n",
    "            saved_files['text_json'] = OptimizedLLMFileManager.save_text_results_as_json(\n",
    "                text_results, output_dirs, filename_prefix\n",
    "            )\n",
    "        \n",
    "        # 테이블 결과 저장\n",
    "        if table_results:\n",
    "            saved_files['table_markdown'] = OptimizedLLMFileManager.save_table_results_as_markdown(\n",
    "                table_results, output_dirs, filename_prefix\n",
    "            )\n",
    "            saved_files['table_json'] = OptimizedLLMFileManager.save_table_results_as_json(\n",
    "                table_results, output_dirs, filename_prefix\n",
    "            )\n",
    "        \n",
    "        return saved_files\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_text_results_as_markdown(text_results, output_dirs, filename_prefix=\"llm_text_analysis\"):\n",
    "        \"\"\"텍스트 분석 결과를 마크다운으로 저장\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_text_{timestamp}.md\"\n",
    "            output_file = output_dirs['markdown_text'] / filename\n",
    "            \n",
    "            # 마크다운 콘텐츠 구성\n",
    "            content_parts = [\n",
    "                \"# 📝 LLM 텍스트 분석 결과\\n\\n\",\n",
    "                f\"**생성 시간:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                f\"**총 텍스트 분석:** {len(text_results)}개\\n\\n\",\n",
    "                \"---\\n\\n\"\n",
    "            ]\n",
    "            \n",
    "            # 각 텍스트 분석 결과 추가\n",
    "            for i, result in enumerate(text_results, 1):\n",
    "                content_type = result.get('content_type', '일반 텍스트')\n",
    "                content_parts.extend([\n",
    "                    f\"## 📝 텍스트 분석 {i}: {content_type}\\n\\n\",\n",
    "                    f\"**📁 원본 파일:** {result.get('source_file', 'unknown')}\\n\",\n",
    "                    f\"**📋 콘텐츠 타입:** {content_type}\\n\",\n",
    "                    f\"**🕒 처리 시간:** {result.get('processed_at', 'unknown')}\\n\\n\",\n",
    "                    f\"### 📄 원본 내용\\n\",\n",
    "                    f\"```\\n{result['original_content'][:1000]}{'...' if len(result['original_content']) > 500 else ''}\\n```\\n\\n\",\n",
    "                    f\"### 🔍 분석 결과\\n\",\n",
    "                    f\"{result['analysis']}\\n\\n\"\n",
    "                ])\n",
    "                \n",
    "                # Hypothetical Questions 추가\n",
    "                questions = result.get('hypothetical_questions', [])\n",
    "                if questions:\n",
    "                    content_parts.append(\"### ❓ Hypothetical Questions\\n\")\n",
    "                    for j, question in enumerate(questions, 1):\n",
    "                        content_parts.append(f\"{j}. {question}\\n\")\n",
    "                    content_parts.append(\"\\n\")\n",
    "                \n",
    "                content_parts.append(\"---\\n\\n\")\n",
    "            \n",
    "            # 파일 저장\n",
    "            output_file.write_text(''.join(content_parts), encoding='utf-8')\n",
    "            \n",
    "            print(f\"✅ 텍스트 마크다운 저장: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 텍스트 마크다운 저장 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_table_results_as_markdown(table_results, output_dirs, filename_prefix=\"llm_table_analysis\"):\n",
    "        \"\"\"테이블 분석 결과를 마크다운으로 저장\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_table_{timestamp}.md\"\n",
    "            output_file = output_dirs['markdown_table'] / filename\n",
    "            \n",
    "            # 마크다운 콘텐츠 구성\n",
    "            content_parts = [\n",
    "                \"# 📊 LLM 테이블 분석 결과\\n\\n\",\n",
    "                f\"**생성 시간:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                f\"**총 테이블 분석:** {len(table_results)}개\\n\\n\",\n",
    "                \"---\\n\\n\"\n",
    "            ]\n",
    "            \n",
    "            # 각 테이블 분석 결과 추가\n",
    "            for i, result in enumerate(table_results, 1):\n",
    "                content_type = result.get('content_type', '데이터 테이블')\n",
    "                content_parts.extend([\n",
    "                    f\"## 📊 테이블 분석 {i}: {content_type}\\n\\n\",\n",
    "                    f\"**📁 원본 파일:** {result.get('source_file', 'unknown')}\\n\",\n",
    "                    f\"**📋 콘텐츠 타입:** {content_type}\\n\",\n",
    "                    f\"**🕒 처리 시간:** {result.get('processed_at', 'unknown')}\\n\\n\",\n",
    "                    f\"### 📄 원본 테이블\\n\",\n",
    "                    f\"```\\n{result['original_content'][:1000]}{'...' if len(result['original_content']) > 500 else ''}\\n```\\n\\n\",\n",
    "                    f\"### 🔍 분석 결과\\n\",\n",
    "                    f\"{result['analysis']}\\n\\n\"\n",
    "                ])\n",
    "                \n",
    "                # Hypothetical Questions 추가\n",
    "                questions = result.get('hypothetical_questions', [])\n",
    "                if questions:\n",
    "                    content_parts.append(\"### ❓ Hypothetical Questions\\n\")\n",
    "                    for j, question in enumerate(questions, 1):\n",
    "                        content_parts.append(f\"{j}. {question}\\n\")\n",
    "                    content_parts.append(\"\\n\")\n",
    "                \n",
    "                content_parts.append(\"---\\n\\n\")\n",
    "            \n",
    "            # 파일 저장\n",
    "            output_file.write_text(''.join(content_parts), encoding='utf-8')\n",
    "            \n",
    "            print(f\"✅ 테이블 마크다운 저장: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테이블 마크다운 저장 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_text_results_as_json(text_results, output_dirs, filename_prefix=\"llm_text_analysis\"):\n",
    "        \"\"\"텍스트 분석 결과를 JSON으로 저장\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_text_{timestamp}.json\"\n",
    "            output_file = output_dirs['json_text'] / filename\n",
    "            \n",
    "            # JSON 데이터 구성\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"generated_at\": datetime.now().isoformat(),\n",
    "                    \"content_type\": \"text_analysis\",\n",
    "                    \"total_text_analyses\": len(text_results),\n",
    "                    \"format_version\": \"1.0\",\n",
    "                    \"pipeline\": \"LLM_text_analysis\"\n",
    "                },\n",
    "                \"text_analysis_results\": text_results\n",
    "            }\n",
    "            \n",
    "            # JSON 파일 저장\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"✅ 텍스트 JSON 저장: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 텍스트 JSON 저장 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_table_results_as_json(table_results, output_dirs, filename_prefix=\"llm_table_analysis\"):\n",
    "        \"\"\"테이블 분석 결과를 JSON으로 저장\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_table_{timestamp}.json\"\n",
    "            output_file = output_dirs['json_table'] / filename\n",
    "            \n",
    "            # JSON 데이터 구성\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"generated_at\": datetime.now().isoformat(),\n",
    "                    \"content_type\": \"table_analysis\", \n",
    "                    \"total_table_analyses\": len(table_results),\n",
    "                    \"format_version\": \"1.0\",\n",
    "                    \"pipeline\": \"LLM_table_analysis\"\n",
    "                },\n",
    "                \"table_analysis_results\": table_results\n",
    "            }\n",
    "            \n",
    "            # JSON 파일 저장\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"✅ 테이블 JSON 저장: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테이블 JSON 저장 실패: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"✅ LLM 타입별 분리 저장 시스템 로딩 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 재귀적 파일 탐색색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_main_pipeline_complete_recursive_fixed(\n",
    "    markdown_files,  # 파일 리스트를 직접 받음\n",
    "    output_dir=\"./llm_analysis_output\",\n",
    "    model_name=\"google/gemma-3-4b-it\"\n",
    "):\n",
    "    \"\"\"\n",
    "    🚀 완전한 LLM 텍스트/테이블 분석 파이프라인 (모든 오류 수정 버전)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"🚀 LLM 텍스트/테이블 분석 파이프라인 시작 (수정 버전)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 1. GPU 메모리 최적화\n",
    "        print(\"\\n1️⃣ GPU 메모리 최적화...\")\n",
    "        LLMGPUMemoryManager.cleanup()  \n",
    "        LLMGPUMemoryManager.print_status()\n",
    "        \n",
    "        # 2. 출력 디렉토리 설정\n",
    "        print(\"\\n2️⃣ 출력 디렉토리 설정...\")\n",
    "        output_dirs = OptimizedLLMFileManager.create_output_directories(output_dir)\n",
    "        \n",
    "        # 3. ✅ 기존 메서드를 이용한 텍스트 및 테이블 추출\n",
    "        print(\"\\n3️⃣ 마크다운 파일 처리...\")\n",
    "        text_processor = OptimizedTextTableProcessor()\n",
    "        all_text_chunks = []\n",
    "        all_tables = []\n",
    "        \n",
    "        for md_file in markdown_files:\n",
    "            print(f\"📄 처리 중: {md_file}\")\n",
    "            \n",
    "            # 파일 읽기\n",
    "            try:\n",
    "                with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 파일 읽기 오류 ({md_file}): {e}\")\n",
    "                continue\n",
    "            \n",
    "            # ✅ 기존 메서드 사용: 파일 타입에 따라 처리 분기\n",
    "            file_path = Path(md_file)\n",
    "            \n",
    "            # 파일 경로로 텍스트/테이블 구분 (ex_text vs ex_table)\n",
    "            if \"ex_text\" in str(file_path) or \"text\" in file_path.name.lower():\n",
    "                # 📝 텍스트 파일 처리\n",
    "                print(f\"📝 텍스트 파일로 처리: {file_path.name}\")\n",
    "                \n",
    "                # 기존 메서드: 테이블 라인 제거\n",
    "                clean_text = text_processor.extract_clean_text(content)\n",
    "                \n",
    "                if clean_text.strip():\n",
    "                    # 기존 메서드: Kiwi 기반 청킹\n",
    "                    text_chunks = text_processor._intelligent_text_chunking(\n",
    "                        clean_text, str(md_file)\n",
    "                    )\n",
    "                    all_text_chunks.extend(text_chunks)\n",
    "                    \n",
    "            elif \"ex_table\" in str(file_path) or \"table\" in file_path.name.lower():\n",
    "                # 📊 테이블 파일 처리\n",
    "                print(f\"📊 테이블 파일로 처리: {file_path.name}\")\n",
    "                \n",
    "                # 기존 메서드와 동일한 구조로 테이블 데이터 생성\n",
    "                table_data = {\n",
    "                    'content': content,\n",
    "                    'source_file': str(md_file),\n",
    "                    'type': 'table',\n",
    "                    'filename': file_path.name\n",
    "                }\n",
    "                all_tables.append(table_data)\n",
    "                \n",
    "            else:\n",
    "                # 🔍 파일명으로 구분이 안 되는 경우, 내용으로 판단\n",
    "                print(f\"🔍 내용 분석하여 처리: {file_path.name}\")\n",
    "                \n",
    "                # 테이블 마커가 많으면 테이블로, 아니면 텍스트로 처리\n",
    "                table_line_count = len([line for line in content.split('\\n') if '|' in line])\n",
    "                total_lines = len(content.split('\\n'))\n",
    "                \n",
    "                if table_line_count > total_lines * 0.3:  # 30% 이상이 테이블 라인\n",
    "                    # 테이블로 처리\n",
    "                    table_data = {\n",
    "                        'content': content,\n",
    "                        'source_file': str(md_file),\n",
    "                        'type': 'table',\n",
    "                        'filename': file_path.name\n",
    "                    }\n",
    "                    all_tables.append(table_data)\n",
    "                else:\n",
    "                    # 텍스트로 처리\n",
    "                    clean_text = text_processor.extract_clean_text(content)\n",
    "                    if clean_text.strip():\n",
    "                        text_chunks = text_processor._intelligent_text_chunking(\n",
    "                            clean_text, str(md_file)\n",
    "                        )\n",
    "                        all_text_chunks.extend(text_chunks)\n",
    "        \n",
    "        print(f\"✅ 텍스트 청크: {len(all_text_chunks)}개, 테이블: {len(all_tables)}개\")\n",
    "        \n",
    "        if not all_text_chunks and not all_tables:\n",
    "            print(\"❌ 추출된 콘텐츠가 없습니다\")\n",
    "            return None\n",
    "        \n",
    "        # 4. LLM 모델 로딩 (안전한 로딩)\n",
    "        print(\"\\n4️⃣ LLM 모델 로딩...\")\n",
    "        llm_generator = OptimizedLLMManager(model_name)\n",
    "        \n",
    "        # 모델 로딩 확인\n",
    "        print(\"🔧 모델 로딩 상태 확인...\")\n",
    "        load_success = llm_generator.load_model()\n",
    "        if not load_success:\n",
    "            print(\"❌ LLM 모델 로딩 실패\")\n",
    "            return None\n",
    "        \n",
    "        print(\"✅ LLM 모델 로딩 성공!\")\n",
    "        \n",
    "        # 5. 배치 처리\n",
    "        print(\"\\n5️⃣ LLM 분석 시작...\")\n",
    "        analysis_results = llm_generator.process_batch(all_text_chunks, all_tables)\n",
    "        \n",
    "        if not analysis_results:\n",
    "            print(\"❌ 분석 결과가 없습니다\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"✅ 분석 완료: {len(analysis_results)}개 결과\")\n",
    "        \n",
    "        # 6. 결과 저장 (수정된 메서드명 사용)\n",
    "        print(\"\\n6️⃣ 결과 저장...\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 마크다운 저장 (올바른 메서드명과 호출 방식)\n",
    "        try:\n",
    "            md_saved = OptimizedLLMFileManager.save_analysis_results_as_markdown(\n",
    "                analysis_results, output_dirs, f\"llm_recursive_analysis_{timestamp}\"\n",
    "            )\n",
    "            print(f\"✅ 마크다운 저장: {md_saved}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 마크다운 저장 실패: {e}\")\n",
    "            md_saved = None\n",
    "        \n",
    "        # JSON 저장\n",
    "        try:\n",
    "            json_saved = OptimizedLLMFileManager.save_analysis_results_as_json(\n",
    "                analysis_results, output_dirs, f\"llm_recursive_analysis_{timestamp}\"\n",
    "            )\n",
    "            print(f\"✅ JSON 저장: {json_saved}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ JSON 저장 실패: {e}\")\n",
    "            json_saved = None\n",
    "        \n",
    "        # 8. 성능 통계\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"\\n🎯 파이프라인 완료!\")\n",
    "        print(f\"⏱️ 총 소요 시간: {elapsed_time:.2f}초\")\n",
    "        print(f\"📊 처리된 항목: {len(analysis_results)}개\")\n",
    "        print(f\"💾 저장 위치: {output_dir}\")\n",
    "        \n",
    "        return {\n",
    "            'text_chunks': all_text_chunks,\n",
    "            'tables': all_tables,\n",
    "            'analysis_results': analysis_results,\n",
    "            'saved_files': {'markdown': md_saved, 'json': json_saved}\n",
    "        }\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파이프라인 실행 오류: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vlm 파이프라인 단독 실행행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "📋 execute_vlm_pipeline_only()\n",
    "│\n",
    "├── 📁 Path(\"./data/images\")                            ✅ pathlib 표준라이브러리\n",
    "│   ├── exists()                                        ✅ pathlib 메서드\n",
    "│   └── rglob('*')                                      ✅ pathlib 메서드\n",
    "│\n",
    "├── 🖼️ gpu_optimized_main_pipeline()                    ✅ Line 3291에서 정의됨\n",
    "│   ├── AdvancedGPUManager()                            ✅ Line 2434에서 정의됨\n",
    "│   │   ├── cleanup_memory(intensive=True)              ✅ 클래스 내 메서드\n",
    "│   │   └── print_detailed_status()                     ✅ 클래스 내 메서드\n",
    "│   │\n",
    "│   ├── MemoryEfficientBatchProcessor()                 ✅ Line 2619에서 정의됨  \n",
    "│   │   ├── __init__(initial_batch_size=2)              ✅ 클래스 내 메서드\n",
    "│   │   └── process_with_adaptive_batching()            ✅ 클래스 내 메서드\n",
    "│   │       ├── torch.cuda.get_device_properties()      ✅ PyTorch CUDA\n",
    "│   │       ├── torch.cuda.memory_allocated()           ✅ PyTorch CUDA\n",
    "│   │       └── psutil.virtual_memory()                 ✅ psutil 라이브러리\n",
    "│   │\n",
    "│   ├── OptimizedFileManager.create_output_directories() ✅ Line 2914에서 정의됨\n",
    "│   │   └── Path.mkdir(parents=True, exist_ok=True)     ✅ pathlib 표준라이브러리\n",
    "│   │\n",
    "│   ├── load_model()                                    ✅ Line 3230에서 정의됨 (VLM용)\n",
    "│   │   ├── Qwen2_5_VLForConditionalGeneration.from_pretrained() ✅ transformers\n",
    "│   │   ├── AutoProcessor.from_pretrained()             ✅ transformers  \n",
    "│   │   ├── torch.bfloat16                              ✅ PyTorch dtype\n",
    "│   │   ├── device_map=\"auto\"                           ✅ transformers 설정\n",
    "│   │   └── torch.cuda.empty_cache()                    ✅ PyTorch\n",
    "│   │\n",
    "│   └── process_images_with_adaptive_batching()         ✅ 추정 - 배치 처리 함수\n",
    "│       ├── analyze_image()                             ✅ 추정 - 이미지 분석 함수\n",
    "│       │   ├── processor()                             ✅ AutoProcessor 메서드\n",
    "│       │   ├── model.generate()                        ✅ Transformers 모델 메서드\n",
    "│       │   │   ├── max_new_tokens=512                  ✅ 생성 파라미터\n",
    "│       │   │   ├── temperature=0.7                     ✅ 생성 파라미터\n",
    "│       │   │   └── do_sample=True                      ✅ 생성 파라미터\n",
    "│       │   └── processor.decode()                      ✅ AutoProcessor 메서드\n",
    "│       └── chunk_analysis_results()                    ✅ 추정 - 결과 청킹 함수\n",
    "│           ├── json.dumps()                            ✅ json 표준라이브러리\n",
    "│           └── Path.write_text()                       ✅ pathlib 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ VLM 파이프라인을 실행합니다...\n",
      "🎬 VLM 파이프라인 단독 실행 시작\n",
      "==================================================\n",
      "\n",
      "🖼️ VLM 파이프라인: 이미지 분석\n",
      "📁 입력: ./data/images\n",
      "📁 출력: ./analysis_output\n",
      "📷 발견된 이미지: 46개\n",
      "============================================================\n",
      "🚀 GPU 최적화 MultiModal RAG 시스템\n",
      "============================================================\n",
      "🔍 초기 GPU 상태:\n",
      "\n",
      "============================================================\n",
      "🖥️ GPU 상태 모니터링\n",
      "============================================================\n",
      "🎯 GPU 코어 활용률: 0%\n",
      "📊 메모리 대역폭 활용률: 1%\n",
      "💾 메모리 사용량: 0.3GB / 24.0GB (1.4%)\n",
      "🆓 사용 가능 메모리: 24.0GB\n",
      "🔥 PyTorch 할당됨: 0.0GB\n",
      "📦 PyTorch 예약됨: 0.0GB\n",
      "⚡ GPU 활용률이 낮습니다 - 배치 크기 증가 권장\n",
      "✅ 메모리 여유 충분\n",
      "============================================================\n",
      "📁 출력 디렉토리 생성 완료: analysis_output\n",
      "📷 발견된 이미지: 46개\n",
      "🔄 Qwen/Qwen2.5-VL-7B-Instruct 모델 로딩 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4010204795040c19521329933a876bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 최적화 완료 (dtype: torch.bfloat16)\n",
      "✅ 모델 로딩 완료! 디바이스: GPU (NVIDIA GeForce RTX 3090)\n",
      "\n",
      "============================================================\n",
      "🖥️ GPU 상태 모니터링\n",
      "============================================================\n",
      "🎯 GPU 코어 활용률: 0%\n",
      "📊 메모리 대역폭 활용률: 0%\n",
      "💾 메모리 사용량: 16.0GB / 24.0GB (66.8%)\n",
      "🆓 사용 가능 메모리: 8.6GB\n",
      "🔥 PyTorch 할당됨: 15.4GB\n",
      "📦 PyTorch 예약됨: 15.4GB\n",
      "⚡ GPU 활용률이 낮습니다 - 배치 크기 증가 권장\n",
      "============================================================\n",
      "🔄 GPU 최적화 이미지 분석 시작...\n",
      "🚀 효율적 배치 처리 시작: 46개 아이템\n",
      "🔧 배치 크기 조정: 2 → 8\n",
      "📦 배치 처리 중: 1-8/46 (크기: 8)\n",
      "📦 배치 처리 중: 9-16/46 (크기: 8)\n",
      "📦 배치 처리 중: 17-24/46 (크기: 8)\n",
      "📦 배치 처리 중: 25-32/46 (크기: 8)\n",
      "📦 배치 처리 중: 33-40/46 (크기: 8)\n",
      "📊 진행률: 87.0% (40/46) | 처리율: 5.1개/분\n",
      "\n",
      "============================================================\n",
      "🖥️ GPU 상태 모니터링\n",
      "============================================================\n",
      "🎯 GPU 코어 활용률: 65%\n",
      "📊 메모리 대역폭 활용률: 55%\n",
      "💾 메모리 사용량: 16.1GB / 24.0GB (67.1%)\n",
      "🆓 사용 가능 메모리: 8.5GB\n",
      "🔥 PyTorch 할당됨: 15.5GB\n",
      "📦 PyTorch 예약됨: 15.5GB\n",
      "============================================================\n",
      "📦 배치 처리 중: 41-46/46 (크기: 6)\n",
      "\n",
      "==================================================\n",
      "📊 배치 처리 완료 요약\n",
      "==================================================\n",
      "📷 총 처리 아이템: 46/46\n",
      "📦 성공한 배치: 6\n",
      "💥 OOM 발생 횟수: 0\n",
      "🔧 배치 크기 조정: 1회\n",
      "🧹 메모리 정리 횟수: 6\n",
      "⚡ 최종 배치 크기: 8\n",
      "⏱️ 총 처리 시간: 9.0분\n",
      "📈 평균 처리율: 5.1개/분\n",
      "==================================================\n",
      "\n",
      "✅ 분석 완료! 성공: 46/46개\n",
      "💾 결과 저장 중...\n",
      "✅ 마크다운 저장: gpu_optimized_analysis_20250623_064538_20250623_064538.md\n",
      "✅ JSON 저장: gpu_optimized_analysis_20250623_064538_20250623_064538.json\n",
      "\n",
      "📊 GPU 최적화 처리 요약:\n",
      "   📷 처리된 이미지: 46개\n",
      "   ⏱️ 총 소요 시간: 9.0분\n",
      "   ⚡ 평균 처리 속도: 5.1개/분\n",
      "   📁 출력 경로: analysis_output\n",
      "\n",
      "🔍 최종 GPU 상태:\n",
      "\n",
      "============================================================\n",
      "🖥️ GPU 상태 모니터링\n",
      "============================================================\n",
      "🎯 GPU 코어 활용률: 20%\n",
      "📊 메모리 대역폭 활용률: 15%\n",
      "💾 메모리 사용량: 16.1GB / 24.0GB (67.1%)\n",
      "🆓 사용 가능 메모리: 8.5GB\n",
      "🔥 PyTorch 할당됨: 15.5GB\n",
      "📦 PyTorch 예약됨: 15.5GB\n",
      "⚡ GPU 활용률이 낮습니다 - 배치 크기 증가 권장\n",
      "============================================================\n",
      "\n",
      "✅ VLM 파이프라인 완료!\n",
      "📊 처리된 이미지: 46개\n",
      "\n",
      "📋 VLM 파이프라인 결과: VLM 파이프라인 성공적으로 완료\n"
     ]
    }
   ],
   "source": [
    "# 🖼️ VLM 파이프라인 단독 실행\n",
    "def execute_vlm_pipeline_only():\n",
    "    \"\"\"VLM 파이프라인만 별도로 실행 (이미지 분석)\"\"\"\n",
    "    \n",
    "    print(\"🎬 VLM 파이프라인 단독 실행 시작\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # 📁 입력 디렉토리 존재 여부 사전 체크\n",
    "        image_dir = Path(\"./data/images\")\n",
    "        if not image_dir.exists():\n",
    "            return {\n",
    "                'success': False,\n",
    "                'vlm_result': None,\n",
    "                'message': f'입력 디렉토리가 존재하지 않음: {image_dir}'\n",
    "            }\n",
    "        \n",
    "        # 이미지 파일 존재 여부 체크\n",
    "        image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "        image_files = [\n",
    "            path for path in image_dir.rglob('*')\n",
    "            if path.suffix.lower() in image_extensions and path.is_file()\n",
    "        ]\n",
    "        \n",
    "        if not image_files:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'vlm_result': None,\n",
    "                'message': f'이미지 파일이 없음: {image_dir} (*.jpg, *.jpeg, *.png)'\n",
    "            }\n",
    "        \n",
    "        # VLM 파이프라인 실행\n",
    "        print(\"\\n🖼️ VLM 파이프라인: 이미지 분석\")\n",
    "        print(\"📁 입력: ./data/images\")\n",
    "        print(\"📁 출력: ./analysis_output\")\n",
    "        print(f\"📷 발견된 이미지: {len(image_files)}개\")\n",
    "        \n",
    "        vlm_result = gpu_optimized_main_pipeline('./data/images', './analysis_output')\n",
    "        \n",
    "       \n",
    "        if vlm_result:\n",
    "            print(f\"\\n✅ VLM 파이프라인 완료!\")\n",
    "            print(f\"📊 처리된 이미지: {len(image_files)}개\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'vlm_result': vlm_result,\n",
    "                'total_images_processed': len(image_files),\n",
    "                'message': 'VLM 파이프라인 성공적으로 완료'\n",
    "            }\n",
    "        else:\n",
    "            print(\"❌ VLM 파이프라인 결과가 없습니다\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'vlm_result': None,\n",
    "                'message': 'VLM 파이프라인 실행 실패'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ VLM 파이프라인 실행 오류: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'success': False,\n",
    "            'vlm_result': None,\n",
    "            'message': f'오류 발생: {str(e)}'\n",
    "        }\n",
    "\n",
    "# VLM 파이프라인 실행\n",
    "print(\"🖼️ VLM 파이프라인을 실행합니다...\")\n",
    "vlm_pipeline_result = execute_vlm_pipeline_only()\n",
    "print(f\"\\n📋 VLM 파이프라인 결과: {vlm_pipeline_result.get('message', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm 파이프라인 단독 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '📄' (U+1F4C4) (3113715157.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m📄 파일 탐색: ./data/ex_text 디렉토리에서 .md 파일 스캔\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '📄' (U+1F4C4)\n"
     ]
    }
   ],
   "source": [
    "워크플로우 단계:\n",
    "📄 파일 탐색: ./data/ex_text 디렉토리에서 .md 파일 스캔\n",
    "📁 디렉토리 생성: 타입별 분리 저장을 위한 출력 디렉토리 생성\n",
    "🤖 모델 로딩: Gemma-3-1B-IT LLM 모델 초기화\n",
    "📖 텍스트 추출: 마크다운 파일에서 텍스트와 테이블 분리\n",
    "🔄 LLM 분석: 배치 처리로 텍스트/테이블 분석\n",
    "💾 분리 저장: 텍스트/테이블 결과를 타입별로 분리 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 수정된 execute_llm_pipelined_final_fixed 함수 로딩 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def execute_llm_pipeline():\n",
    "    \"\"\"최종 수정된 성능 최적화 LLM 파이프라인 - 모든 문제 해결 버전\"\"\"\n",
    "    \n",
    "    print(\"🛠️ 최종 수정된 LLM 파이프라인 실행 시작! (모든 문제 해결)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. 파일 디렉토리 존재 여부 사전 체크\n",
    "        markdown_dir = Path(\"./data/ex_text\")\n",
    "        \n",
    "        if not markdown_dir.exists():\n",
    "            return {\n",
    "                'success': False, \n",
    "                'result': None, \n",
    "                'message': f'디렉토리가 존재하지 않음: {markdown_dir}'\n",
    "            }\n",
    "        \n",
    "        # 마크다운 파일 존재 여부 체크\n",
    "        markdown_files = list(markdown_dir.rglob(\"*.md\"))\n",
    "        if not markdown_files:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'result': None,\n",
    "                'message': f'마크다운 파일이 없음: {markdown_dir}'\n",
    "            }\n",
    "        \n",
    "        print(f\"📄 처리할 디렉토리: {markdown_dir}\")\n",
    "        print(f\"📝 발견된 마크다운 파일: {len(markdown_files)}개\")\n",
    "        \n",
    "        # 2. 출력 디렉토리 생성\n",
    "        output_dirs = OptimizedLLMFileManager.create_output_directories(\"./llm_analysis_output\")\n",
    "        \n",
    "        print(\"🔍 생성된 출력 디렉토리 구조:\")\n",
    "        for key, value in output_dirs.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # 3. ✅ 수정된 LLM 매니저 사용\n",
    "        print(\"🤖 수정된 LLM 모델 로딩 중...\")\n",
    "        llm_manager = OptimizedLLMManager()  # ← 수정된 클래스 사용\n",
    "        if not llm_manager.load_model():\n",
    "            return {\n",
    "                'success': False, \n",
    "                'result': None, \n",
    "                'message': '모델 로딩 실패'\n",
    "            }\n",
    "        \n",
    "        # 4. ✅ 수정된 텍스트 추출기 초기화\n",
    "        text_extractor = OptimizedTextTableProcessor()\n",
    "        \n",
    "        # 5. ✅ 개선된 디렉토리 분리 처리\n",
    "        print(\"📖 개선된 텍스트/테이블 분리 처리 시작...\")\n",
    "        \n",
    "        text_dir = Path(\"./data/ex_text\")      # 텍스트 디렉토리\n",
    "        table_dir = Path(\"./data/ex_table\")    # 테이블 디렉토리\n",
    "        \n",
    "        # 출력 디렉토리 설정\n",
    "        if 'base' in output_dirs:\n",
    "            extraction_output_dir = output_dirs['base']\n",
    "        elif 'markdown_text' in output_dirs:\n",
    "            extraction_output_dir = output_dirs['markdown_text']\n",
    "        else:\n",
    "            extraction_output_dir = list(output_dirs.values())[0]\n",
    "            \n",
    "        extraction_result = text_extractor.process_markdown_file(\n",
    "            text_dir, table_dir, extraction_output_dir\n",
    "        )\n",
    "        \n",
    "        # 결과 확인\n",
    "        if not extraction_result:\n",
    "            return {\n",
    "                'success': False, \n",
    "                'result': None, \n",
    "                'message': '파일 처리 실패'\n",
    "            }\n",
    "        \n",
    "        all_text_chunks = extraction_result.get('text_chunks', [])\n",
    "        all_tables = extraction_result.get('tables', [])\n",
    "        \n",
    "        print(f\"📝 총 텍스트 청크: {len(all_text_chunks)}개\")\n",
    "        print(f\"📊 총 테이블: {len(all_tables)}개\")\n",
    "        \n",
    "        # 6. ✅ 배치 크기 동적 조정 및 수정된 process_batch 호출\n",
    "        print(\"🤖 최적화된 LLM 분석 시작...\")\n",
    "        \n",
    "        total_items = len(all_text_chunks) + len(all_tables)\n",
    "        if total_items == 0:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'result': None,\n",
    "                'message': '처리할 텍스트/테이블이 없습니다'\n",
    "            }\n",
    "        \n",
    "        if total_items > 100:\n",
    "            batch_size = 8\n",
    "        elif total_items > 50:\n",
    "            batch_size = 4\n",
    "        else:\n",
    "            batch_size = 2\n",
    "            \n",
    "        print(f\"📦 동적 배치 크기: {batch_size} (총 {total_items}개 항목)\")\n",
    "        \n",
    "        # ✅ 수정된 process_batch 메서드 호출\n",
    "        analysis_results = llm_manager.process_batch(all_text_chunks, all_tables, batch_size)\n",
    "        \n",
    "        # 7. ✅ 수정된 저장 로직 (타입 키 일치 처리)\n",
    "        print(\"💾 분석 결과 저장 시작...\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 🔧 분석 결과 데이터 구조 수정 (original_content, analysis 키 추가)\n",
    "        print(\"🔧 분석 결과 데이터 구조 수정 중...\")\n",
    "        for result in analysis_results:\n",
    "            # original_content 키 추가 (마크다운 저장에 필요)\n",
    "            if 'original_content' not in result:\n",
    "                result['original_content'] = result.get('text_content', '')\n",
    "            \n",
    "            # ✅ type 키는 이미 올바르게 설정됨 (수정된 process_batch에서)\n",
    "            # 기존의 강제 'text' 설정 제거\n",
    "            \n",
    "            # analysis 키 추가 (마크다운 저장에 필요)\n",
    "            if 'analysis' not in result:\n",
    "                result['analysis'] = result.get('analysis_result', '')\n",
    "        \n",
    "        print(f\"🔧 {len(analysis_results)}개 결과 데이터 구조 수정 완료\")\n",
    "        \n",
    "        # 타입별 분포 확인\n",
    "        text_count = len([r for r in analysis_results if r.get('type') == 'text'])\n",
    "        table_count = len([r for r in analysis_results if r.get('type') == 'table'])\n",
    "        print(f\"📊 타입별 분포: 텍스트 {text_count}개, 테이블 {table_count}개\")\n",
    "        \n",
    "        # ✅ 타입별 분리 저장 사용\n",
    "        saved_files = OptimizedLLMFileManager.save_analysis_results_by_type(\n",
    "            analysis_results, output_dirs, f\"llm_analysis_final_{timestamp}\"\n",
    "        )\n",
    "        \n",
    "        # 결과 통합\n",
    "        md_saved = saved_files.get('text_markdown') or saved_files.get('table_markdown')\n",
    "        json_saved = saved_files.get('text_json') or saved_files.get('table_json')\n",
    "        \n",
    "        print(f\"✅ 저장된 파일:\")\n",
    "        print(f\"   📝 마크다운: {md_saved}\")\n",
    "        print(f\"   📊 JSON: {json_saved}\")\n",
    "        \n",
    "        # 모든 저장된 파일 표시\n",
    "        if saved_files:\n",
    "            print(f\"   📄 전체 저장 파일:\")\n",
    "            for file_type, file_path in saved_files.items():\n",
    "                if file_path:\n",
    "                    print(f\"      {file_type}: {file_path}\")\n",
    "        \n",
    "        # ✅ 최종 반환값\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'total_files_processed': len(markdown_files),\n",
    "            'total_text_chunks': len(all_text_chunks),\n",
    "            'total_tables': len(all_tables),\n",
    "            'total_analysis_results': len(analysis_results),\n",
    "            'text_results_count': text_count,\n",
    "            'table_results_count': table_count,\n",
    "            'saved_files': {\n",
    "                'markdown': md_saved,\n",
    "                'json': json_saved,\n",
    "                'all_saved_files': saved_files\n",
    "            },\n",
    "            'message': '모든 문제 해결 완료!'\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파이프라인 실행 오류: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'success': False, \n",
    "            'result': None, \n",
    "            'message': f'오류 발생: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # 리소스 정리\n",
    "        if 'llm_manager' in locals():\n",
    "            if hasattr(llm_manager, 'cleanup'):\n",
    "                llm_manager.cleanup()\n",
    "            else:\n",
    "                print(\"🧹 LLM 매니저 리소스 정리 (cleanup 메서드 없음)\")\n",
    "\n",
    "print(\"✅ 최종 수정된 execute_llm_pipelined_final_fixed 함수 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 수정된 LLM 파이프라인을 실행합니다... (모든 문제 해결)\n",
      "============================================================\n",
      "🛠️ 최종 수정된 LLM 파이프라인 실행 시작! (모든 문제 해결)\n",
      "============================================================\n",
      "📄 처리할 디렉토리: data/ex_text\n",
      "📝 발견된 마크다운 파일: 9개\n",
      "📁 LLM 출력 디렉토리 생성 완료: llm_analysis_output\n",
      "   📝 텍스트 마크다운: markdown_text_result\n",
      "   📊 테이블 마크다운: markdown_table_result\n",
      "   📝 텍스트 JSON: json_text_result\n",
      "   📊 테이블 JSON: json_table_result\n",
      "🔍 생성된 출력 디렉토리 구조:\n",
      "   base: llm_analysis_output\n",
      "   markdown_text: llm_analysis_output/markdown_text_result\n",
      "   markdown_table: llm_analysis_output/markdown_table_result\n",
      "   json_text: llm_analysis_output/json_text_result\n",
      "   json_table: llm_analysis_output/json_table_result\n",
      "   analysis: llm_analysis_output/analysis_results\n",
      "   extraction: llm_analysis_output/text_extraction_output\n",
      "🤖 수정된 LLM 모델 로딩 중...\n",
      "🔄 google/gemma-3-4b-it 모델 로딩 중...\n",
      "🧹 GPU 메모리 정리 완료\n",
      "📝 토크나이저 로딩...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4787dc4642a4c58a19be8b2135545d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fcee45779b40e09bc3d0030dff8f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00cb9044a04194a6200ece5072d10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece70363969344a1920ab11975803897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dda92158d64227b60c5c624420b5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 모델 로딩...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e510e7cf42c49a9af4b3d750c2c967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc8a25c6f604b9fb18b3ef583695abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25721583acef4498ad18b7b4bafc87ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd8f8eb6f5a43a287e5c5a566e8aea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7147e615a045bf8a289cd0cc69da43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554ba58d9b924eb39a48a5fc5b6d3938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd934af6134f4ee5915bccfc5f72f0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ google/gemma-3-4b-it 로딩 완료!\n",
      "🎯 사용 디바이스: cuda\n",
      "🔄 Kiwi 형태소 분석기 초기화 중...\n",
      "✅ Kiwi 초기화 완료\n",
      "📖 개선된 텍스트/테이블 분리 처리 시작...\n",
      "📖 텍스트 파일 처리 시작...\n",
      "📁 9개의 텍스트 파일 발견\n",
      "🔄 텍스트 처리 중: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.md (1/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f.md\n",
      "📝 29개 문장 분리 완료\n",
      "✅ 6개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.md (2/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F.md\n",
      "📝 25개 문장 분리 완료\n",
      "✅ 5개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.md (3/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f.md\n",
      "📝 24개 문장 분리 완료\n",
      "✅ 4개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.md (4/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.06.04_[현지정보]_25년_6월_캐나다_중앙은행_정책회의_결과_및_시장_반응.md\n",
      "📝 22개 문장 분리 완료\n",
      "✅ 7개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.md (5/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.05.20_[현지정보]_미국_신용등급_하향_조정에_대한_시장참가자_평가.md\n",
      "📝 46개 문장 분리 완료\n",
      "✅ 9개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.md (6/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.05.19_(현지정보_20250516)_Moody’s社,_미국_신용등급_하향조정_f.md\n",
      "📝 5개 문장 분리 완료\n",
      "✅ 3개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.md (7/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응.md\n",
      "📝 24개 문장 분리 완료\n",
      "✅ 5개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.md (8/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.05.08_(현지정보_250507)_2025_5월_FOMC_시장반응_f.md\n",
      "📝 27개 문장 분리 완료\n",
      "✅ 6개 청크 생성 완료\n",
      "🔄 텍스트 처리 중: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.md (9/9)\n",
      "🧠 Kiwi 문장 분리 시작: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가.md\n",
      "📝 19개 문장 분리 완료\n",
      "✅ 5개 청크 생성 완료\n",
      "📊 테이블 파일 처리 시작...\n",
      "📊 HTML 테이블 처리 시작: data/ex_table\n",
      "🍲 HTML 테이블 감지: 등록일_2025.06.19_(현지정보_250618)_2025_6월_FOMC_시장반응_f-table-1.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F-table-1.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.06.12_[현지정보]_美_2025.5월_소비자물가_동향_및_금융시장_반응_F-table-2.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.06.09_(현지정보)_美_2025.5월_고용지표_내용_및_뉴욕_금융시장_반응_f-table-1.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응-table-1.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.14_[현지정보]_美_2025.4월_소비자물가_동향_및_금융시장_반응-table-2.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-1.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-2.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-3.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-4.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-5.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-6.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-7.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-8.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-9.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-10.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "🍲 HTML 테이블 감지: 등록일_2025.05.07_최근(2025.4월)의_미국경제_상황과_평가-table-11.md\n",
      "   ✅ HTML 테이블 1 변환 완료\n",
      "📊 총 17개 테이블 로드 완료\n",
      "\n",
      "======================================================================\n",
      "📊 Kiwi 기반 문장 분리 텍스트/테이블 추출 완료\n",
      "======================================================================\n",
      "📁 처리된 파일: 9개\n",
      "📊 로드된 테이블: 17개\n",
      "📝 생성된 텍스트 청크: 50개\n",
      "🧠 Kiwi 분석 문장: 221개\n",
      "📏 평균 청크 길이: 1004.42자\n",
      "⏱️ 처리 시간: 1.30초\n",
      "======================================================================\n",
      "🎯 Kiwi 형태소 분석기 전용 문장 분리 청킹\n",
      "✅ 높은 정확도의 문장 경계 탐지\n",
      "======================================================================\n",
      "📝 총 텍스트 청크: 50개\n",
      "📊 총 테이블: 17개\n",
      "🤖 최적화된 LLM 분석 시작...\n",
      "📦 동적 배치 크기: 4 (총 67개 항목)\n",
      "\n",
      "🚀 수정된 배치 처리 시작!\n",
      "📦 텍스트 청크: 50개\n",
      "📊 테이블: 17개\n",
      "🔧 배치 크기: 4\n",
      "==================================================\n",
      "📊 총 처리할 항목: 67개\n",
      "🔄 배치 1/17: 4개 항목 처리 중...\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 1 완료\n",
      "🔄 배치 2/17: 4개 항목 처리 중...\n",
      "📈 진행률: 7.5% (5/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 2 완료\n",
      "🔄 배치 3/17: 4개 항목 처리 중...\n",
      "📈 진행률: 14.9% (10/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 3 완료\n",
      "🔄 배치 4/17: 4개 항목 처리 중...\n",
      "📈 진행률: 22.4% (15/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 4 완료\n",
      "🔄 배치 5/17: 4개 항목 처리 중...\n",
      "📈 진행률: 29.9% (20/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 5 완료\n",
      "🔄 배치 6/17: 4개 항목 처리 중...\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 6 완료\n",
      "🔄 배치 7/17: 4개 항목 처리 중...\n",
      "📈 진행률: 37.3% (25/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 7 완료\n",
      "🔄 배치 8/17: 4개 항목 처리 중...\n",
      "📈 진행률: 44.8% (30/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 8 완료\n",
      "🔄 배치 9/17: 4개 항목 처리 중...\n",
      "📈 진행률: 52.2% (35/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 9 완료\n",
      "🔄 배치 10/17: 4개 항목 처리 중...\n",
      "📈 진행률: 59.7% (40/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 10 완료\n",
      "🔄 배치 11/17: 4개 항목 처리 중...\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 11 완료\n",
      "🔄 배치 12/17: 4개 항목 처리 중...\n",
      "📈 진행률: 67.2% (45/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 12 완료\n",
      "🔄 배치 13/17: 4개 항목 처리 중...\n",
      "📈 진행률: 74.6% (50/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 13 완료\n",
      "🔄 배치 14/17: 4개 항목 처리 중...\n",
      "📈 진행률: 82.1% (55/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 14 완료\n",
      "🔄 배치 15/17: 4개 항목 처리 중...\n",
      "📈 진행률: 89.6% (60/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 15 완료\n",
      "🔄 배치 16/17: 4개 항목 처리 중...\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 16 완료\n",
      "🔄 배치 17/17: 3개 항목 처리 중...\n",
      "📈 진행률: 97.0% (65/67)\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 배치 17 완료\n",
      "\n",
      "🎉 수정된 배치 처리 완료!\n",
      "📊 처리 통계:\n",
      "   - 총 처리 항목: 67개\n",
      "   - 총 소요 시간: 1655.55초\n",
      "   - 평균 처리 시간: 24.71초/항목\n",
      "💾 분석 결과 저장 시작...\n",
      "🔧 분석 결과 데이터 구조 수정 중...\n",
      "🔧 67개 결과 데이터 구조 수정 완료\n",
      "📊 타입별 분포: 텍스트 50개, 테이블 17개\n",
      "✅ 텍스트 마크다운 저장: llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "✅ 텍스트 JSON 저장: llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "✅ 테이블 마크다운 저장: llm_analysis_final_20250623_135438_table_20250623_135438.md\n",
      "✅ 테이블 JSON 저장: llm_analysis_final_20250623_135438_table_20250623_135438.json\n",
      "✅ 저장된 파일:\n",
      "   📝 마크다운: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "   📊 JSON: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "   📄 전체 저장 파일:\n",
      "      text_markdown: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "      text_json: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "      table_markdown: llm_analysis_output/markdown_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.md\n",
      "      table_json: llm_analysis_output/json_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.json\n",
      "🧹 GPU 메모리 정리 완료\n",
      "🧹 수정된 LLM 매니저 리소스 정리 완료\n",
      "\n",
      "📋 최종 LLM 파이프라인 결과: 모든 문제 해결 완료!\n",
      "\n",
      "🎉 최종 성공!\n",
      "   📁 처리된 파일: 9개\n",
      "   📝 텍스트 청크: 50개\n",
      "   📊 테이블: 17개\n",
      "   🔍 분석 결과: 67개\n",
      "   📝 텍스트 결과: 50개\n",
      "   📊 테이블 결과: 17개\n",
      "\n",
      "💾 저장된 파일들:\n",
      "   📄 markdown: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "   📄 json: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "\n",
      "📂 전체 저장 파일 목록:\n",
      "   text_markdown: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "   text_json: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "   table_markdown: llm_analysis_output/markdown_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.md\n",
      "   table_json: llm_analysis_output/json_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.json\n",
      "\n",
      "============================================================\n",
      "🏁 수정된 LLM 파이프라인 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 🧪 수정된 LLM 파이프라인 테스트 실행\n",
    "\n",
    "print(\"🧪 수정된 LLM 파이프라인을 실행합니다... (모든 문제 해결)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 최종 수정 버전 실행\n",
    "llm_pipeline_result_final = execute_llm_pipeline()\n",
    "\n",
    "print(f\"\\n📋 최종 LLM 파이프라인 결과: {llm_pipeline_result_final.get('message', 'N/A')}\")\n",
    "\n",
    "if llm_pipeline_result_final.get('success'):\n",
    "    print(f\"\\n🎉 최종 성공!\")\n",
    "    print(f\"   📁 처리된 파일: {llm_pipeline_result_final.get('total_files_processed')}개\")\n",
    "    print(f\"   📝 텍스트 청크: {llm_pipeline_result_final.get('total_text_chunks')}개\") \n",
    "    print(f\"   📊 테이블: {llm_pipeline_result_final.get('total_tables')}개\")\n",
    "    print(f\"   🔍 분석 결과: {llm_pipeline_result_final.get('total_analysis_results')}개\")\n",
    "    print(f\"   📝 텍스트 결과: {llm_pipeline_result_final.get('text_results_count')}개\")\n",
    "    print(f\"   📊 테이블 결과: {llm_pipeline_result_final.get('table_results_count')}개\")\n",
    "    \n",
    "    saved_files = llm_pipeline_result_final.get('saved_files', {})\n",
    "    print(f\"\\n💾 저장된 파일들:\")\n",
    "    for file_type, file_path in saved_files.items():\n",
    "        if file_path and file_type != 'all_saved_files':\n",
    "            print(f\"   📄 {file_type}: {file_path}\")\n",
    "            \n",
    "    # 모든 저장 파일 상세 표시\n",
    "    all_saved = saved_files.get('all_saved_files', {})\n",
    "    if all_saved:\n",
    "        print(f\"\\n📂 전체 저장 파일 목록:\")\n",
    "        for key, path in all_saved.items():\n",
    "            if path:\n",
    "                print(f\"   {key}: {path}\")\n",
    "else:\n",
    "    print(f\"\\n❌ 실행 실패: {llm_pipeline_result_final.get('message')}\")\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🏁 수정된 LLM 파이프라인 테스트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant 벡터 DB & Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🗄️ 벡터 DB: 분석 결과를 임베딩하여 Qdrant에 저장\n",
    "🔍 검색 테스트: 여러 검색어로 벡터 검색 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers 및 기타 라이브러리\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033f427f393c403b8e3b361d9e57131f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "0abe43cbb56347f19e78fb12c32baaca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b356e8149b74f3e8d79d3acd521fe16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cf704292f934c418b9cebad64e2fde7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30e4909e547446baa522d20a45fe12d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3290db6325a54c09a90c12e9ca357dab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39a895b8a115411795e34835edca0c87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9ac128c0ce47448c819b6f131cdb40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f1ea86674444193bacb88fa555911c2",
      "placeholder": "​",
      "style": "IPY_MODEL_c738cce6df644dd3838daee6bd590502",
      "value": "Connecting..."
     }
    },
    "46daaed28d4d4f8999fbaf7341d6a3b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "478f761b7ee842bba506d83296051b85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ed247f1c8394d74b3c86453e020f032": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_e03b8902afe141b9860d954a5aaf7f1c"
     }
    },
    "5166717845a140f6b3bc96407aab3a2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59444f7d6a0a4412a1fee4afb59e01ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7117954f4c234c6580290d4df479d5cf",
      "placeholder": "​",
      "style": "IPY_MODEL_0b356e8149b74f3e8d79d3acd521fe16",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "5a3483fef64240319e498507056657ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "6465ed01974049ba9e931c823347376d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f1ea86674444193bacb88fa555911c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fc3a6fca6a443a28c9f3964aa851c1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7117954f4c234c6580290d4df479d5cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "785131af0d8d45c482d828adc1a14c96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d8a7301696c41bbaf6a11ec79bb74bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f848f9398764d09b021e97521166ada": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_adb5fd8f883f42e487d3c67c59b191fa"
     }
    },
    "8492c1e195d74c148a8d1deb7d177959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_1cf704292f934c418b9cebad64e2fde7",
      "style": "IPY_MODEL_99ded4beacb349f59951c7878418c36b",
      "value": true
     }
    },
    "84d65025b29542cf976deb3068cbbabd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_e00f902d50a2407a88df4c480e8d8ebd",
      "style": "IPY_MODEL_033f427f393c403b8e3b361d9e57131f",
      "tooltip": ""
     }
    },
    "8b29d68cb0b64741aacb813aa5f8ca05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0abe43cbb56347f19e78fb12c32baaca",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9c3dd7ef6fa449b8ffd0a2194a46423",
      "value": 2
     }
    },
    "8d791323e7ae4dda8d4e92b367362759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_478f761b7ee842bba506d83296051b85",
      "placeholder": "​",
      "style": "IPY_MODEL_a73c41ecf01540f0b28f2362a4b57e8a",
      "value": ""
     }
    },
    "8f48f700b1dd4facaba16f74d020a18f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99225575063e4335ad2affd85b8b5d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbee42484444455bbe68b749b77e2800",
      "placeholder": "​",
      "style": "IPY_MODEL_b08fe184782f43c7a3e22ae78efacfa5",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "99ded4beacb349f59951c7878418c36b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2f65e3e57aa4fefb77e75b70b9af2a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a73c41ecf01540f0b28f2362a4b57e8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8257d9bd02048f4969bc76b1a37a31e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad2a17a01db944339df8a3386ef0bc63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_785131af0d8d45c482d828adc1a14c96",
      "placeholder": "​",
      "style": "IPY_MODEL_d755f6c63ab04465818d7b3020c5799e",
      "value": " 2/2 [00:28&lt;00:00, 14.08s/it]"
     }
    },
    "adb5fd8f883f42e487d3c67c59b191fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "af1232fb89c241f3aef22c7bf0121aa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_f4851b2d697742e1aff69230206ae9e4",
      "style": "IPY_MODEL_d89bc6eb2d8440cf958074dff71e21ed",
      "value": true
     }
    },
    "aff7be0e7b674a76922729fc1c68c5a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_7d8a7301696c41bbaf6a11ec79bb74bf",
      "style": "IPY_MODEL_5a3483fef64240319e498507056657ef",
      "tooltip": ""
     }
    },
    "b08fe184782f43c7a3e22ae78efacfa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b12bd8cbf65e4c749c7b1197ae599a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_6fc3a6fca6a443a28c9f3964aa851c1a",
      "placeholder": "​",
      "style": "IPY_MODEL_3290db6325a54c09a90c12e9ca357dab",
      "value": ""
     }
    },
    "b18bbc866d0b4239bb0360e71851d04c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6465ed01974049ba9e931c823347376d",
      "placeholder": "​",
      "style": "IPY_MODEL_c1726258e9b14e2089ebdf13148cbef3",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "c1726258e9b14e2089ebdf13148cbef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c280ad0b86b147f3869196f225ebbb2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39a895b8a115411795e34835edca0c87",
      "placeholder": "​",
      "style": "IPY_MODEL_5166717845a140f6b3bc96407aab3a2e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c738cce6df644dd3838daee6bd590502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee5fbcc075540579ad05ff632749e2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8257d9bd02048f4969bc76b1a37a31e",
      "placeholder": "​",
      "style": "IPY_MODEL_46daaed28d4d4f8999fbaf7341d6a3b5",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "d755f6c63ab04465818d7b3020c5799e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d89bc6eb2d8440cf958074dff71e21ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbee42484444455bbe68b749b77e2800": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddee3f2203ae4fa092ef6bb610cf2f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2f65e3e57aa4fefb77e75b70b9af2a6",
      "placeholder": "​",
      "style": "IPY_MODEL_30e4909e547446baa522d20a45fe12d1",
      "value": "Connecting..."
     }
    },
    "e00f902d50a2407a88df4c480e8d8ebd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e03b8902afe141b9860d954a5aaf7f1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "e9a636aa7a2b47d9b8dd5b0b82a49dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c280ad0b86b147f3869196f225ebbb2d",
       "IPY_MODEL_8b29d68cb0b64741aacb813aa5f8ca05",
       "IPY_MODEL_ad2a17a01db944339df8a3386ef0bc63"
      ],
      "layout": "IPY_MODEL_8f48f700b1dd4facaba16f74d020a18f"
     }
    },
    "e9c3dd7ef6fa449b8ffd0a2194a46423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4851b2d697742e1aff69230206ae9e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
