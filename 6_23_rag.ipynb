{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì²˜ë¦¬\n",
    "! pip install -U docling\n",
    "\n",
    "! pip install qwen-vl-utils\n",
    "! pip install accelerate\n",
    "! pip install packaging ninja\n",
    "! pip install flash-attn==2.6.3 --no-build-isolation\n",
    "! pip install \"transformers==4.51.3\"\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ëŠ” ë™ì¼\n",
    "! pip install nvidia-ml-py3\n",
    "! pip install -q FlagEmbedding\n",
    "! pip install selenium webdriver-manager requests tqdm beautifulsoup4 lxml\n",
    "! pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í˜„ì¬ ì„¤ì¹˜ëœ ë²„ì „ í™•ì¸ ===\n",
      "PyTorch ë²„ì „: 2.4.1+cu124\n",
      "Transformers ë²„ì „: 4.51.3\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "CUDA ë²„ì „: 12.4\n",
      "ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: 1\n",
      "Accelerate ë²„ì „: 1.8.1\n",
      "Flash Attention ë²„ì „: 2.6.3\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ importë“¤\n",
    "import torch\n",
    "import transformers\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from contextlib import contextmanager\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor,AutoModelForCausalLM\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import huggingface_hub\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pynvml\n",
    "\n",
    "# ë²„ì „ í™•ì¸\n",
    "print(\"=== í˜„ì¬ ì„¤ì¹˜ëœ ë²„ì „ í™•ì¸ ===\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"Transformers ë²„ì „: {transformers.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"Accelerate ë²„ì „: {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Accelerate íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ\")\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"Flash Attention ë²„ì „: {flash_attn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Flash Attention íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iwuNlhfCWVe"
   },
   "source": [
    "# 1. Crwaling Data\n",
    "## URL, PDF_URL, ì¶œíŒì¼\n",
    "## PDF_URL ë‹¤ìš´ë¡œë“œ ë°›ì•„ ex_pdf_fileì— ì €ì¥.\n",
    "## pdf íŒŒì¼ëª…: ë…„ë„_ì œëª©.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸\n",
    "apt update\n",
    "\n",
    "# Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜\n",
    "wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add -\n",
    "echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list\n",
    "apt update\n",
    "apt install -y google-chrome-stable\n",
    "apt install -y wget curl unzip xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# Chrome ì„¤ì¹˜ í™•ì¸\n",
    "google-chrome --version\n",
    "\n",
    "# Python íŒ¨í‚¤ì§€ í™•ì¸\n",
    "python -c \"import selenium; print('Selenium ì„¤ì¹˜ë¨')\"\n",
    "python -c \"import webdriver_manager; print('WebDriver Manager ì„¤ì¹˜ë¨')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HzdfE_s788NZ",
    "outputId": "83aa5c1d-58ba-49a8-9c60-21447e414daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:22:32,271 - INFO - ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±: ./data/ex_pdf\n",
      "2025-06-23 06:22:32,271 - INFO - ====== WebDriver manager ======\n",
      "2025-06-23 06:22:32,329 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:32,389 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:32,452 - INFO - There is no [linux64] chromedriver \"137.0.7151.119\" for browser google-chrome \"137.0.7151\" in cache\n",
      "2025-06-23 06:22:32,453 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:32,613 - INFO - WebDriver version 137.0.7151.119 selected\n",
      "2025-06-23 06:22:32,615 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/137.0.7151.119/linux64/chromedriver-linux64.zip\n",
      "2025-06-23 06:22:32,615 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/137.0.7151.119/linux64/chromedriver-linux64.zip\n",
      "2025-06-23 06:22:32,700 - INFO - Driver downloading response is 200\n",
      "2025-06-23 06:22:32,887 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-23 06:22:33,034 - INFO - Driver has been saved in cache [/root/.wdm/drivers/chromedriver/linux64/137.0.7151.119]\n",
      "2025-06-23 06:22:33,370 - INFO - Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ\n",
      "2025-06-23 06:22:33,370 - INFO - \n",
      "==================================================\n",
      "2025-06-23 06:22:33,371 - INFO - í˜ì´ì§€ 1 ì²˜ë¦¬ ì‹œì‘\n",
      "2025-06-23 06:22:33,371 - INFO - ==================================================\n",
      "2025-06-23 06:22:33,371 - INFO - í˜ì´ì§€ 1 ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘: https://www.bok.or.kr/portal/singl/newsData/list.do?pageIndex=1&targetDepth=3&menuNo=200081&syncMenuChekKey=1&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&date=&sdate=&edate=&sort=1&pageUnit=10\n",
      "2025-06-23 06:22:44,816 - INFO - ì´ 10ê°œì˜ ë‰´ìŠ¤ í•­ëª© ë°œê²¬\n",
      "2025-06-23 06:22:44,881 - INFO -   1. [í˜„ì§€ì •ë³´] 2025_6ì›” FOMC ì‹œì¥ë°˜ì‘... (ë“±ë¡ì¼\n",
      "2025.06.19)\n",
      "2025-06-23 06:22:44,915 - INFO -   2. [í˜„ì§€ì •ë³´] ë‰´ìš•ì‚¬ë¬´ì†Œ ì´ì „ ê¸°ë…í–‰ì‚¬ ê°œìµœ... (ë“±ë¡ì¼\n",
      "2025.06.17)\n",
      "2025-06-23 06:22:44,971 - INFO -   3. [í˜„ì§€ì •ë³´] ç¾ 2025.5ì›” ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘... (ë“±ë¡ì¼\n",
      "2025.06.12)\n",
      "2025-06-23 06:22:45,026 - INFO -   4. [í˜„ì§€ì •ë³´] ç¾ 2025.5ì›” ê³ ìš©ì§€í‘œ ë‚´ìš© ë° ë‰´ìš• ê¸ˆìœµì‹œì¥ ë°˜ì‘... (ë“±ë¡ì¼\n",
      "2025.06.09)\n",
      "2025-06-23 06:22:45,082 - INFO -   5. [í˜„ì§€ì •ë³´] 25ë…„ 6ì›” ìºë‚˜ë‹¤ ì¤‘ì•™ì€í–‰ ì •ì±…íšŒì˜ ê²°ê³¼ ë° ì‹œì¥ ë°˜ì‘... (ë“±ë¡ì¼\n",
      "2025.06.04)\n",
      "2025-06-23 06:22:45,143 - INFO -   6. [í˜„ì§€ì •ë³´] ë¯¸êµ­ ì‹ ìš©ë“±ê¸‰ í•˜í–¥ ì¡°ì •ì— ëŒ€í•œ ì‹œì¥ì°¸ê°€ì í‰ê°€... (ë“±ë¡ì¼\n",
      "2025.05.20)\n",
      "2025-06-23 06:22:45,206 - INFO -   7. [í˜„ì§€ì •ë³´] Moodyâ€™sç¤¾, ë¯¸êµ­ ì‹ ìš©ë“±ê¸‰ í•˜í–¥ì¡°ì •... (ë“±ë¡ì¼\n",
      "2025.05.19)\n",
      "2025-06-23 06:22:45,261 - INFO -   8. [í˜„ì§€ì •ë³´] ç¾ 2025.4ì›” ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘... (ë“±ë¡ì¼\n",
      "2025.05.14)\n",
      "2025-06-23 06:22:45,309 - INFO -   9. [í˜„ì§€ì •ë³´] 2025ë…„ 5ì›” FOMC ì‹œì¥ë°˜ì‘... (ë“±ë¡ì¼\n",
      "2025.05.08)\n",
      "2025-06-23 06:22:45,339 - INFO -   10. [í˜„ì§€ì •ë³´] ìµœê·¼(2025.4ì›”)ì˜ ë¯¸êµ­ê²½ì œ ìƒí™©ê³¼ í‰ê°€... (ë“±ë¡ì¼\n",
      "2025.05.07)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:   0%|          | 0/10 [00:00<?, ?it/s]2025-06-23 06:22:45,347 - INFO - \n",
      "[1/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] 2025_6ì›” FOMC ì‹œì¥ë°˜ì‘...\n",
      "2025-06-23 06:22:45,348 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091991&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:22:52,637 - INFO -   PDF íŒŒì¼ ë°œê²¬: (í˜„ì§€ì •ë³´ 250618) 2025_6ì›” FOMC ì‹œì¥ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:22:52,654 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:22:52,655 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:22:59,248 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf (307,388 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  10%|â–ˆ         | 1/10 [00:14<02:14, 14.90s/it]2025-06-23 06:23:00,252 - INFO - \n",
      "[2/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] ë‰´ìš•ì‚¬ë¬´ì†Œ ì´ì „ ê¸°ë…í–‰ì‚¬ ê°œìµœ...\n",
      "2025-06-23 06:23:00,253 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091946&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:04,939 - INFO -   PDFê°€ ì•„ë‹Œ íŒŒì¼ì€ ê±´ë„ˆëœ€: [í˜„ì§€ì •ë³´] ë‰´ìš•ì‚¬ë¬´ì†Œ ì´ì „ ê¸°ë…í–‰ì‚¬ ê°œìµœ.hwp (hwp)\n",
      "2025-06-23 06:23:04,961 - INFO - PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\n",
      "2025-06-23 06:23:04,962 - INFO - ì´ ë‰´ìŠ¤ì—ëŠ” PDF íŒŒì¼ì´ ì—†ìŒ\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  20%|â–ˆâ–ˆ        | 2/10 [00:19<01:11,  8.91s/it]2025-06-23 06:23:04,963 - INFO - \n",
      "[3/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] ç¾ 2025.5ì›” ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘...\n",
      "2025-06-23 06:23:04,964 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091882&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:10,267 - INFO -   PDF íŒŒì¼ ë°œê²¬: [í˜„ì§€ì •ë³´] ç¾ 2025.5ì›” ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘_F.pdf\n",
      "2025-06-23 06:23:10,294 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:23:10,294 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf\n",
      "2025-06-23 06:23:17,215 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf (325,603 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:32<01:16, 10.89s/it]2025-06-23 06:23:18,218 - INFO - \n",
      "[4/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] ç¾ 2025.5ì›” ê³ ìš©ì§€í‘œ ë‚´ìš© ë° ë‰´ìš• ê¸ˆìœµì‹œì¥ ë°˜ì‘...\n",
      "2025-06-23 06:23:18,220 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091787&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:23,505 - INFO -   PDF íŒŒì¼ ë°œê²¬: (í˜„ì§€ì •ë³´) ç¾ 2025.5ì›” ê³ ìš©ì§€í‘œ ë‚´ìš© ë° ë‰´ìš• ê¸ˆìœµì‹œì¥ ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:23:23,535 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:23:23,536 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:23:27,954 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf (268,806 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:43<01:04, 10.83s/it]2025-06-23 06:23:28,957 - INFO - \n",
      "[5/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] 25ë…„ 6ì›” ìºë‚˜ë‹¤ ì¤‘ì•™ì€í–‰ ì •ì±…íšŒì˜ ê²°ê³¼ ë° ì‹œì¥ ë°˜ì‘...\n",
      "2025-06-23 06:23:28,958 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091760&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:34,235 - INFO -   PDF íŒŒì¼ ë°œê²¬: [í˜„ì§€ì •ë³´] 25ë…„ 6ì›” ìºë‚˜ë‹¤ ì¤‘ì•™ì€í–‰ ì •ì±…íšŒì˜ ê²°ê³¼ ë° ì‹œì¥ ë°˜ì‘.pdf\n",
      "2025-06-23 06:23:34,264 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:23:34,265 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf\n",
      "2025-06-23 06:23:40,863 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf (471,063 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:56<00:57, 11.58s/it]2025-06-23 06:23:41,867 - INFO - \n",
      "[6/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] ë¯¸êµ­ ì‹ ìš©ë“±ê¸‰ í•˜í–¥ ì¡°ì •ì— ëŒ€í•œ ì‹œì¥ì°¸ê°€ì í‰ê°€...\n",
      "2025-06-23 06:23:41,868 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091484&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:23:47,883 - INFO -   PDF íŒŒì¼ ë°œê²¬: [í˜„ì§€ì •ë³´] ë¯¸êµ­ ì‹ ìš©ë“±ê¸‰ í•˜í–¥ ì¡°ì •ì— ëŒ€í•œ ì‹œì¥ì°¸ê°€ì í‰ê°€.pdf\n",
      "2025-06-23 06:23:47,906 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:23:47,907 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf\n",
      "2025-06-23 06:23:54,650 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf (304,873 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:10<00:49, 12.33s/it]2025-06-23 06:23:55,651 - INFO - \n",
      "[7/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] Moodyâ€™sç¤¾, ë¯¸êµ­ ì‹ ìš©ë“±ê¸‰ í•˜í–¥ì¡°ì •...\n",
      "2025-06-23 06:23:55,651 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091459&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:00,334 - INFO -   PDF íŒŒì¼ ë°œê²¬: (í˜„ì§€ì •ë³´ 20250516) Moodyâ€™sç¤¾, ë¯¸êµ­ ì‹ ìš©ë“±ê¸‰ í•˜í–¥ì¡°ì •_f.pdf\n",
      "2025-06-23 06:24:00,359 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:24:00,360 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf\n",
      "2025-06-23 06:24:04,424 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf (104,921 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:20<00:34, 11.50s/it]2025-06-23 06:24:05,428 - INFO - \n",
      "[8/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] ç¾ 2025.4ì›” ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘...\n",
      "2025-06-23 06:24:05,429 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091387&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:11,294 - INFO -   PDF íŒŒì¼ ë°œê²¬: [í˜„ì§€ì •ë³´] ç¾ 2025.4ì›” ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘.pdf\n",
      "2025-06-23 06:24:11,323 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:24:11,324 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf\n",
      "2025-06-23 06:24:16,758 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf (319,471 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:32<00:23, 11.76s/it]2025-06-23 06:24:17,763 - INFO - \n",
      "[9/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] 2025ë…„ 5ì›” FOMC ì‹œì¥ë°˜ì‘...\n",
      "2025-06-23 06:24:17,764 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091313&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:22,503 - INFO -   PDF íŒŒì¼ ë°œê²¬: (í˜„ì§€ì •ë³´ 250507) 2025_5ì›” FOMC ì‹œì¥ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:24:22,531 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:24:22,532 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:24:27,004 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf (305,178 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:42<00:11, 11.29s/it]2025-06-23 06:24:28,007 - INFO - \n",
      "[10/10] ì²˜ë¦¬ ì¤‘: [í˜„ì§€ì •ë³´] ìµœê·¼(2025.4ì›”)ì˜ ë¯¸êµ­ê²½ì œ ìƒí™©ê³¼ í‰ê°€...\n",
      "2025-06-23 06:24:28,008 - INFO - PDF ë§í¬ ì¶”ì¶œ ì¤‘: https://www.bok.or.kr/portal/bbs/P0002221/view.do?nttId=10091256&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&depth=200081&pageUnit=10&pageIndex=1&programType=newsData&menuNo=200081&oldMenuNo=200081\n",
      "2025-06-23 06:24:33,006 - INFO -   PDF íŒŒì¼ ë°œê²¬: ìµœê·¼(2025.4ì›”)ì˜ ë¯¸êµ­ê²½ì œ ìƒí™©ê³¼ í‰ê°€.pdf\n",
      "2025-06-23 06:24:33,014 - INFO - ì´ 1ê°œì˜ PDF íŒŒì¼ ë°œê²¬\n",
      "2025-06-23 06:24:33,014 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf\n",
      "2025-06-23 06:24:37,754 - INFO - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf (1,006,696 bytes)\n",
      "í˜ì´ì§€ 1 ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:53<00:00, 11.34s/it]\n",
      "2025-06-23 06:24:38,756 - INFO - \n",
      "============================================================\n",
      "2025-06-23 06:24:38,756 - INFO - PDF ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½\n",
      "2025-06-23 06:24:38,756 - INFO - ============================================================\n",
      "2025-06-23 06:24:38,756 - INFO - ì´ ë‰´ìŠ¤ í•­ëª©: 10\n",
      "2025-06-23 06:24:38,757 - INFO - ì´ PDF íŒŒì¼: 9\n",
      "2025-06-23 06:24:38,757 - INFO - ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ: 9\n",
      "2025-06-23 06:24:38,757 - INFO - ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ: 0\n",
      "2025-06-23 06:24:38,757 - INFO - ì €ì¥ëœ PDF íŒŒì¼ ìˆ˜: 9\n",
      "2025-06-23 06:24:38,757 - INFO - \n",
      "ì €ì¥ëœ PDF íŒŒì¼ ëª©ë¡:\n",
      "2025-06-23 06:24:38,757 - INFO -   - ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf (307,388 bytes)\n",
      "2025-06-23 06:24:38,757 - INFO -   - ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf (325,603 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf (268,806 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf (471,063 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf (304,873 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf (104,921 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf (319,471 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf (305,178 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO -   - ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf (1,006,696 bytes)\n",
      "2025-06-23 06:24:38,758 - INFO - \n",
      "PDFê°€ ì—†ëŠ” ë‰´ìŠ¤ í•­ëª©: 1ê°œ\n",
      "2025-06-23 06:24:38,822 - INFO - ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class BOKNewsScraper:\n",
    "    def __init__(self, save_dir='./data/ex_pdf_file', headless=True):\n",
    "        self.BASE_URL = 'https://www.bok.or.kr/portal/singl/newsData/list.do?pageIndex={page}&targetDepth=3&menuNo=200081&syncMenuChekKey=1&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200080&depth3=200081&date=&sdate=&edate=&sort=1&pageUnit=10'\n",
    "        self.DETAIL_BASE = 'https://www.bok.or.kr'\n",
    "        self.save_dir = save_dir\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "        \n",
    "        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        logger.info(f\"ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.save_dir}\")\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"RunPod í™˜ê²½ì— ìµœì í™”ëœ Chrome ë“œë¼ì´ë²„ ì„¤ì •\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            \n",
    "            # RunPod í™˜ê²½ì„ ìœ„í•œ Chrome ì˜µì…˜ë“¤\n",
    "            if self.headless:\n",
    "                options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--disable-web-security\")\n",
    "            options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "            options.add_argument(\"--window-size=1920,1080\")\n",
    "            options.add_argument(\"--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "            \n",
    "            # ChromeDriverManagerë¥¼ ì‚¬ìš©í•´ ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ ê´€ë¦¬\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì„¤ì •\n",
    "            self.driver.set_page_load_timeout(30)\n",
    "            self.driver.implicitly_wait(10)\n",
    "            \n",
    "            logger.info(\"Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_news_list(self, page=1, max_retries=3):\n",
    "        \"\"\"ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "        url = self.BASE_URL.format(page=page)\n",
    "        logger.info(f\"í˜ì´ì§€ {page} ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {url}\")\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "                WebDriverWait(self.driver, 15).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.bd-line\"))\n",
    "                )\n",
    "                \n",
    "                # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„\n",
    "                time.sleep(2)\n",
    "                \n",
    "                bd_line_div = self.driver.find_element(By.CSS_SELECTOR, \"div.bd-line\")\n",
    "                news_items = bd_line_div.find_elements(By.CSS_SELECTOR, \"li.bbsRowCls\")\n",
    "                \n",
    "                if not news_items:\n",
    "                    logger.warning(f\"í˜ì´ì§€ {page}ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "                    return []\n",
    "                \n",
    "                results = []\n",
    "                logger.info(f\"ì´ {len(news_items)}ê°œì˜ ë‰´ìŠ¤ í•­ëª© ë°œê²¬\")\n",
    "                \n",
    "                for idx, item in enumerate(news_items):\n",
    "                    try:\n",
    "                        # ë§í¬ ì¶”ì¶œ\n",
    "                        set_div = item.find_element(By.CSS_SELECTOR, \"div.set\")\n",
    "                        a_tag = set_div.find_element(By.TAG_NAME, \"a\")\n",
    "                        href = a_tag.get_attribute('href')\n",
    "                        detail_url = urljoin(self.DETAIL_BASE, href)\n",
    "                        \n",
    "                        # ì œëª© ì¶”ì¶œ\n",
    "                        title_text = a_tag.text.strip()\n",
    "                        \n",
    "                        # ë‚ ì§œ ì¶”ì¶œ\n",
    "                        try:\n",
    "                            date_span = item.find_element(By.CSS_SELECTOR, \"span.date\")\n",
    "                            date_text = date_span.text.strip()\n",
    "                        except NoSuchElementException:\n",
    "                            date_text = \"ë‚ ì§œì—†ìŒ\"\n",
    "                        \n",
    "                        results.append({\n",
    "                            'title': title_text,\n",
    "                            'detail_url': detail_url,\n",
    "                            'date': date_text,\n",
    "                        })\n",
    "                        \n",
    "                        logger.info(f\"  {idx + 1}. {title_text[:50]}... ({date_text})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"ë‰´ìŠ¤ í•­ëª© {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                return results\n",
    "                \n",
    "            except TimeoutException:\n",
    "                logger.warning(f\"í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.error(\"í˜ì´ì§€ ë¡œë”© ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼\")\n",
    "                    return []\n",
    "            except Exception as e:\n",
    "                logger.error(f\"ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    return []\n",
    "    \n",
    "    def get_pdf_links(self, detail_url, max_retries=3):\n",
    "        \"\"\"ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ë§í¬ë§Œ ì¶”ì¶œ\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"PDF ë§í¬ ì¶”ì¶œ ì¤‘: {detail_url}\")\n",
    "                self.driver.get(detail_url)\n",
    "                time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "                \n",
    "                # ëª¨ë“  íŒŒì¼ ë§í¬ ì°¾ê¸°\n",
    "                file_elements = self.driver.find_elements(By.XPATH, \"//a[starts-with(@href, '/fileSrc/')]\")\n",
    "                \n",
    "                if not file_elements:\n",
    "                    logger.info(\"ì²¨ë¶€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "                    return []\n",
    "                \n",
    "                pdf_links = []\n",
    "                for file_element in file_elements:\n",
    "                    try:\n",
    "                        file_href = file_element.get_attribute(\"href\")\n",
    "                        file_title = file_element.get_attribute(\"title\") or file_element.text.strip()\n",
    "                        \n",
    "                        if file_title:\n",
    "                            # íŒŒì¼ í™•ì¥ì í™•ì¸\n",
    "                            file_ext = self.get_file_extension_from_url(file_href) or self.get_file_extension_from_title(file_title)\n",
    "                            \n",
    "                            # PDF íŒŒì¼ë§Œ í•„í„°ë§\n",
    "                            if file_ext and file_ext.lower() == 'pdf':\n",
    "                                pdf_links.append({\n",
    "                                    'title': file_title,\n",
    "                                    'file_url': urljoin(self.DETAIL_BASE, file_href),\n",
    "                                    'file_type': 'pdf'\n",
    "                                })\n",
    "                                logger.info(f\"  PDF íŒŒì¼ ë°œê²¬: {file_title}\")\n",
    "                            else:\n",
    "                                logger.info(f\"  PDFê°€ ì•„ë‹Œ íŒŒì¼ì€ ê±´ë„ˆëœ€: {file_title} ({file_ext})\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"íŒŒì¼ ë§í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if pdf_links:\n",
    "                    logger.info(f\"ì´ {len(pdf_links)}ê°œì˜ PDF íŒŒì¼ ë°œê²¬\")\n",
    "                else:\n",
    "                    logger.info(\"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "                \n",
    "                return pdf_links\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"PDF ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    return []\n",
    "    \n",
    "    def get_file_extension_from_url(self, url):\n",
    "        \"\"\"URLì—ì„œ íŒŒì¼ í™•ì¥ì ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            # URLì—ì„œ íŒŒì¼ëª… ë¶€ë¶„ ì¶”ì¶œ\n",
    "            filename = url.split('/')[-1]\n",
    "            if '.' in filename:\n",
    "                return filename.split('.')[-1]\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def get_file_extension_from_title(self, title):\n",
    "        \"\"\"ì œëª©ì—ì„œ íŒŒì¼ í™•ì¥ì ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            if '.' in title:\n",
    "                return title.split('.')[-1]\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def download_pdf(self, file_url, save_path, max_retries=3):\n",
    "        \"\"\"PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ\"\"\"\n",
    "        if os.path.exists(save_path):\n",
    "            logger.info(f\"ì´ë¯¸ ì¡´ì¬í•¨, ê±´ë„ˆëœ€: {os.path.basename(save_path)}\")\n",
    "            return True\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {os.path.basename(save_path)}\")\n",
    "                \n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(file_url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # PDF íŒŒì¼ì¸ì§€ í™•ì¸ (Content-Type ì²´í¬)\n",
    "                content_type = response.headers.get('content-type', '').lower()\n",
    "                if 'pdf' not in content_type and not save_path.lower().endswith('.pdf'):\n",
    "                    logger.warning(f\"PDFê°€ ì•„ë‹Œ íŒŒì¼ë¡œ ë³´ì„: {content_type}\")\n",
    "                \n",
    "                with open(save_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                file_size = os.path.getsize(save_path)\n",
    "                logger.info(f\"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {os.path.basename(save_path)} ({file_size:,} bytes)\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    return False\n",
    "    \n",
    "    def sanitize_filename(self, filename, max_length=100):\n",
    "        \"\"\"íŒŒì¼ëª… ì •ë¦¬\"\"\"\n",
    "        # ë¶ˆë²• ë¬¸ì ì œê±°\n",
    "        filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename.strip())\n",
    "        # ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
    "        filename = re.sub(r'\\s+', '_', filename)\n",
    "        # ì½œë¡ ì„ ëŒ€ì‹œë¡œ\n",
    "        filename = filename.replace(\":\", \"-\")\n",
    "        # ê¸¸ì´ ì œí•œ\n",
    "        if len(filename) > max_length:\n",
    "            filename = filename[:max_length]\n",
    "        return filename\n",
    "    \n",
    "    def run_scraper(self, start_page=1, end_page=1):\n",
    "        \"\"\"ë©”ì¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ (PDF ì „ìš©)\"\"\"\n",
    "        if not self.setup_driver():\n",
    "            logger.error(\"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨ë¡œ ì¢…ë£Œ\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            all_results = []\n",
    "            \n",
    "            for page in range(start_page, end_page + 1):\n",
    "                logger.info(f\"\\n{'='*50}\")\n",
    "                logger.info(f\"í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹œì‘\")\n",
    "                logger.info(f\"{'='*50}\")\n",
    "                \n",
    "                news_list = self.get_news_list(page)\n",
    "                if not news_list:\n",
    "                    logger.warning(f\"í˜ì´ì§€ {page}ì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "                    continue\n",
    "                \n",
    "                # ê° ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬\n",
    "                for i, news in enumerate(tqdm(news_list, desc=f\"í˜ì´ì§€ {page} ì²˜ë¦¬\")):\n",
    "                    logger.info(f\"\\n[{i+1}/{len(news_list)}] ì²˜ë¦¬ ì¤‘: {news['title'][:50]}...\")\n",
    "                    \n",
    "                    # PDF ë§í¬ë§Œ ì¶”ì¶œ\n",
    "                    pdf_links = self.get_pdf_links(news['detail_url'])\n",
    "                    news['pdf_files'] = pdf_links\n",
    "                    \n",
    "                    if not pdf_links:\n",
    "                        logger.info(\"ì´ ë‰´ìŠ¤ì—ëŠ” PDF íŒŒì¼ì´ ì—†ìŒ\")\n",
    "                        all_results.append(news)\n",
    "                        continue\n",
    "                    \n",
    "                    # PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "                    for j, pdf_info in enumerate(pdf_links):\n",
    "                        file_title = pdf_info['title']\n",
    "                        file_url = pdf_info['file_url']\n",
    "                        \n",
    "                        # íŒŒì¼ëª… ìƒì„±\n",
    "                        safe_date = self.sanitize_filename(news['date']) if news['date'] else 'no_date'\n",
    "                        safe_title = self.sanitize_filename(file_title, 50)\n",
    "                        \n",
    "                        # PDF í™•ì¥ì í™•ì¸ ë° ì¶”ê°€\n",
    "                        if not safe_title.lower().endswith('.pdf'):\n",
    "                            safe_title += '.pdf'\n",
    "                        \n",
    "                        filename = f\"{safe_date}_{safe_title}\"\n",
    "                        save_path = os.path.join(self.save_dir, filename)\n",
    "                        \n",
    "                        # PDF ë‹¤ìš´ë¡œë“œ\n",
    "                        success = self.download_pdf(file_url, save_path)\n",
    "                        pdf_info['download_success'] = success\n",
    "                        pdf_info['local_path'] = save_path if success else None\n",
    "                    \n",
    "                    all_results.append(news)\n",
    "                    time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
    "            \n",
    "            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "            self.print_summary(all_results)\n",
    "            return all_results\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"\\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        finally:\n",
    "            self.cleanup()\n",
    "    \n",
    "    def print_summary(self, results):\n",
    "        \"\"\"ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (PDF ì „ìš©)\"\"\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(\"PDF ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        total_news = len(results)\n",
    "        total_pdfs = sum(len(news.get('pdf_files', [])) for news in results)\n",
    "        successful_downloads = sum(\n",
    "            sum(1 for pdf_info in news.get('pdf_files', []) if pdf_info.get('download_success', False))\n",
    "            for news in results\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"ì´ ë‰´ìŠ¤ í•­ëª©: {total_news}\")\n",
    "        logger.info(f\"ì´ PDF íŒŒì¼: {total_pdfs}\")\n",
    "        logger.info(f\"ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ: {successful_downloads}\")\n",
    "        logger.info(f\"ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ: {total_pdfs - successful_downloads}\")\n",
    "        \n",
    "        # ì €ì¥ëœ PDF íŒŒì¼ ëª©ë¡\n",
    "        saved_pdfs = [f for f in os.listdir(self.save_dir) if f.endswith('.pdf')]\n",
    "        logger.info(f\"ì €ì¥ëœ PDF íŒŒì¼ ìˆ˜: {len(saved_pdfs)}\")\n",
    "        \n",
    "        if saved_pdfs:\n",
    "            logger.info(\"\\nì €ì¥ëœ PDF íŒŒì¼ ëª©ë¡:\")\n",
    "            for pdf_file in saved_pdfs:\n",
    "                file_path = os.path.join(self.save_dir, pdf_file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                logger.info(f\"  - {pdf_file} ({file_size:,} bytes)\")\n",
    "        \n",
    "        # PDFê°€ ì—†ëŠ” ë‰´ìŠ¤ í•­ëª© í†µê³„\n",
    "        news_without_pdf = sum(1 for news in results if not news.get('pdf_files'))\n",
    "        if news_without_pdf > 0:\n",
    "            logger.info(f\"\\nPDFê°€ ì—†ëŠ” ë‰´ìŠ¤ í•­ëª©: {news_without_pdf}ê°œ\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"ë¦¬ì†ŒìŠ¤ ì •ë¦¬\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    # PDF ì „ìš© ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    scraper = BOKNewsScraper(\n",
    "        save_dir='./data/ex_pdf',  # ì €ì¥ ê²½ë¡œ\n",
    "        headless=True  # GUI ì—†ì´ ì‹¤í–‰ (RunPodì—ì„œëŠ” True ê¶Œì¥)\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 1í˜ì´ì§€ë§Œ ìŠ¤í¬ë˜í•‘ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "        results = scraper.run_scraper(start_page=1, end_page=1)\n",
    "        \n",
    "        # ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ì›í•  ê²½ìš°:\n",
    "        # results = scraper.run_scraper(start_page=1, end_page=3)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqFB8ceBGWvt"
   },
   "source": [
    "# 2. Extracted Data\n",
    "#### pdf(raw data) -> Docling -> Table, Text, Image(graph, chart etc..)\n",
    "\n",
    "\n",
    "#### 1) ex_images(ì´ë¯¸ì§€ë§Œ ëª¨ìŒ.)\n",
    "#### 2) ex_text (í…ìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´ í˜•íƒœ)\n",
    "#### 3_ ex_table (í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ í˜•íƒœíƒœ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b03gnccU99Mk"
   },
   "source": [
    "## í…ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZTb_fG-xGzey"
   },
   "outputs": [],
   "source": [
    "# JSON ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - ë°ì´í„° ì§ë ¬í™” ë° ì—­ì§ë ¬í™”ì— ì‚¬ìš©\n",
    "import json\n",
    "# ë¡œê¹… ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - ë””ë²„ê¹… ë° ì •ë³´ ê¸°ë¡ì— ì‚¬ìš©\n",
    "import logging\n",
    "# ì‹œê°„ ì¸¡ì • ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ì— ì‚¬ìš©\n",
    "import time\n",
    "# íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ê²½ë¡œ ê´€ë¦¬ì— ì‚¬ìš©\n",
    "from pathlib import Path\n",
    "# ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - í˜„ì¬ ëª¨ë“ˆì˜ ë¡œê¹…ì„ ìœ„í•´ ì‚¬ìš©\n",
    "import os\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o9QApKB5Gzhn"
   },
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë°ì´í„° ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° - ì…ë ¥ í˜•ì‹ ì •ì˜ì— ì‚¬ìš©\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "# PDF íŒŒì´í”„ë¼ì¸ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸° - PDF ì²˜ë¦¬ ì„¤ì •ì— ì‚¬ìš©\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "# ë¬¸ì„œ ë³€í™˜ê¸° ë° PDF í˜•ì‹ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸° - ë¬¸ì„œ ë³€í™˜ì— ì‚¬ìš©\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# PDF íŒŒì´í”„ë¼ì¸ ì˜µì…˜ ì„¤ì •\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "\n",
    "pipeline_options.do_ocr = False  # OCR ê¸°ëŠ¥ ë¹„í™œì„±í™” (ì´ë¯¸ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” PDF ì‚¬ìš©)\n",
    "pipeline_options.ocr_options.lang = [\"ko\"]  # OCR ì–¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ ì„¤ì • (OCR ì‚¬ìš© ì‹œ)\n",
    "\n",
    "pipeline_options.do_table_structure = True  # í‘œ êµ¬ì¡° ì¸ì‹ í™œì„±í™” --> doclingì˜ tableformer í™œìš©í•´ í‘œ ìƒì„¸ ì‚¬í•­ íŒŒì•…\n",
    "pipeline_options.table_structure_options.do_cell_matching = True  # í‘œ ì…€ ë§¤ì¹­ í™œì„±í™”\n",
    "\n",
    "# ë¬¸ì„œ ë³€í™˜ê¸° ìƒì„± ë° PDF í˜•ì‹ ì˜µì…˜ ì„¤ì •\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCwUd2vhGzkt",
    "outputId": "82e3ae8f-3476-49d3-c9e9-94781dab01bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:40,983 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:40,983 - INFO - Initializing pipeline for StandardPdfPipeline with options hash ab7aa2351bda7a3639289f49ddf570b8\n",
      "2025-06-23 06:24:40,987 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-06-23 06:24:40,987 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-06-23 06:24:40,988 - INFO - Accelerator device: 'cuda:0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:47,791 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-06-23 06:24:48,050 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-06-23 06:24:48,051 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-06-23 06:24:48,051 - INFO - Processing document ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf\n",
      "2025-06-23 06:24:50,178 - INFO - Finished converting document ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf in 9.20 sec.\n",
      "2025-06-23 06:24:50,191 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:50,192 - INFO - Processing document ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf (9.20ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:51,390 - INFO - Finished converting document ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf in 1.20 sec.\n",
      "2025-06-23 06:24:51,400 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:51,400 - INFO - Processing document ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.pdf (1.20ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:52,285 - INFO - Finished converting document ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf in 0.89 sec.\n",
      "2025-06-23 06:24:52,294 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:52,295 - INFO - Processing document ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.pdf (0.89ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:53,555 - INFO - Finished converting document ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf in 1.26 sec.\n",
      "2025-06-23 06:24:53,562 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:53,563 - INFO - Processing document ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.pdf (1.27ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:54,633 - INFO - Finished converting document ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf in 1.07 sec.\n",
      "2025-06-23 06:24:54,644 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:54,644 - INFO - Processing document ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.pdf (1.08ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:54,957 - INFO - Finished converting document ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf in 0.31 sec.\n",
      "2025-06-23 06:24:54,966 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:54,966 - INFO - Processing document ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.pdf (0.32ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:55,981 - INFO - Finished converting document ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf in 1.02 sec.\n",
      "2025-06-23 06:24:55,987 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:55,987 - INFO - Processing document ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.pdf (1.02ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:24:56,815 - INFO - Finished converting document ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf in 0.83 sec.\n",
      "2025-06-23 06:24:56,823 - INFO - Going to convert document batch...\n",
      "2025-06-23 06:24:56,824 - INFO - Processing document ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.pdf (0.83ì´ˆ)\n",
      "ğŸ”„ ë³€í™˜ ì‹œì‘: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:25:01,390 - INFO - Finished converting document ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf in 4.57 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë³€í™˜ ì™„ë£Œ: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.pdf (4.57ì´ˆ)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ì…ë ¥ ë° ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "input_dir = Path(\"./data/ex_pdf\")\n",
    "output_base_dir = Path(\"./data/ex_text\")\n",
    "output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# PDF íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "pdf_files = [f for f in os.listdir(input_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "if not pdf_files:\n",
    "    raise FileNotFoundError(\"ì…ë ¥ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ê° PDF íŒŒì¼ ì²˜ë¦¬\n",
    "for pdf_file in pdf_files:\n",
    "    input_doc_path = input_dir / pdf_file\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸ”„ ë³€í™˜ ì‹œì‘: {pdf_file}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ë¬¸ì„œ ë³€í™˜ ì‹¤í–‰\n",
    "        conv_result = doc_converter.convert(str(input_doc_path))\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"âœ… ë³€í™˜ ì™„ë£Œ: {pdf_file} ({end_time:.2f}ì´ˆ)\")\n",
    "\n",
    "        # íŒŒì¼ ì´ë¦„(í™•ì¥ì ì œì™¸)\n",
    "        doc_filename = conv_result.input.file.stem\n",
    "\n",
    "        # PDFë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        pdf_output_dir = output_base_dir / doc_filename\n",
    "        pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "        # ë§ˆí¬ë‹¤ìš´ ì €ì¥\n",
    "        with (pdf_output_dir / f\"{doc_filename}.md\").open(\"w\", encoding=\"utf-8\") as fp:\n",
    "            fp.write(conv_result.document.export_to_markdown())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë³€í™˜ ì‹¤íŒ¨: {pdf_file} - {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…Œì´ë¸” ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - ë°ì´í„° ì§ë ¬í™” ë° ì—­ì§ë ¬í™”ì— ì‚¬ìš©\n",
    "import json\n",
    "# ë¡œê¹… ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - ë””ë²„ê¹… ë° ì •ë³´ ê¸°ë¡ì— ì‚¬ìš©\n",
    "import logging\n",
    "# ì‹œê°„ ì¸¡ì • ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ì— ì‚¬ìš©\n",
    "import time\n",
    "# íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° - íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ê²½ë¡œ ê´€ë¦¬ì— ì‚¬ìš©\n",
    "from pathlib import Path\n",
    "# ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - í˜„ì¬ ëª¨ë“ˆì˜ ë¡œê¹…ì„ ìœ„í•´ ì‚¬ìš©\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "_log = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(\"./data/ex_pdf\")\n",
    "    output_dir = Path(\"./data/ex_table\")\n",
    "\n",
    "    # PDF íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    pdf_files = [f for f in os.listdir(input_doc_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "    doc_converter = DocumentConverter()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ê° PDF íŒŒì¼ì„ ê°œë³„ì ìœ¼ë¡œ ë³€í™˜\n",
    "    for pdf_file in pdf_files:\n",
    "        full_pdf_path = input_doc_path / pdf_file  # íŒŒì¼ ê²½ë¡œ ì¡°í•©\n",
    "        doc_filename = Path(pdf_file).stem # íŒŒì¼ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (í™•ì¥ì ì œì™¸)\n",
    "\n",
    "        # íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        file_output_dir = output_dir / doc_filename\n",
    "        file_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        conv_res = doc_converter.convert(full_pdf_path)  # ê°œë³„ íŒŒì¼ ì „ë‹¬\n",
    "\n",
    "\n",
    "        # Export tables\n",
    "        for table_ix, table in enumerate(conv_res.document.tables):\n",
    "            table_df: pd.DataFrame = table.export_to_dataframe()\n",
    "            # íŒŒì¼ ì´ë¦„ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥\n",
    "            print(f\"## {doc_filename} - Table {table_ix}\")\n",
    "            print(table_df.to_markdown())\n",
    "\n",
    "            # Save the table as md in the dedicated directory\n",
    "            element_md_filename = file_output_dir / f\"{doc_filename}-table-{table_ix + 1}.md\"\n",
    "            _log.info(f\"Saving md table to {element_md_filename}\")\n",
    "            with element_md_filename.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(table.export_to_html(doc=conv_res.document))\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(f\"Documents converted and tables exported in {end_time:.2f} seconds.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0ZUs6p6-AeT"
   },
   "source": [
    "## ì´ë¯¸ì§€ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C49gYsp-Gz1g"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2IY7FGR9Gz5F"
   },
   "outputs": [],
   "source": [
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ap9RYpsRGz73"
   },
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IaTB9h5QG0CD"
   },
   "outputs": [],
   "source": [
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s0FxMAZb89-j"
   },
   "outputs": [],
   "source": [
    "IMAGE_RESOLUTION_SCALE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aiOEIYPM8-BD"
   },
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ í•´ìƒë„ ìŠ¤ì¼€ì¼ ì„¤ì • (ì˜ˆ: 2 = 144 DPI)\n",
    "IMAGE_RESOLUTION_SCALE = 2\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    _log = logging.getLogger(__name__)\n",
    "\n",
    "    input_dir = Path(\"./data/ex_pdf\")\n",
    "    output_base_dir = Path(\"./data/images\")\n",
    "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = [f for f in input_dir.iterdir() if f.suffix.lower() == \".pdf\"]\n",
    "\n",
    "    if not pdf_files:\n",
    "        _log.warning(\"No PDF files found.\")\n",
    "        return\n",
    "\n",
    "    # ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì„¤ì •\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            _log.info(f\"Processing: {pdf_file.name}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            conv_res = doc_converter.convert(str(pdf_file))\n",
    "            doc_filename = pdf_file.stem\n",
    "\n",
    "            # PDFë³„ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "            pdf_output_dir = output_base_dir / doc_filename\n",
    "            pdf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # í‘œ ë° ì´ë¯¸ì§€ ì €ì¥\n",
    "            table_counter = 0\n",
    "            picture_counter = 0\n",
    "            for element, _ in conv_res.document.iterate_items():\n",
    "                if isinstance(element, TableItem):\n",
    "                    table_counter += 1\n",
    "                    img_path = pdf_output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "                    with img_path.open(\"wb\") as fp:\n",
    "                        element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "                if isinstance(element, PictureItem):\n",
    "                    picture_counter += 1\n",
    "                    img_path = pdf_output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "                    with img_path.open(\"wb\") as fp:\n",
    "                        element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "            elapsed = time.time() - start_time\n",
    "            _log.info(f\"{pdf_file.name} processed in {elapsed:.2f} seconds.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            _log.error(f\"âŒ Failed to process {pdf_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9qEt7wV8-De",
    "outputId": "be46dd2c-76d8-4f41-e5a8-9c2a16e96845"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqh49TqoG0OX"
   },
   "source": [
    "# 3. VLMì„ ì´ìš©í•´ì„œ ê° Imageì— ëŒ€í•œ ìš”ì•½ ìƒì„±\n",
    "## qwen 2.5 VL 7B-awqì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹…ì„ ìœ„í•œ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹…ì„ ìœ„í•œ ê°œì„ ëœ ì½”ë“œ\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def debug_load_model():\n",
    "    \"\"\"VLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… í•¨ìˆ˜\"\"\"\n",
    "    print(\"ğŸ” VLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. ê¸°ë³¸ í™˜ê²½ í™•ì¸\n",
    "        print(\"1ï¸âƒ£ ê¸°ë³¸ í™˜ê²½ í™•ì¸:\")\n",
    "        print(f\"   ğŸ Python ë²„ì „: {sys.version}\")\n",
    "        print(f\"   ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "        print(f\"   ğŸ–¥ï¸ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "            print(f\"   ğŸ”¢ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "            print(f\"   ğŸ“› GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "        print(\"2ï¸âƒ£ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸:\")\n",
    "        \n",
    "        try:\n",
    "            from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "            print(\"   âœ… transformers ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   âŒ transformers ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            from PIL import Image\n",
    "            print(\"   âœ… PIL ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   âŒ PIL ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "            return None, None\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        # 3. ëª¨ë¸ ì´ë¦„ í™•ì¸\n",
    "        model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # ë˜ëŠ” ì‚¬ìš©í•˜ë ¤ëŠ” ëª¨ë¸ëª…\n",
    "        print(f\"3ï¸âƒ£ ë¡œë”©í•  ëª¨ë¸: {model_name}\")\n",
    "        print()\n",
    "        \n",
    "        # 4. í”„ë¡œì„¸ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸\n",
    "        print(\"4ï¸âƒ£ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...\")\n",
    "        try:\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            print(\"   âœ… í”„ë¡œì„¸ì„œ ë¡œë”© ì„±ê³µ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ í”„ë¡œì„¸ì„œ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            print(f\"   ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # 5. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸\n",
    "        print(\"5ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        try:\n",
    "            model = AutoModelForVision2Seq.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "            )\n",
    "            print(\"   âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            print(f\"   ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # 6. GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"6ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"   ğŸ“Š í• ë‹¹ëœ ë©”ëª¨ë¦¬: {memory_allocated:.2f}GB\")\n",
    "            print(f\"   ğŸ“¦ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {memory_reserved:.2f}GB\")\n",
    "            print(f\"   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {total_memory:.2f}GB\")\n",
    "            print(f\"   ğŸ†“ ì‚¬ìš© ê°€ëŠ¥: {total_memory - memory_reserved:.2f}GB\")\n",
    "        \n",
    "        print()\n",
    "        print(\"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ëª¨ë¸ ë¡œë”© ì„±ê³µ\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(f\"ğŸ“‹ ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” VLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… ì‹œì‘\n",
      "============================================================\n",
      "1ï¸âƒ£ ê¸°ë³¸ í™˜ê²½ í™•ì¸:\n",
      "   ğŸ Python ë²„ì „: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]\n",
      "   ğŸ”¥ PyTorch ë²„ì „: 2.4.1+cu124\n",
      "   ğŸ–¥ï¸ CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "   ğŸ¯ CUDA ë²„ì „: 12.4\n",
      "   ğŸ”¢ GPU ê°œìˆ˜: 1\n",
      "   ğŸ“› GPU ì´ë¦„: NVIDIA GeForce RTX 3090\n",
      "\n",
      "2ï¸âƒ£ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸:\n",
      "   âœ… transformers ì„í¬íŠ¸ ì„±ê³µ\n",
      "   âœ… PIL ì„í¬íŠ¸ ì„±ê³µ\n",
      "\n",
      "3ï¸âƒ£ ë¡œë”©í•  ëª¨ë¸: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "\n",
      "4ï¸âƒ£ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… í”„ë¡œì„¸ì„œ ë¡œë”© ì„±ê³µ\n",
      "\n",
      "5ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:28:01,250 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ea577e42104676a2428de67b3f81b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ\n",
      "\n",
      "6ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\n",
      "   ğŸ“Š í• ë‹¹ëœ ë©”ëª¨ë¦¬: 15.98GB\n",
      "   ğŸ“¦ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: 17.59GB\n",
      "   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: 23.68GB\n",
      "   ğŸ†“ ì‚¬ìš© ê°€ëŠ¥: 6.09GB\n",
      "\n",
      "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ëª¨ë¸ ë¡œë”© ì„±ê³µ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Qwen2_5_VLForConditionalGeneration(\n",
       "   (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "     (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "       (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "     )\n",
       "     (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "     (blocks): ModuleList(\n",
       "       (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "         (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "         (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "         (attn): Qwen2_5_VLVisionFlashAttention2(\n",
       "           (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "           (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "         )\n",
       "         (mlp): Qwen2_5_VLMLP(\n",
       "           (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "           (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "           (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (merger): Qwen2_5_VLPatchMerger(\n",
       "       (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "       (mlp): Sequential(\n",
       "         (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "         (1): GELU(approximate='none')\n",
       "         (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (model): Qwen2_5_VLModel(\n",
       "     (embed_tokens): Embedding(152064, 3584)\n",
       "     (layers): ModuleList(\n",
       "       (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
       "         (self_attn): Qwen2_5_VLFlashAttention2(\n",
       "           (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "           (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "           (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "           (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "           (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "           (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "           (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "         (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "     (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       " ),\n",
       " Qwen2_5_VLProcessor:\n",
       " - image_processor: Qwen2VLImageProcessor {\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"max_pixels\": 12845056,\n",
       "   \"merge_size\": 2,\n",
       "   \"min_pixels\": 3136,\n",
       "   \"patch_size\": 14,\n",
       "   \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"longest_edge\": 12845056,\n",
       "     \"shortest_edge\": 3136\n",
       "   },\n",
       "   \"temporal_patch_size\": 2\n",
       " }\n",
       " \n",
       " - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " }\n",
       " )\n",
       " \n",
       " {\n",
       "   \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
       " })"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ ë‹¨ë…í…ŒíŠ¸íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª LLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...\n",
      "ğŸ§ª LLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… ì‹œì‘\n",
      "============================================================\n",
      "1ï¸âƒ£ ê¸°ë³¸ í™˜ê²½ í™•ì¸:\n",
      "   ğŸ Python ë²„ì „: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0]\n",
      "   ğŸ”¥ PyTorch ë²„ì „: 2.4.1+cu124\n",
      "   ğŸ–¥ï¸ CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "   ğŸ¯ CUDA ë²„ì „: 12.4\n",
      "   ğŸ”¢ GPU ê°œìˆ˜: 1\n",
      "   ğŸ“› GPU ì´ë¦„: NVIDIA GeForce RTX 3090\n",
      "\n",
      "2ï¸âƒ£ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸:\n",
      "   âœ… transformers ì„í¬íŠ¸ ì„±ê³µ\n",
      "\n",
      "3ï¸âƒ£ ë¡œë”©í•  LLM ëª¨ë¸: google/gemma-3-1b-it\n",
      "\n",
      "4ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\n",
      "   âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ\n",
      "\n",
      "5ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 06:30:06,478 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ\n",
      "\n",
      "6ï¸âƒ£ ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸:\n",
      "   âœ… ì¶”ë¡  ì„±ê³µ\n",
      "   ğŸ’¬ ì¶œë ¥ ì˜ˆì‹œ: AIê°€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "\n",
      "AIëŠ” **ì¸ê³µì§€ëŠ¥(Artificial Intelligence)**ì˜ ì•½ìë¡œ, ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ...\n",
      "\n",
      "7ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\n",
      "   ğŸ“Š í• ë‹¹ëœ ë©”ëª¨ë¦¬: 17.85GB\n",
      "   ğŸ“¦ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: 20.02GB\n",
      "   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: 23.68GB\n",
      "   ğŸ†“ ì‚¬ìš© ê°€ëŠ¥: 3.66GB\n",
      "\n",
      "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! LLM ëª¨ë¸ ë¡œë”© ì„±ê³µ\n",
      "\n",
      "ğŸ“‹ ìµœì¢… ê²°ê³¼: ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def debug_load_llm_model():\n",
    "    \"\"\"LLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… í•¨ìˆ˜\"\"\"\n",
    "    print(\"ğŸ§ª LLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹… ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        # 1. ê¸°ë³¸ í™˜ê²½ í™•ì¸\n",
    "        print(\"1ï¸âƒ£ ê¸°ë³¸ í™˜ê²½ í™•ì¸:\")\n",
    "        print(f\"   ğŸ Python ë²„ì „: {sys.version}\")\n",
    "        print(f\"   ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "        print(f\"   ğŸ–¥ï¸ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "            print(f\"   ğŸ”¢ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "            print(f\"   ğŸ“› GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "        print()\n",
    "\n",
    "        # 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "        print(\"2ï¸âƒ£ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸:\")\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "            print(\"   âœ… transformers ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "        except ImportError as e:\n",
    "            print(f\"   âŒ transformers ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # 3. ëª¨ë¸ ì´ë¦„ ì§€ì •\n",
    "        model_name = \"google/gemma-3-4b-it\"  # ì›í•˜ëŠ” LLM ì´ë¦„ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "        print(f\"3ï¸âƒ£ ë¡œë”©í•  LLM ëª¨ë¸: {model_name}\")\n",
    "        print()\n",
    "\n",
    "        # 4. í† í¬ë‚˜ì´ì € ë¡œë”© í…ŒìŠ¤íŠ¸\n",
    "        print(\"4ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"   âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            print(f\"   ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # 5. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸\n",
    "        print(\"5ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"   âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            print(f\"   ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜:\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "\n",
    "        print()\n",
    "\n",
    "        # 6. ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "        print(\"6ï¸âƒ£ ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸:\")\n",
    "        try:\n",
    "            input_text = \"AIê°€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(\"   âœ… ì¶”ë¡  ì„±ê³µ\")\n",
    "            print(f\"   ğŸ’¬ ì¶œë ¥ ì˜ˆì‹œ: {decoded[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # 7. GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\n7ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"   ğŸ“Š í• ë‹¹ëœ ë©”ëª¨ë¦¬: {memory_allocated:.2f}GB\")\n",
    "            print(f\"   ğŸ“¦ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {memory_reserved:.2f}GB\")\n",
    "            print(f\"   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {total_memory:.2f}GB\")\n",
    "            print(f\"   ğŸ†“ ì‚¬ìš© ê°€ëŠ¥: {total_memory - memory_reserved:.2f}GB\")\n",
    "\n",
    "        print()\n",
    "        print(\"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! LLM ëª¨ë¸ ë¡œë”© ì„±ê³µ\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(f\"ğŸ“‹ ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ğŸ§ª LLM ëª¨ë¸ ë¡œë”© ë””ë²„ê¹…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...\")\n",
    "llm_model, llm_tokenizer = debug_load_llm_model()\n",
    "print(f\"\\nğŸ“‹ ìµœì¢… ê²°ê³¼: {'ì„±ê³µ' if llm_model and llm_tokenizer else 'ì‹¤íŒ¨'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. gpu ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜ë“¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "ì´ ì„¹ì…˜ì—ì„œëŠ” GPU ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤. \n",
    "ì£¼ìš” ê¸°ëŠ¥ìœ¼ë¡œëŠ”:\n",
    "\n",
    "1) GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (get_gpu_memory_info)\n",
    "\n",
    "2) ë©”ëª¨ë¦¬ ì •ë¦¬ (cleanup_gpu_memory)\n",
    "\n",
    "3) ë©”ëª¨ë¦¬ ì •ë¦¬ ë™ì‘ ë°©ì‹:\n",
    " 1. gc.collect(): Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰\n",
    " 2. torch.cuda.empty_cache(): PyTorch GPU ìºì‹œ ë¹„ìš°ê¸°\n",
    " 3. torch.cuda.synchronize(): GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°\n",
    "\n",
    "4)  ë©”ëª¨ë¦¬ ì„ê³„ê°’ í™•ì¸ ë° OOM(Out of Memory) ë°©ì§€\n",
    "\n",
    "5)  ì•ˆì „í•œ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NVML ì´ˆê¸°í™” ì„±ê³µ\n",
      "âœ… ê³ ê¸‰ GPU ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU ê´€ë¦¬ë¥¼ ìœ„í•œ ì „ì—­ ì„¤ì • (í´ë˜ìŠ¤ ì •ì˜ ì „ì— ì¶”ê°€)\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()  # ì „ì—­ ì´ˆê¸°í™”\n",
    "    PYNVML_AVAILABLE = True\n",
    "    print(\"âœ… NVML ì´ˆê¸°í™” ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    PYNVML_AVAILABLE = False\n",
    "    print(f\"âš ï¸ NVML ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "class AdvancedGPUManager:\n",
    "    \"\"\"ê³ ê¸‰ GPU í™œìš©ë¥  ìµœì í™” ë° OOM ë°©ì§€ ê´€ë¦¬ì\"\"\"\n",
    "    \n",
    "    def __init__(self, target_utilization=85.0, safety_margin=0.9):\n",
    "        self.target_utilization = target_utilization  # ëª©í‘œ GPU í™œìš©ë¥  (%)\n",
    "        self.safety_margin = safety_margin  # ë©”ëª¨ë¦¬ ì•ˆì „ ë§ˆì§„ (90%)\n",
    "        self.memory_history = []\n",
    "        self.optimal_batch_size = 1\n",
    "        self.max_batch_size = 8\n",
    "        \n",
    "        # pynvml ì´ˆê¸°í™”\n",
    "        if PYNVML_AVAILABLE:\n",
    "            try:\n",
    "                pynvml.nvmlInit()\n",
    "                self.nvml_enabled = True\n",
    "            except:\n",
    "                self.nvml_enabled = False\n",
    "        else:\n",
    "            self.nvml_enabled = False\n",
    "    \n",
    "    def get_gpu_metrics(self):\n",
    "        \"\"\"GPU í™œìš©ë¥ ê³¼ ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìˆ˜ì§‘\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return None\n",
    "            \n",
    "        if self.nvml_enabled:\n",
    "            return self._get_nvml_metrics()\n",
    "        else:\n",
    "            return self._get_torch_memory_info()\n",
    "    \n",
    "    def _get_nvml_metrics(self):\n",
    "        \"\"\"NVMLì„ ì‚¬ìš©í•œ ìƒì„¸í•œ GPU ë©”íŠ¸ë¦­ ìˆ˜ì§‘\"\"\"\n",
    "        try:\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            \n",
    "            # GPU í™œìš©ë¥  ì •ë³´\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë³´\n",
    "            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            total_memory = memory_info.total\n",
    "            used_memory = memory_info.used\n",
    "            free_memory = memory_info.free\n",
    "            \n",
    "            # PyTorch ë©”ëª¨ë¦¬ ì •ë³´\n",
    "            torch_allocated = torch.cuda.memory_allocated()\n",
    "            torch_reserved = torch.cuda.memory_reserved()\n",
    "            \n",
    "            return {\n",
    "                'gpu_utilization': utilization.gpu,  # GPU ì½”ì–´ í™œìš©ë¥ \n",
    "                'memory_utilization': utilization.memory,  # ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë¥ \n",
    "                'total_memory_gb': total_memory / (1024**3),\n",
    "                'used_memory_gb': used_memory / (1024**3),\n",
    "                'free_memory_gb': free_memory / (1024**3),\n",
    "                'memory_usage_percent': (used_memory / total_memory) * 100,\n",
    "                'torch_allocated_gb': torch_allocated / (1024**3),\n",
    "                'torch_reserved_gb': torch_reserved / (1024**3),\n",
    "                'available_memory_gb': (total_memory - torch_reserved) / (1024**3)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ NVML ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n",
    "            return self._get_torch_memory_info()\n",
    "    \n",
    "    def _get_torch_memory_info(self):\n",
    "        \"\"\"PyTorchë§Œì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë©”ëª¨ë¦¬ ì •ë³´\"\"\"\n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        reserved = torch.cuda.memory_reserved()\n",
    "        total = torch.cuda.get_device_properties(0).total_memory\n",
    "        \n",
    "        return {\n",
    "            'gpu_utilization': 0,  # ì¶”ì • ë¶ˆê°€\n",
    "            'memory_utilization': 0,\n",
    "            'total_memory_gb': total / (1024**3),\n",
    "            'used_memory_gb': reserved / (1024**3),\n",
    "            'free_memory_gb': (total - reserved) / (1024**3),\n",
    "            'memory_usage_percent': (reserved / total) * 100,\n",
    "            'torch_allocated_gb': allocated / (1024**3),\n",
    "            'torch_reserved_gb': reserved / (1024**3),\n",
    "            'available_memory_gb': (total - reserved) / (1024**3)\n",
    "        }\n",
    "    \n",
    "    def calculate_optimal_batch_size(self, current_metrics, base_memory_per_item=0.8):\n",
    "        \"\"\"í˜„ì¬ ë©”ëª¨ë¦¬ ìƒí™©ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°\"\"\"\n",
    "        if not current_metrics:\n",
    "            return 1\n",
    "            \n",
    "        # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (ì•ˆì „ ë§ˆì§„ ì ìš©)\n",
    "        available_memory = current_metrics['available_memory_gb'] * self.safety_margin\n",
    "        \n",
    "        # ì˜ˆìƒ ë°°ì¹˜ í¬ê¸° ê³„ì‚° (VLMì€ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©)\n",
    "        estimated_batch_size = max(1, int(available_memory / base_memory_per_item))\n",
    "        \n",
    "        # ìµœëŒ€ê°’ ì œí•œ\n",
    "        optimal_batch_size = min(estimated_batch_size, self.max_batch_size)\n",
    "        \n",
    "        # GPU í™œìš©ë¥ ì´ ë‚®ë‹¤ë©´ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ì‹œë„\n",
    "        if current_metrics['gpu_utilization'] < self.target_utilization and optimal_batch_size < self.max_batch_size:\n",
    "            optimal_batch_size = min(optimal_batch_size + 1, self.max_batch_size)\n",
    "        \n",
    "        return optimal_batch_size\n",
    "    \n",
    "    def should_process_batch(self, batch_size=1):\n",
    "        \"\"\"ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨\"\"\"\n",
    "        metrics = self.get_gpu_metrics()\n",
    "        if not metrics:\n",
    "            return True, 1\n",
    "            \n",
    "        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ì•ˆì „ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°\n",
    "        if metrics['memory_usage_percent'] > (self.safety_margin * 100):\n",
    "            print(f\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ ({metrics['memory_usage_percent']:.1f}%) - ëŒ€ê¸°\")\n",
    "            return False, max(1, batch_size // 2)\n",
    "        \n",
    "        # ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°\n",
    "        optimal_size = self.calculate_optimal_batch_size(metrics)\n",
    "        \n",
    "        return True, optimal_size\n",
    "    \n",
    "    def cleanup_memory(self, intensive=False):\n",
    "        \"\"\"ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            if intensive:\n",
    "                torch.cuda.synchronize()\n",
    "    \n",
    "    def print_detailed_status(self):\n",
    "        \"\"\"ìƒì„¸í•œ GPU ìƒíƒœ ì¶œë ¥\"\"\"\n",
    "        metrics = self.get_gpu_metrics()\n",
    "        if not metrics:\n",
    "            print(\"âŒ GPU ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ–¥ï¸ GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ğŸ¯ GPU ì½”ì–´ í™œìš©ë¥ : {metrics['gpu_utilization']}%\")\n",
    "        print(f\"ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë¥ : {metrics['memory_utilization']}%\")\n",
    "        print(f\"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics['used_memory_gb']:.1f}GB / {metrics['total_memory_gb']:.1f}GB ({metrics['memory_usage_percent']:.1f}%)\")\n",
    "        print(f\"ğŸ†“ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {metrics['available_memory_gb']:.1f}GB\")\n",
    "        print(f\"ğŸ”¥ PyTorch í• ë‹¹ë¨: {metrics['torch_allocated_gb']:.1f}GB\")\n",
    "        print(f\"ğŸ“¦ PyTorch ì˜ˆì•½ë¨: {metrics['torch_reserved_gb']:.1f}GB\")\n",
    "        \n",
    "        # ìƒíƒœ í‰ê°€\n",
    "        if metrics['gpu_utilization'] < 50:\n",
    "            print(\"âš¡ GPU í™œìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ - ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê¶Œì¥\")\n",
    "        elif metrics['gpu_utilization'] > 90:\n",
    "            print(\"ğŸ”¥ GPU í™œìš©ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!\")\n",
    "            \n",
    "        if metrics['memory_usage_percent'] > 85:\n",
    "            print(\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì£¼ì˜ - OOM ìœ„í—˜\")\n",
    "        elif metrics['memory_usage_percent'] < 50:\n",
    "            print(\"âœ… ë©”ëª¨ë¦¬ ì—¬ìœ  ì¶©ë¶„\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "print(\"âœ… ê³ ê¸‰ GPU ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ë°°ì¹˜ ì ì‘í˜• ë° ì ì‘í˜• ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "\n",
    "ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ì„ ì¡°ì •í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤:\n",
    "\n",
    "1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 90% ì´ˆê³¼ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¶•ì†Œ\n",
    "2. ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ 60% ë¯¸ë§Œì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¦ê°€ì‹œì¼œ íš¨ìœ¨ì„± í–¥ìƒ\n",
    "3. ì„ê³„ê°’ ì´ˆê³¼ì‹œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì¼ì‹œì •ì§€í•˜ê³  ë©”ëª¨ë¦¬ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬ê¸° ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class MemoryEfficientBatchProcessor:\n",
    "    \"\"\"ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_batch_size=1, max_batch_size=4):\n",
    "        self.gpu_manager = AdvancedGPUManager()\n",
    "        self.current_batch_size = initial_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.processing_stats = {\n",
    "            'total_processed': 0,\n",
    "            'successful_batches': 0,\n",
    "            'oom_events': 0,\n",
    "            'memory_cleanups': 0,\n",
    "            'batch_size_adjustments': 0\n",
    "        }\n",
    "    \n",
    "    def process_items_efficiently(self, items, process_function, **kwargs):\n",
    "        \"\"\"ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì•„ì´í…œ ì²˜ë¦¬\"\"\"\n",
    "        results = []\n",
    "        total_items = len(items)\n",
    "        processed_count = 0\n",
    "        \n",
    "        print(f\"ğŸš€ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {total_items}ê°œ ì•„ì´í…œ\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while processed_count < total_items:\n",
    "            # í˜„ì¬ GPU ìƒíƒœ í™•ì¸\n",
    "            can_process, optimal_batch_size = self.gpu_manager.should_process_batch(self.current_batch_size)\n",
    "            \n",
    "            if not can_process:\n",
    "                print(\"â¸ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¼ì‹œ ì •ì§€ - ì •ë¦¬ ì¤‘...\")\n",
    "                self._emergency_cleanup()\n",
    "                time.sleep(3)\n",
    "                continue\n",
    "            \n",
    "            # ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "            if optimal_batch_size != self.current_batch_size:\n",
    "                print(f\"ğŸ”§ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {self.current_batch_size} â†’ {optimal_batch_size}\")\n",
    "                self.current_batch_size = optimal_batch_size\n",
    "                self.processing_stats['batch_size_adjustments'] += 1\n",
    "            \n",
    "            # ë°°ì¹˜ ìƒì„±\n",
    "            batch_start = processed_count\n",
    "            batch_end = min(processed_count + self.current_batch_size, total_items)\n",
    "            current_batch = items[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {batch_start+1}-{batch_end}/{total_items} (í¬ê¸°: {len(current_batch)})\")\n",
    "            \n",
    "            try:\n",
    "                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "                batch_results = self._process_batch_safe(current_batch, process_function, **kwargs)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "                processed_count = batch_end\n",
    "                self.processing_stats['successful_batches'] += 1\n",
    "                \n",
    "                # ì§„í–‰ë¥  ë° GPU ìƒíƒœ í‘œì‹œ (5ë°°ì¹˜ë§ˆë‹¤)\n",
    "                if self.processing_stats['successful_batches'] % 5 == 0:\n",
    "                    progress = (processed_count / total_items) * 100\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = processed_count / (elapsed / 60)  # ë¶„ë‹¹ ì²˜ë¦¬ìœ¨\n",
    "                    \n",
    "                    print(f\"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({processed_count}/{total_items}) | ì²˜ë¦¬ìœ¨: {rate:.1f}ê°œ/ë¶„\")\n",
    "                    self.gpu_manager.print_detailed_status()\n",
    "                \n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(f\"ğŸ’¥ OOM ë°œìƒ! ë°°ì¹˜ í¬ê¸° {self.current_batch_size} â†’ {max(1, self.current_batch_size // 2)}\")\n",
    "                self.processing_stats['oom_events'] += 1\n",
    "                self.current_batch_size = max(1, self.current_batch_size // 2)\n",
    "                self._emergency_cleanup()\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "                # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±\n",
    "                individual_results = self._fallback_individual_processing(current_batch, process_function, **kwargs)\n",
    "                results.extend(individual_results)\n",
    "                processed_count = batch_end\n",
    "        \n",
    "        self._print_processing_summary(total_items, time.time() - start_time)\n",
    "        return results\n",
    "    \n",
    "    def _process_batch_safe(self, batch, process_function, **kwargs):\n",
    "        \"\"\"ì•ˆì „í•œ ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í¬í•¨)\"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        for i, item in enumerate(batch):\n",
    "            # ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì²´í¬ (ë°°ì¹˜ ì¤‘ê°„ì—ë„)\n",
    "            if i > 0 and i % 2 == 0:  # 2ê°œë§ˆë‹¤ ì²´í¬\n",
    "                current_metrics = self.gpu_manager.get_gpu_metrics()\n",
    "                if current_metrics and current_metrics['memory_usage_percent'] > 88:\n",
    "                    print(f\"âš ï¸ ë°°ì¹˜ ì¤‘ê°„ ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ ({current_metrics['memory_usage_percent']:.1f}%) - ì •ë¦¬\")\n",
    "                    self._incremental_cleanup()\n",
    "            \n",
    "            # ì•„ì´í…œ ì²˜ë¦¬\n",
    "            try:\n",
    "                result = process_function(item, **kwargs)\n",
    "                batch_results.append(result)\n",
    "                self.processing_stats['total_processed'] += 1\n",
    "                \n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                # ë°°ì¹˜ ì¤‘ê°„ OOM ë°œìƒì‹œ ì¦‰ì‹œ ì •ë¦¬ í›„ ì¬ì‹œë„\n",
    "                print(f\"ğŸ’¥ ë°°ì¹˜ ì¤‘ê°„ OOM - ê¸´ê¸‰ ì •ë¦¬ í›„ ì¬ì‹œë„\")\n",
    "                self._emergency_cleanup()\n",
    "                result = process_function(item, **kwargs)\n",
    "                batch_results.append(result)\n",
    "                self.processing_stats['total_processed'] += 1\n",
    "        \n",
    "        # ë°°ì¹˜ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        self._incremental_cleanup()\n",
    "        return batch_results\n",
    "    \n",
    "    def _fallback_individual_processing(self, batch, process_function, **kwargs):\n",
    "        \"\"\"ê°œë³„ ì²˜ë¦¬ í´ë°±\"\"\"\n",
    "        print(f\"ğŸ”„ ê°œë³„ ì²˜ë¦¬ ëª¨ë“œë¡œ ì „í™˜ ({len(batch)}ê°œ ì•„ì´í…œ)\")\n",
    "        individual_results = []\n",
    "        \n",
    "        for item in batch:\n",
    "            try:\n",
    "                self._incremental_cleanup()  # ê° ì•„ì´í…œ ì²˜ë¦¬ ì „ ì •ë¦¬\n",
    "                result = process_function(item, **kwargs)\n",
    "                individual_results.append(result)\n",
    "                self.processing_stats['total_processed'] += 1\n",
    "            except Exception as item_error:\n",
    "                print(f\"âŒ ê°œë³„ ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨: {item_error}\")\n",
    "                individual_results.append(None)\n",
    "        \n",
    "        return individual_results\n",
    "    \n",
    "    def _incremental_cleanup(self):\n",
    "        \"\"\"ì ì§„ì  ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        self.processing_stats['memory_cleanups'] += 1\n",
    "    \n",
    "    def _emergency_cleanup(self):\n",
    "        \"\"\"ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "        print(\"ğŸ§¹ ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰...\")\n",
    "        self.gpu_manager.cleanup_memory(intensive=True)\n",
    "        time.sleep(2)\n",
    "        self.processing_stats['memory_cleanups'] += 1\n",
    "    \n",
    "    def _print_processing_summary(self, total_items, total_time):\n",
    "        \"\"\"ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"ğŸ“· ì´ ì²˜ë¦¬ ì•„ì´í…œ: {self.processing_stats['total_processed']}/{total_items}\")\n",
    "        print(f\"ğŸ“¦ ì„±ê³µí•œ ë°°ì¹˜: {self.processing_stats['successful_batches']}\")\n",
    "        print(f\"ğŸ’¥ OOM ë°œìƒ íšŸìˆ˜: {self.processing_stats['oom_events']}\")\n",
    "        print(f\"ğŸ”§ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {self.processing_stats['batch_size_adjustments']}íšŒ\")\n",
    "        print(f\"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ íšŸìˆ˜: {self.processing_stats['memory_cleanups']}\")\n",
    "        print(f\"âš¡ ìµœì¢… ë°°ì¹˜ í¬ê¸°: {self.current_batch_size}\")\n",
    "        print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "        print(f\"ğŸ“ˆ í‰ê·  ì²˜ë¦¬ìœ¨: {self.processing_stats['total_processed']/(total_time/60):.1f}ê°œ/ë¶„\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "print(\"âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬ê¸° ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. xmlíŒŒì‹±ë° ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… í•¨ìˆ˜ë“¤\n",
    "1. VLM(Vision Language Model) ì¶œë ¥ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤:\n",
    "2. XML í˜•íƒœì˜ ëª¨ë¸ ì¶œë ¥ì„ íŒŒì‹±í•˜ì—¬ ì œëª©, ìš”ì•½, ì—”í‹°í‹°, ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¦¬\n",
    "3. ë§ˆí¬ë‹¤ìš´ í˜•íƒœì˜ ì‘ë‹µë„ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "4. íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì í™”ëœ ì¶œë ¥ íŒŒì„œ ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class OptimizedOutputParser:\n",
    "    \"\"\"VLM ì¶œë ¥ íŒŒì‹±ì„ ìœ„í•œ ìµœì í™”ëœ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ ì»´íŒŒì¼ (ì„±ëŠ¥ ìµœì í™”)\n",
    "    XML_PATTERNS = {\n",
    "        'title': re.compile(r'<title>(.*?)</title>', re.DOTALL | re.IGNORECASE),\n",
    "        'summary': re.compile(r'<summary>(.*?)</summary>', re.DOTALL | re.IGNORECASE),\n",
    "        'entities': re.compile(r'<entities>(.*?)</entities>', re.DOTALL | re.IGNORECASE),\n",
    "        'questions': re.compile(r'<hypothetical_questions>(.*?)</hypothetical_questions>', re.DOTALL | re.IGNORECASE)\n",
    "    }\n",
    "    \n",
    "    MARKDOWN_PATTERNS = {\n",
    "        'title': re.compile(r'###\\s*ğŸ–¼ï¸\\s*[ì´ë¯¸ì§€\\s]*ì œëª©[\\s\\S]*?\\n(.+?)\\n', re.IGNORECASE),\n",
    "        'summary': re.compile(r'###\\s*ğŸ“‹\\s*[ì´ë¯¸ì§€\\s]*ìš”ì•½[\\s\\S]*?\\n([\\s\\S]*?)(?=###|$)', re.IGNORECASE),\n",
    "        'entities': re.compile(r'###\\s*ğŸ·ï¸\\s*[í•µì‹¬\\s]*ì—”í‹°í‹°[\\s\\S]*?\\n([\\s\\S]*?)(?=###|$)', re.IGNORECASE),\n",
    "        'questions': re.compile(r'###\\s*â“\\s*[ê°€ìƒì§ˆë¬¸|ê´€ë ¨ì§ˆë¬¸][\\s\\S]*?\\n([\\s\\S]*?)(?=###|$)', re.IGNORECASE)\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_image_summary(cls, raw_output):\n",
    "        \"\"\"í†µí•© íŒŒì‹± í•¨ìˆ˜ (XML ìš°ì„ , ë§ˆí¬ë‹¤ìš´ ë°±ì—…)\"\"\"\n",
    "        # ê¸°ë³¸ êµ¬ì¡°\n",
    "        parsed_data = {\n",
    "            'title': '',\n",
    "            'summary': '',\n",
    "            'entities': '',\n",
    "            'hypothetical_questions': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # XML íŒŒì‹± ì‹œë„ (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)\n",
    "            for key, pattern in cls.XML_PATTERNS.items():\n",
    "                match = pattern.search(raw_output)\n",
    "                if match:\n",
    "                    parsed_data[key] = match.group(1).strip()\n",
    "            \n",
    "            # XMLì´ ì‹¤íŒ¨í•˜ë©´ ë§ˆí¬ë‹¤ìš´ íŒŒì‹±\n",
    "            if not any(parsed_data.values()):\n",
    "                markdown_keys = {'title': 'title', 'summary': 'summary', 'entities': 'entities', 'questions': 'hypothetical_questions'}\n",
    "                for md_key, data_key in markdown_keys.items():\n",
    "                    if md_key in cls.MARKDOWN_PATTERNS:\n",
    "                        match = cls.MARKDOWN_PATTERNS[md_key].search(raw_output)\n",
    "                        if match:\n",
    "                            parsed_data[data_key] = match.group(1).strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "            # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ summaryì— ì €ì¥\n",
    "            parsed_data['summary'] = raw_output[:500] + \"...\" if len(raw_output) > 500 else raw_output\n",
    "        \n",
    "        return parsed_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_formatted_markdown(parsed_data):\n",
    "        \"\"\"íŒŒì‹±ëœ ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (ìµœì í™”ë¨)\"\"\"\n",
    "        return f\"\"\"# ğŸ“Š ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼\n",
    "\n",
    "## ğŸ–¼ï¸ {parsed_data.get('title', 'ì œëª© ì—†ìŒ')}\n",
    "\n",
    "### ğŸ“‹ ìš”ì•½\n",
    "{parsed_data.get('summary', 'ë¶„ì„ ë‚´ìš© ì—†ìŒ')}\n",
    "\n",
    "### ğŸ·ï¸ í•µì‹¬ ì—”í‹°í‹°\n",
    "{parsed_data.get('entities', 'ì—”í‹°í‹° ì—†ìŒ')}\n",
    "\n",
    "### â“ ê´€ë ¨ ì§ˆë¬¸ë“¤\n",
    "{parsed_data.get('hypothetical_questions', 'ì§ˆë¬¸ ì—†ìŒ')}\n",
    "\n",
    "---\n",
    "*ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "\n",
    "# ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ìƒˆ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´\n",
    "def parse_image_summary_xml(raw_output):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    return OptimizedOutputParser.parse_image_summary(raw_output)\n",
    "\n",
    "def parse_markdown_response(raw_output):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    return OptimizedOutputParser.parse_image_summary(raw_output)\n",
    "\n",
    "def create_formatted_markdown_from_parsed(parsed_data):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    return OptimizedOutputParser.create_formatted_markdown(parsed_data)\n",
    "\n",
    "print(\"âœ… ìµœì í™”ëœ ì¶œë ¥ íŒŒì„œ ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. íŒŒì¼ ì €ì¥ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì í™”ëœ íŒŒì¼ ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class OptimizedFileManager:\n",
    "    \"\"\"íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ìµœì í™”ëœ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_output_directories(base_output_dir=\"./analysis_output\"):\n",
    "        \"\"\"ì¶œë ¥ìš© ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„± (pathlib ì‚¬ìš©)\"\"\"\n",
    "        base_path = Path(base_output_dir)\n",
    "        directories = {\n",
    "            'base': base_path,\n",
    "            'xml': base_path / \"xml_results\",\n",
    "            'markdown': base_path / \"markdown_results\", \n",
    "            'json': base_path / \"json_results\"\n",
    "        }\n",
    "        \n",
    "        # ë””ë ‰í† ë¦¬ ìƒì„± (í•œ ë²ˆì— ì²˜ë¦¬)\n",
    "        for dir_name, dir_path in directories.items():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {base_path}\")\n",
    "        return directories\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_results_as_markdown(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "        \"\"\"ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ìµœì í™”ë¨)\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_{timestamp}.md\"\n",
    "            output_file = output_dirs['markdown'] / filename\n",
    "            \n",
    "            # í…œí”Œë¦¿ ë¬¸ìì—´ë¡œ í•œ ë²ˆì— ìƒì„±\n",
    "            content_parts = [\n",
    "                \"# ğŸ“Š ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼\\n\",\n",
    "                f\"**ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                f\"**ì´ ë¶„ì„ ì´ë¯¸ì§€:** {len(results)}ê°œ\\n\",\n",
    "                \"---\\n\"\n",
    "            ]\n",
    "            \n",
    "            # ê° ì´ë¯¸ì§€ ê²°ê³¼ ì¶”ê°€\n",
    "            for i, (image_path, analysis) in enumerate(results, 1):\n",
    "                content_parts.extend([\n",
    "                    f\"\\n## ğŸ–¼ï¸ ì´ë¯¸ì§€ {i}: {Path(image_path).name}\\n\",\n",
    "                    f\"**ğŸ“ íŒŒì¼ ê²½ë¡œ:** `{image_path}`\\n\",\n",
    "                    f\"**ğŸ” ë¶„ì„ ê²°ê³¼:**\\n{analysis}\\n\",\n",
    "                    \"---\\n\"\n",
    "                ])\n",
    "            \n",
    "            # í•œ ë²ˆì— íŒŒì¼ ì“°ê¸°\n",
    "            output_file.write_text(''.join(content_parts), encoding='utf-8')\n",
    "            \n",
    "            print(f\"âœ… ë§ˆí¬ë‹¤ìš´ ì €ì¥: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_results_as_json(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "        \"\"\"JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ìµœì í™”ë¨)\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_{timestamp}.json\"\n",
    "            output_file = output_dirs['json'] / filename\n",
    "            \n",
    "            # JSON ë°ì´í„° êµ¬ì¡° ìƒì„± (list comprehension ì‚¬ìš©)\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"generated_at\": datetime.now().isoformat(),\n",
    "                    \"total_images\": len(results),\n",
    "                    \"format_version\": \"1.0\"\n",
    "                },\n",
    "                \"analysis_results\": [\n",
    "                    {\n",
    "                        \"id\": i,\n",
    "                        \"file_path\": image_path,\n",
    "                        \"filename\": Path(image_path).name,\n",
    "                        \"analysis\": analysis,\n",
    "                        \"word_count\": len(analysis.split()),\n",
    "                        \"processed_at\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    for i, (image_path, analysis) in enumerate(results, 1)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # JSON íŒŒì¼ ì €ì¥\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"âœ… JSON ì €ì¥: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ìƒˆ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´\n",
    "def create_output_directories(base_output_dir=\"./output\"):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    return OptimizedFileManager.create_output_directories(base_output_dir)\n",
    "\n",
    "def save_results_as_markdown(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    return OptimizedFileManager.save_results_as_markdown(results, output_dirs, filename_prefix)\n",
    "\n",
    "def save_results_as_json(results, output_dirs, filename_prefix=\"image_analysis\"):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    return OptimizedFileManager.save_results_as_json(results, output_dirs, filename_prefix)\n",
    "\n",
    "print(\"âœ… ìµœì í™”ëœ íŒŒì¼ ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. vlm ëª¨ë¸ ë¡œë”© ë° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì í™”ëœ VLM ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class OptimizedVLMManager:\n",
    "    \"\"\"VLM ê´€ë¦¬ì\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "        self.gpu_manager = AdvancedGPUManager()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"ğŸš€  GPU ìµœì í™” Qwen2.5-VL ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ”„ {self.model_name} ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "            \n",
    "            # ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            self.gpu_manager.cleanup_memory(intensive=True)\n",
    "            \n",
    "            # ëª¨ë¸ ë¡œë”© (AWQ ìµœì í™”)\n",
    "            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                attn_implementation=\"flash_attention_2\",  # ì„±ëŠ¥ ìµœì í™”\n",
    "                device_map=\"auto\",  # ìë™ GPU ë°°ì¹˜\n",
    "                trust_remote_code=True,  # ì›ê²© ì½”ë“œ ì‹ ë¢°\n",
    "                low_cpu_mem_usage=True,  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”\n",
    "            )\n",
    "            \n",
    "            # í”„ë¡œì„¸ì„œ ë¡œë”©\n",
    "            self.processor = AutoProcessor.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # AWQ ëª¨ë¸ ìµœì í™” ì„¤ì • (half() í˜¸ì¶œ ì œê±°)\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "                print(f\"âœ… ëª¨ë¸ ìµœì í™” ì™„ë£Œ (dtype: {self.model.dtype})\")\n",
    "            \n",
    "            device_info = f\"CPU\" if not torch.cuda.is_available() else f\"GPU ({torch.cuda.get_device_name(0)})\"\n",
    "            print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë””ë°”ì´ìŠ¤: {device_info}\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "            self.gpu_manager.print_detailed_status()\n",
    "            \n",
    "            return self.model, self.processor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "            print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "            print(\"   1. Hugging Face ë¡œê·¸ì¸ í™•ì¸: huggingface_hub.login()\")\n",
    "            print(\"   2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸\")\n",
    "            print(\"   3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\")\n",
    "            return None, None\n",
    "    \n",
    "    def get_optimized_prompt(self):\n",
    "        \"\"\"ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë°˜í™˜\"\"\"\n",
    "        return \"\"\"You are an expert in extracting useful information from IMAGE.\n",
    "Extract key entities, summarize them, and write useful information for retrieval.\n",
    "Provide five hypothetical questions based on the image.\n",
    "\n",
    "Please analyze this image and provide a structured summary in the following markdown format:\n",
    "\n",
    "### ğŸ–¼ï¸ Image Title\n",
    "[Write a clear, descriptive title for the image in Korean]\n",
    "\n",
    "### ğŸ“‹ Image Summary\n",
    "[Provide a comprehensive summary of the image content in Korean - describe what you see, key information, trends, patterns, etc.]\n",
    "\n",
    "### ğŸ·ï¸ Key Entities\n",
    "[List the main entities, keywords, and important elements found in the image in Korean]\n",
    "\n",
    "### â“ Hypothetical Questions\n",
    "1. [Question 1]\n",
    "2. [Question 2] \n",
    "3. [Question 3]\n",
    "4. [Question 4]\n",
    "5. [Question 5]\n",
    "\n",
    "Important: \n",
    "- All output must be in Korean\n",
    "- Focus on financial/business content if applicable\n",
    "- Include specific numbers, percentages, or data points if visible\n",
    "- Make the summary detailed and informative for retrieval purposes\"\"\"\n",
    "    \n",
    "    def process_single_image_safe(self, image_path, max_retries=3):\n",
    "        \"\"\"ğŸ›¡ï¸ OOM ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œ ì•ˆì „í•œ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬\"\"\"\n",
    "        if not self.model or not self.processor:\n",
    "            print(\"âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        prompt_text = self.get_optimized_prompt()\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # ğŸ” ì²˜ë¦¬ ì „ ë©”ëª¨ë¦¬ í™•ì¸\n",
    "                memory_info = self.gpu_manager.get_gpu_metrics()\n",
    "                if memory_info and memory_info['available_memory_gb'] < 3.0:  # 3GB ì„ê³„ê°’\n",
    "                    print(f\"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ({memory_info['available_memory_gb']:.1f}GB) - ì •ë¦¬ ì¤‘...\")\n",
    "                    self.gpu_manager.cleanup_memory(intensive=True)\n",
    "                    time.sleep(2)\n",
    "                \n",
    "                # ğŸ“¸ ì´ë¯¸ì§€ ë¡œë“œ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                except Exception as img_error:\n",
    "                    print(f\"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ {image_path}: {img_error}\")\n",
    "                    return f\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(img_error)}\"\n",
    "                \n",
    "                # ë©”ì‹œì§€ êµ¬ì„±\n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": prompt_text}\n",
    "                    ]\n",
    "                }]\n",
    "                \n",
    "                # ğŸ”§ ì…ë ¥ ì¤€ë¹„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                image_inputs, video_inputs = process_vision_info(messages)\n",
    "                inputs = self.processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # GPUë¡œ ì´ë™\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.to(\"cuda\")\n",
    "                \n",
    "                # ğŸ¯ ì‘ë‹µ ìƒì„± (ìµœì í™”ëœ ì„¤ì •)\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(\n",
    "                        **inputs, \n",
    "                        max_new_tokens=768,  # ë” ê¸´ ì‘ë‹µ í—ˆìš©\n",
    "                        do_sample=True,  # ìƒ˜í”Œë§ í™œì„±í™”\n",
    "                        temperature=0.7,  # ì°½ì˜ì„± ì¡°ì ˆ\n",
    "                        top_p=0.8,  # í† í° ì„ íƒ ë‹¤ì–‘ì„±\n",
    "                        pad_token_id=self.processor.tokenizer.eos_token_id\n",
    "                    )\n",
    "                    \n",
    "                    # ì‘ë‹µ ë””ì½”ë”©\n",
    "                    generated_ids_trimmed = [\n",
    "                        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                    ]\n",
    "                    output_text = self.processor.batch_decode(\n",
    "                        generated_ids_trimmed, \n",
    "                        skip_special_tokens=True, \n",
    "                        clean_up_tokenization_spaces=False\n",
    "                    )\n",
    "                \n",
    "                # ğŸ§¹ ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "                del inputs, generated_ids, generated_ids_trimmed, image\n",
    "                self.gpu_manager.cleanup_memory()\n",
    "                \n",
    "                result = output_text[0] if output_text else \"ë¶„ì„ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\"\n",
    "                \n",
    "                # ê²°ê³¼ ê²€ì¦\n",
    "                if len(result.strip()) < 100:\n",
    "                    print(f\"âš ï¸ ì§§ì€ ì‘ë‹µ ê°ì§€ ({len(result)} ë¬¸ì) - ì¬ì‹œë„...\")\n",
    "                    continue\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                print(f\"ğŸ’¥ OOM ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries})\")\n",
    "                self.gpu_manager.cleanup_memory(intensive=True)\n",
    "                time.sleep(5)  # OOM í›„ ë” ì˜¤ë˜ ëŒ€ê¸°\n",
    "                \n",
    "                if attempt == max_retries - 1:\n",
    "                    return f\"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ {Path(image_path).name} ì²˜ë¦¬ ì‹¤íŒ¨\"\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì²˜ë¦¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return f\"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}\"\n",
    "                time.sleep(2)\n",
    "        \n",
    "        return f\"{max_retries}ë²ˆ ì‹œë„ í›„ {Path(image_path).name} ì²˜ë¦¬ ì‹¤íŒ¨\"\n",
    "\n",
    "# ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ í˜¸í™˜ì„± ìœ ì§€\n",
    "def load_model():\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    vlm_manager = OptimizedVLMManager()\n",
    "    return vlm_manager.load_model()\n",
    "\n",
    "def process_single_image_safe(model, processor, image_path, max_retries=3):\n",
    "    \"\"\"ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€\"\"\"\n",
    "    vlm_manager = OptimizedVLMManager()\n",
    "    vlm_manager.model = model\n",
    "    vlm_manager.processor = processor\n",
    "    return vlm_manager.process_single_image_safe(image_path, max_retries)\n",
    "\n",
    "print(\"âœ… ìµœì í™”ëœ VLM ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. GPU ìµœì í™” ë©”ì¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU ìµœì í™” ë©”ì¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def gpu_optimized_main_pipeline(image_dir=\"./data/images\", output_dir=\"./analysis_output\"):\n",
    "    \"\"\"GPU í™œìš©ë¥  ìµœì í™”ëœ ë©”ì¸ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ GPU ìµœì í™” MultiModal RAG ì‹œìŠ¤í…œ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. ê³ ê¸‰ GPU ê´€ë¦¬ì ë° ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”\n",
    "    gpu_manager = AdvancedGPUManager(target_utilization=85.0)\n",
    "    batch_processor = MemoryEfficientBatchProcessor(initial_batch_size=2, max_batch_size=6)\n",
    "    \n",
    "    # ì´ˆê¸° GPU ìƒíƒœ í™•ì¸\n",
    "    print(\"ğŸ” ì´ˆê¸° GPU ìƒíƒœ:\")\n",
    "    gpu_manager.print_detailed_status()\n",
    "    \n",
    "    # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™” \n",
    "    output_dirs = OptimizedFileManager.create_output_directories(output_dir)\n",
    "    \n",
    "    # 3. ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [\n",
    "        str(path) for path in Path(image_dir).rglob('*')\n",
    "        if path.suffix.lower() in image_extensions and path.is_file()\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"âŒ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“· ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ\")\n",
    "    \n",
    "    # 4. VLM ëª¨ë¸ ë¡œë”©\n",
    "    try:\n",
    "        model, processor = load_model()\n",
    "        if not model:\n",
    "            print(\"âŒ VLM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "    def process_single_image_optimized(image_path):\n",
    "        \"\"\"ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (ìµœì í™”ë¨)\"\"\"\n",
    "        return process_single_image_safe(model, processor, image_path, max_retries=2)\n",
    "    \n",
    "    # 6. GPU ìµœì í™” ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰\n",
    "    print(f\"ğŸ”„ GPU ìµœì í™” ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš©\n",
    "    analysis_results = batch_processor.process_items_efficiently(\n",
    "        image_files, \n",
    "        process_single_image_optimized\n",
    "    )\n",
    "    \n",
    "    # 7. ê²°ê³¼ í•„í„°ë§ ë° ì •ë¦¬\n",
    "    valid_results = []\n",
    "    for i, (image_path, analysis) in enumerate(zip(image_files, analysis_results)):\n",
    "        if analysis and len(str(analysis).strip()) > 50:\n",
    "            valid_results.append((image_path, analysis))\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"âŒ ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ ì—†ìŒ\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ! ì„±ê³µ: {len(valid_results)}/{len(image_files)}ê°œ\")\n",
    "    \n",
    "    # 8. ê²°ê³¼ ì €ì¥ \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename_prefix = f\"gpu_optimized_analysis_{timestamp}\"\n",
    "    \n",
    "    print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
    "    \n",
    "    md_file = OptimizedFileManager.save_results_as_markdown(valid_results, output_dirs, filename_prefix)\n",
    "    json_file = OptimizedFileManager.save_results_as_json(valid_results, output_dirs, filename_prefix)\n",
    "    \n",
    "    # 9. ìµœì¢… ì •ë¦¬ ë° ê²°ê³¼ ìš”ì•½\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nğŸ“Š GPU ìµœì í™” ì²˜ë¦¬ ìš”ì•½:\")\n",
    "    print(f\"   ğŸ“· ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(valid_results)}ê°œ\")\n",
    "    print(f\"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "    print(f\"   âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(valid_results)/(total_time/60):.1f}ê°œ/ë¶„\")\n",
    "    print(f\"   ğŸ“ ì¶œë ¥ ê²½ë¡œ: {output_dirs['base']}\")\n",
    "    \n",
    "    # ìµœì¢… GPU ìƒíƒœ í™•ì¸\n",
    "    print(\"\\nğŸ” ìµœì¢… GPU ìƒíƒœ:\")\n",
    "    gpu_manager.print_detailed_status()\n",
    "    \n",
    "    return {\n",
    "        'results': valid_results,\n",
    "        'output_dirs': output_dirs,\n",
    "        'files': {'markdown': md_file, 'json': json_file},\n",
    "        'processing_time': total_time,\n",
    "        'gpu_stats': batch_processor.processing_stats\n",
    "    }\n",
    "\n",
    "print(\"âœ… GPU ìµœì í™” ë©”ì¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– 2. LLM (Large Language Model) íŒŒì´í”„ë¼ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”§ 1ë‹¨ê³„: í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM íŒŒì´í”„ë¼ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "=======================================================================\n",
    "ğŸ¤– LLM í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ì„ íŒŒì´í”„ë¼ì¸ (VLM íŒŒì´í”„ë¼ì¸ ìŠ¤íƒ€ì¼)\n",
    "=======================================================================\n",
    "VLM ì´ë¯¸ì§€ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì˜ êµ¬ì¡°ë¥¼ ë”°ë¼ êµ¬í˜„ëœ \n",
    "Gemma-3-1B-ITë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ì„ ì‹œìŠ¤í…œ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ì–µì œ\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… LLM íŒŒì´í”„ë¼ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ 2ë‹¨ê³„: GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (VLM íŒŒì´í”„ë¼ì¸ì—ì„œ ì¬ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class LLMGPUMemoryManager:\n",
    "    \"\"\"LLM íŒŒì´í”„ë¼ì¸ìš© ê³ ê¸‰ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì (VLM ìŠ¤íƒ€ì¼)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleanup():\n",
    "        \"\"\"GPU ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                print(\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_status():\n",
    "        \"\"\"GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            allocated = torch.cuda.memory_allocated()\n",
    "            reserved = torch.cuda.memory_reserved()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory\n",
    "            \n",
    "            allocated_gb = allocated / (1024**3)\n",
    "            reserved_gb = reserved / (1024**3)\n",
    "            total_gb = total / (1024**3)\n",
    "            free_gb = (total - reserved) / (1024**3)\n",
    "            usage_percent = (allocated / total) * 100\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ğŸ–¥ï¸ LLM GPU ë©”ëª¨ë¦¬ ìƒíƒœ\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"ğŸ“Š ì´ ë©”ëª¨ë¦¬: {total_gb:.2f} GB\")\n",
    "            print(f\"ğŸ’¾ ì‚¬ìš© ì¤‘: {allocated_gb:.2f} GB ({usage_percent:.1f}%)\")\n",
    "            print(f\"ğŸ“¦ ì˜ˆì•½ë¨: {reserved_gb:.2f} GB\")\n",
    "            print(f\"ğŸ†“ ì‚¬ìš© ê°€ëŠ¥: {free_gb:.2f} GB\")\n",
    "            \n",
    "            if usage_percent > 85:\n",
    "                print(\"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ!\")\n",
    "            else:\n",
    "                print(\"âœ… GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì–‘í˜¸\")\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_available_memory():\n",
    "        \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬ ë°˜í™˜ (GB)\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            reserved = torch.cuda.memory_reserved()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory\n",
    "            return (total - reserved) / (1024**3)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "print(\"âœ… LLM GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“‹ 3ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ë¦¬ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'ğŸ“–' (U+1F4D6) (2547790868.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ğŸ“– extract_clean_text()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'ğŸ“–' (U+1F4D6)\n"
     ]
    }
   ],
   "source": [
    "ğŸ“– extract_clean_text()\n",
    "â”œâ”€â”€ í…Œì´ë¸” ë¼ì¸ ì œê±° (| íŒ¨í„´ í•„í„°ë§)\n",
    "â”œâ”€â”€ ê³µë°± ì •ê·œí™”\n",
    "â””â”€â”€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "\n",
    "ğŸ§  Kiwi í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„\n",
    "â”œâ”€â”€ ë¬¸ì¥ ê²½ê³„ íƒì§€\n",
    "â”œâ”€â”€ ë¬¸ì¥ë³„ ë¶„ë¦¬ (start, end ìœ„ì¹˜ í¬í•¨)\n",
    "â””â”€â”€ ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´\n",
    "\n",
    "ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹\n",
    "â”œâ”€â”€ ìµœëŒ€ ì²­í¬ í¬ê¸°: 1200ì\n",
    "â”œâ”€â”€ ë¬¸ì¥ ê²½ê³„ ë³´ì¡´\n",
    "â””â”€â”€ ì˜ˆìƒ ê²°ê³¼: 50-80ê°œ ì²­í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "ğŸ² BeautifulSoup HTML íŒŒì‹±\n",
    "â”œâ”€â”€ <table> íƒœê·¸ ê°ì§€\n",
    "â”œâ”€â”€ <tr>, <th>, <td> ì…€ ì¶”ì¶œ\n",
    "â””â”€â”€ í…ìŠ¤íŠ¸ ë‚´ìš© ì •ë¦¬\n",
    "\n",
    "ğŸ“Š ë§ˆí¬ë‹¤ìš´ ë³€í™˜\n",
    "â”œâ”€â”€ HTML â†’ | êµ¬ë¶„ì í…Œì´ë¸”\n",
    "â”œâ”€â”€ í—¤ë” êµ¬ë¶„ì„  (---) ì¶”ê°€\n",
    "â””â”€â”€ ì˜ˆìƒ ê²°ê³¼: 10-20ê°œ í…Œì´ë¸”\n",
    "\n",
    "# ë³€í™˜ ì˜ˆì‹œ:\n",
    "HTML: <table><tr><th>í•­ëª©</th><th>ê°’</th></tr><tr><td>GDP</td><td>2.1%</td></tr></table>\n",
    "â†“\n",
    "ë§ˆí¬ë‹¤ìš´: | í•­ëª© | ê°’ |\n",
    "         |------|-----|\n",
    "         | GDP  | 2.1% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "ğŸ¤– Gemma-3-1B-IT ëª¨ë¸ ë¡œë”©\n",
    "â”œâ”€â”€ Eager ëª¨ë“œ (Dynamo ì˜¤ë¥˜ ë°©ì§€)\n",
    "â”œâ”€â”€ BFloat16 ì •ë°€ë„\n",
    "â””â”€â”€ CUDA GPU í™œìš©\n",
    "\n",
    "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬\n",
    "â”œâ”€â”€ ì´ ì•„ì´í…œ: 60-100ê°œ (í…ìŠ¤íŠ¸ + í…Œì´ë¸”)\n",
    "â”œâ”€â”€ ë™ì  ë°°ì¹˜ í¬ê¸°: 4-8ê°œ\n",
    "â””â”€â”€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬\n",
    "\n",
    "ğŸ“„ ë¶„ì„ ê²°ê³¼\n",
    "â”œâ”€â”€ ìš”ì•½ ìƒì„±\n",
    "â”œâ”€â”€ ê°€ì„¤ì  ì§ˆë¬¸ ìƒì„±\n",
    "â””â”€â”€ ì½˜í…ì¸  íƒ€ì… ë¶„ë¥˜\n",
    "\n",
    "\n",
    "\n",
    "ğŸ’¾ íŒŒì¼ ì €ì¥\n",
    "â”œâ”€â”€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ (ë¶„ì„ ê²°ê³¼)\n",
    "â”œâ”€â”€ JSON í˜•ì‹ (êµ¬ì¡°í™”ëœ ë°ì´í„°)\n",
    "â””â”€â”€ íƒ€ì…ë³„ ë¶„ë¦¬ ì €ì¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizedTextTableProcessor ì „ì²´ í´ë˜ìŠ¤ ì½”ë“œ (IndentationError ìˆ˜ì • í¬í•¨)\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "class OptimizedTextTableProcessor:\n",
    "    \"\"\"ê°œì„ ëœ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ë¶„ë¦¬ ì²˜ë¦¬ê¸° - Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ ì •í™•í•œ ë¬¸ì¥ ë¶„ë¦¬ ì²­í‚¹\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.table_pattern = re.compile(r'^\\s*\\|.*\\|\\s*$', re.MULTILINE)\n",
    "        self.table_separator_pattern = re.compile(r'^\\s*\\|[\\s\\-:]*\\|\\s*$', re.MULTILINE)\n",
    "\n",
    "        print(\"ğŸ”„ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...\")\n",
    "        self.kiwi = Kiwi()\n",
    "        print(\"âœ… Kiwi ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "        self.processing_stats = {\n",
    "            'total_files': 0,\n",
    "            'tables_extracted': 0,\n",
    "            'text_chunks_created': 0,\n",
    "            'processing_time': 0,\n",
    "            'kiwi_sentences_analyzed': 0,\n",
    "            'average_chunk_length': 0\n",
    "        }\n",
    "\n",
    "    def process_markdown_file(self, text_dir, table_dir, output_dir=\"./text_extraction_output\"):\n",
    "        start_time = time.time()\n",
    "\n",
    "        text_path = Path(text_dir)\n",
    "        table_path = Path(table_dir)\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"ğŸ“– í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        text_files = list(text_path.rglob(\"*.md\"))\n",
    "        all_text_chunks = []\n",
    "\n",
    "        if not text_files:\n",
    "            print(f\"âŒ {text_dir}ì—ì„œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        else:\n",
    "            print(f\"ğŸ“ {len(text_files)}ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë°œê²¬\")\n",
    "\n",
    "            for i, text_file in enumerate(text_files, 1):\n",
    "                print(f\"ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: {text_file.name} ({i}/{len(text_files)})\")\n",
    "\n",
    "                try:\n",
    "                    content = text_file.read_text(encoding='utf-8')\n",
    "                    clean_text = self.extract_clean_text(content)\n",
    "\n",
    "                    if clean_text.strip():\n",
    "                        text_chunks = self._intelligent_text_chunking(clean_text, text_file.name)\n",
    "                        all_text_chunks.extend(text_chunks)\n",
    "\n",
    "                        self.processing_stats['total_files'] += 1\n",
    "                        self.processing_stats['text_chunks_created'] += len(text_chunks)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ {text_file.name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "        print(\"ğŸ“Š í…Œì´ë¸” íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        all_tables = self.load_table_files(table_path)\n",
    "        self.processing_stats['tables_extracted'] = len(all_tables)\n",
    "\n",
    "        saved_files = {}\n",
    "        if all_text_chunks or all_tables:\n",
    "            saved_files = self._save_extraction_results(all_text_chunks, all_tables, output_path)\n",
    "\n",
    "        if all_text_chunks:\n",
    "            avg_length = sum(len(chunk['content']) for chunk in all_text_chunks) / len(all_text_chunks)\n",
    "            self.processing_stats['average_chunk_length'] = round(avg_length, 2)\n",
    "\n",
    "        self.processing_stats['processing_time'] = time.time() - start_time\n",
    "        self._print_processing_summary()\n",
    "\n",
    "        return {\n",
    "            'text_chunks': all_text_chunks,\n",
    "            'tables': all_tables,\n",
    "            'output_dir': output_path,\n",
    "            'saved_files': saved_files,\n",
    "            'stats': self.processing_stats\n",
    "        }\n",
    "\n",
    "    def extract_clean_text(self, content):\n",
    "        if not content or not isinstance(content, str):\n",
    "            return \"\"\n",
    "\n",
    "        lines = content.split('\\n')\n",
    "        clean_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped.startswith('|') and '|' not in line:\n",
    "                if line_stripped or len(clean_lines) == 0 or clean_lines[-1].strip():\n",
    "                    clean_lines.append(line)\n",
    "\n",
    "        return '\\n'.join(clean_lines)\n",
    "\n",
    "    def load_table_files(self, table_dir):\n",
    "        tables = []\n",
    "        table_path = Path(table_dir)\n",
    "\n",
    "        print(f\"ğŸ“Š HTML í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘: {table_dir}\")\n",
    "\n",
    "        for file_path in table_path.rglob(\"*.md\"):\n",
    "            try:\n",
    "                content = file_path.read_text(encoding='utf-8')\n",
    "\n",
    "                if '<table>' in content:\n",
    "                    print(f\"ğŸ² HTML í…Œì´ë¸” ê°ì§€: {file_path.name}\")\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    html_tables = soup.find_all('table')\n",
    "\n",
    "                    for i, table in enumerate(html_tables):\n",
    "                        markdown_table = self._html_table_to_markdown(table)\n",
    "\n",
    "                        tables.append({\n",
    "                            'source_file': str(file_path),\n",
    "                            'filename': f\"{file_path.stem}-table-{i+1}.md\",\n",
    "                            'content': markdown_table,\n",
    "                            'table_type': 'html_converted',\n",
    "                            'original_html': str(table)\n",
    "                        })\n",
    "\n",
    "                        print(f\"   âœ… HTML í…Œì´ë¸” {i+1} ë³€í™˜ ì™„ë£Œ\")\n",
    "\n",
    "                elif '|' in content and ('---' in content or '|-' in content):\n",
    "                    tables.append({\n",
    "                        'source_file': str(file_path),\n",
    "                        'filename': file_path.name,\n",
    "                        'content': content,\n",
    "                        'table_type': 'markdown_native'\n",
    "                    })\n",
    "                    print(f\"ğŸ“‹ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì²˜ë¦¬: {file_path.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}\")\n",
    "\n",
    "        print(f\"ğŸ“Š ì´ {len(tables)}ê°œ í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ\")\n",
    "        return tables\n",
    "\n",
    "    def _html_table_to_markdown(self, table):\n",
    "        rows = table.find_all('tr')\n",
    "        markdown_lines = []\n",
    "\n",
    "        for i, row in enumerate(rows):\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            cell_texts = [re.sub(r'\\s+', ' ', cell.get_text(strip=True)) for cell in cells]\n",
    "\n",
    "            if cell_texts:\n",
    "                markdown_line = '| ' + ' | '.join(cell_texts) + ' |'\n",
    "                markdown_lines.append(markdown_line)\n",
    "\n",
    "                if i == 0:\n",
    "                    separator = '|' + '|'.join(['---'] * len(cell_texts)) + '|'\n",
    "                    markdown_lines.append(separator)\n",
    "\n",
    "        return '\\n'.join(markdown_lines)\n",
    "\n",
    "    def _intelligent_text_chunking(self, text, source_file, max_chunk_size=1200, overlap=150):\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return []\n",
    "\n",
    "        print(f\"ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: {source_file}\")\n",
    "        sentences = self._extract_sentences_with_kiwi(text)\n",
    "        self.processing_stats['kiwi_sentences_analyzed'] += len(sentences)\n",
    "        print(f\"ğŸ“ {len(sentences)}ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "        chunks = self._create_sentence_based_chunks(sentences, source_file, max_chunk_size, overlap)\n",
    "        print(f\"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\")\n",
    "        return chunks\n",
    "\n",
    "    def _extract_sentences_with_kiwi(self, text):\n",
    "        text = self._preprocess_text(text)\n",
    "        sentences = []\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.strip():\n",
    "                sent_results = self.kiwi.split_into_sents(paragraph.strip())\n",
    "                for sent in sent_results:\n",
    "                    if sent.text.strip() and len(sent.text.strip()) > 5:\n",
    "                        sentences.append({\n",
    "                            'text': sent.text.strip(),\n",
    "                            'start': sent.start,\n",
    "                            'end': sent.end,\n",
    "                            'word_count': len(sent.text.split())\n",
    "                        })\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[\"\"\\\"]', '\"', text)\n",
    "        text = re.sub(r'[\\'\\']', \"'\", text)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _create_sentence_based_chunks(self, sentences, source_file, max_chunk_size, overlap):\n",
    "        if not sentences:\n",
    "            return []\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = {\n",
    "            'sentences': [],\n",
    "            'content': '',\n",
    "            'char_count': 0,\n",
    "            'word_count': 0\n",
    "        }\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_text = sentence['text']\n",
    "            sentence_length = len(sentence_text)\n",
    "\n",
    "            if current_chunk['char_count'] + sentence_length + 1 <= max_chunk_size:\n",
    "                current_chunk['sentences'].append(sentence)\n",
    "                current_chunk['content'] += sentence_text + ' '\n",
    "                current_chunk['char_count'] += sentence_length + 1\n",
    "                current_chunk['word_count'] += sentence['word_count']\n",
    "            else:\n",
    "                if current_chunk['sentences']:\n",
    "                    chunk_data = self._finalize_chunk(current_chunk, source_file, len(chunks))\n",
    "                    chunks.append(chunk_data)\n",
    "\n",
    "                current_chunk = {\n",
    "                    'sentences': [sentence],\n",
    "                    'content': sentence_text + ' ',\n",
    "                    'char_count': sentence_length + 1,\n",
    "                    'word_count': sentence['word_count']\n",
    "                }\n",
    "\n",
    "        if current_chunk['sentences']:\n",
    "            chunk_data = self._finalize_chunk(current_chunk, source_file, len(chunks))\n",
    "            chunks.append(chunk_data)\n",
    "\n",
    "        return [chunk for chunk in chunks if len(chunk['content'].strip()) > 30]\n",
    "\n",
    "    def _finalize_chunk(self, chunk_data, source_file, chunk_index):\n",
    "        content = chunk_data['content'].strip()\n",
    "        return {\n",
    "            'content': content,\n",
    "            'source_file': source_file,\n",
    "            'type': 'text',\n",
    "            'chunk_type': 'sentence_based',\n",
    "            'chunk_index': chunk_index,\n",
    "            'word_count': chunk_data['word_count'],\n",
    "            'char_count': len(content),\n",
    "            'sentence_count': len(chunk_data['sentences']),\n",
    "            'analysis_method': 'kiwi_sentence_splitting'\n",
    "        }\n",
    "    def _save_extraction_results(self, text_chunks, tables, output_path):\n",
    "        \"\"\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì²­í¬ì™€ í…Œì´ë¸”ì„ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        saved_files = {}\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        try:\n",
    "            # í…ìŠ¤íŠ¸ ì²­í¬ ì €ì¥\n",
    "            if text_chunks:\n",
    "                text_output_file = output_path / f\"extracted_text_chunks_{timestamp}.json\"\n",
    "                # ... JSON ì €ì¥ ë¡œì§\n",
    "            \n",
    "            # í…Œì´ë¸” ì €ì¥  \n",
    "            if tables:\n",
    "                table_output_file = output_path / f\"extracted_tables_{timestamp}.json\"\n",
    "                # ... JSON ì €ì¥ ë¡œì§\n",
    "                \n",
    "            # í†µí•© ê²°ê³¼ ì €ì¥\n",
    "            combined_output_file = output_path / f\"extraction_results_{timestamp}.json\"\n",
    "            # ... í†µí•© JSON ì €ì¥ ë¡œì§\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            \n",
    "        return saved_files\n",
    "\n",
    "    def _print_processing_summary(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ“Š Kiwi ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬ í…ìŠ¤íŠ¸/í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: {self.processing_stats['total_files']}ê°œ\")\n",
    "        print(f\"ğŸ“Š ë¡œë“œëœ í…Œì´ë¸”: {self.processing_stats['tables_extracted']}ê°œ\")\n",
    "        print(f\"ğŸ“ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬: {self.processing_stats['text_chunks_created']}ê°œ\")\n",
    "        print(f\"ğŸ§  Kiwi ë¶„ì„ ë¬¸ì¥: {self.processing_stats['kiwi_sentences_analyzed']}ê°œ\")\n",
    "        print(f\"ğŸ“ í‰ê·  ì²­í¬ ê¸¸ì´: {self.processing_stats['average_chunk_length']}ì\")\n",
    "        print(f\"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {self.processing_stats['processing_time']:.2f}ì´ˆ\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ğŸ¯ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì „ìš© ë¬¸ì¥ ë¶„ë¦¬ ì²­í‚¹\")\n",
    "        print(\"âœ… ë†’ì€ ì •í™•ë„ì˜ ë¬¸ì¥ ê²½ê³„ íƒì§€\")\n",
    "        print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– 4ë‹¨ê³„: Gemma-3-1B-IT ëª¨ë¸ ë¡œë”© ë° LLM ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìˆ˜ì •ëœ OptimizedLLMManagerFixed í´ë˜ìŠ¤ ë¡œë”© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ ìˆ˜ì •ëœ OptimizedLLMManager (ë¬¸ì œ í•´ê²° ë²„ì „)\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "class OptimizedLLMManager:\n",
    "    \"\"\"ìˆ˜ì •ëœ LLM ë§¤ë‹ˆì € - í”„ë¡¬í”„íŠ¸ ë° íŒŒì‹± ë¡œì§ ìˆ˜ì •\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google/gemma-3-4b-it\", max_length=4156):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.generation_stats = {\n",
    "            'total_processed': 0,\n",
    "            'total_time': 0,\n",
    "            'avg_time_per_item': 0\n",
    "        }\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ ë¡œì§ ìœ ì§€)\"\"\"\n",
    "        try:\n",
    "            torch._dynamo.config.suppress_errors = True\n",
    "            \n",
    "            print(f\"ğŸ”„ {self.model_name} ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "            \n",
    "            LLMGPUMemoryManager.cleanup()\n",
    "            \n",
    "            print(\"ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(\"ğŸ¤– ëª¨ë¸ ë¡œë”©...\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"eager\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            print(f\"âœ… {self.model_name} ë¡œë”© ì™„ë£Œ!\")\n",
    "            print(f\"ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_content(self, text, source_file=\"\"):\n",
    "        \"\"\"ğŸ”§ ìˆ˜ì •ëœ ì½˜í…ì¸  ë¶„ì„ - í”„ë¡¬í”„íŠ¸ì™€ íŒŒì‹± ë¡œì§ ì¼ì¹˜\"\"\"\n",
    "        try:\n",
    "            if not self.model or not self.tokenizer:\n",
    "                raise ValueError(\"ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ë°ì´í„° ì•ˆì „í•œ ì¶”ì¶œ\n",
    "            if isinstance(text, dict):\n",
    "                actual_text = text.get('content', str(text))\n",
    "                if isinstance(source_file, dict):\n",
    "                    source_file = text.get('source_file', str(source_file))\n",
    "            else:\n",
    "                actual_text = str(text)\n",
    "            \n",
    "            # ğŸ”§ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì™„í™” (1000 â†’ 2000)\n",
    "            text_preview = actual_text[:4000] if len(actual_text) > 4000 else actual_text\n",
    "            \n",
    "            # ğŸ”§ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ í˜•ì‹ìœ¼ë¡œ ì¼ì¹˜)\n",
    "            prompt = f\"\"\"ë‹¤ìŒ í…ìŠ¤íŠ¸/í…Œì´ë¸”ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ê´€ë ¨ ê°€ì„¤ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì¤‘ìš”í•œ í•´ì„ ê°€ì´ë“œë¼ì¸:\n",
    "\n",
    "ë‚ ì§œ í˜•ì‹ í•´ì„:\n",
    "- \"25.2ì›”\" = 2025ë…„ 2ì›”\n",
    "- \"21.12ì›”\" = 2021ë…„ 12ì›”  \n",
    "- \"24.10ì›”\" = 2024ë…„ 10ì›”\n",
    "- ë…„ë„.ì›” í˜•ì‹ì€ í•´ë‹¹ ë…„ë„ì˜ í•´ë‹¹ ì›”ë¡œ í•´ì„\n",
    "\n",
    "í…Œì´ë¸” íƒ€ì… êµ¬ë¶„:\n",
    "- ëª©ì°¨ í…Œì´ë¸”: í˜ì´ì§€ ë²ˆí˜¸ë‚˜ êµ¬ì¡°ì  ì„¹ì…˜/ì±•í„° í¬í•¨\n",
    "- ë°ì´í„° í…Œì´ë¸”: ì‹¤ì œ ìˆ«ì, í†µê³„, ë°±ë¶„ìœ¨ í¬í•¨\n",
    "\n",
    "ëª©ì°¨ í…Œì´ë¸”ì˜ ê²½ìš° \"ì´ ë¬¸ì„œëŠ” Xê°œ ì±•í„°ë¡œ êµ¬ì„±ë¨...\" ìœ¼ë¡œ ìš”ì•½ ì‹œì‘\n",
    "\n",
    "í…ìŠ¤íŠ¸/í…Œì´ë¸”:\n",
    "{text_preview}\n",
    "\n",
    "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:\n",
    "\n",
    "## ì½˜í…ì¸  íƒ€ì…:\n",
    "[ë°ì´í„° í…Œì´ë¸” / ëª©ì°¨ í…Œì´ë¸” / ì¼ë°˜ í…ìŠ¤íŠ¸ ì¤‘ ì„ íƒ]\n",
    "\n",
    "## ìš”ì•½:\n",
    "[ìœ„ ê°€ì´ë“œë¼ì¸ì— ë”°ë¼ ì •í™•í•œ í•´ì„ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½]\n",
    "\n",
    "## Hypothetical Questions:\n",
    "1. [ì§ˆë¬¸ 1]\n",
    "2. [ì§ˆë¬¸ 2] \n",
    "3. [ì§ˆë¬¸ 3]\n",
    "4. [ì§ˆë¬¸ 4]\n",
    "5. [ì§ˆë¬¸ 5]\n",
    "\n",
    "ì¤‘ìš”ì‚¬í•­:\n",
    "- ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±\n",
    "- ê¸ˆìœµ/ë¹„ì¦ˆë‹ˆìŠ¤ ë‚´ìš©ì— ì¤‘ì \n",
    "- ê²€ìƒ‰ ëª©ì ì„ ìœ„í•œ ìƒì„¸í•˜ê³  ìœ ìµí•œ ìš”ì•½ ì‘ì„±\n",
    "\"\"\"\n",
    "            \n",
    "            # í† í°í™”\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # ì¶”ë¡ \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.5,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # ë””ì½”ë”©\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            # ğŸ”§ ìˆ˜ì •ëœ ì‘ë‹µ íŒŒì‹± (í•œêµ­ì–´ í˜•ì‹ì— ë§ì¶¤)\n",
    "            content_type = \"ì¼ë°˜ í…ìŠ¤íŠ¸\"\n",
    "            summary_text = \"\"\n",
    "            questions = []\n",
    "            \n",
    "            try:\n",
    "                # ì½˜í…ì¸  íƒ€ì… ì¶”ì¶œ\n",
    "                if \"## ì½˜í…ì¸  íƒ€ì…:\" in response:\n",
    "                    sections = response.split(\"## ìš”ì•½:\")\n",
    "                    type_section = sections[0].replace(\"## ì½˜í…ì¸  íƒ€ì…:\", \"\").strip()\n",
    "                    content_type = type_section if type_section else \"ì¼ë°˜ í…ìŠ¤íŠ¸\"\n",
    "                    \n",
    "                    if len(sections) > 1:\n",
    "                        remaining = \"## ìš”ì•½:\" + sections[1]\n",
    "                    else:\n",
    "                        remaining = response\n",
    "                else:\n",
    "                    remaining = response\n",
    "                \n",
    "                # ìš”ì•½ê³¼ ì§ˆë¬¸ ì¶”ì¶œ\n",
    "                if \"## ìš”ì•½:\" in remaining and \"## Hypothetical Questions:\" in remaining:\n",
    "                    parts = remaining.split(\"## Hypothetical Questions:\")\n",
    "                    summary_text = parts[0].replace(\"## ìš”ì•½:\", \"\").strip()\n",
    "                    \n",
    "                    # ì§ˆë¬¸ ì¶”ì¶œ (ë” ê²¬ê³ í•œ ì •ê·œì‹)\n",
    "                    questions_section = parts[1] if len(parts) > 1 else \"\"\n",
    "                    import re\n",
    "                    question_matches = re.findall(r'\\d+\\.\\s*(.+?)(?=\\n\\d+\\.|$)', questions_section, re.DOTALL)\n",
    "                    questions = [q.strip() for q in question_matches if q.strip()]\n",
    "                    \n",
    "                    # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œ\n",
    "                    if not questions:\n",
    "                        lines = questions_section.strip().split('\\n')\n",
    "                        for line in lines:\n",
    "                            line = line.strip()\n",
    "                            if re.match(r'\\d+\\.', line):\n",
    "                                question = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "                                if question:\n",
    "                                    questions.append(question)\n",
    "                else:\n",
    "                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ìš”ì•½ìœ¼ë¡œ ì²˜ë¦¬\n",
    "                    summary_text = remaining.replace(\"## ìš”ì•½:\", \"\").strip()\n",
    "                    \n",
    "                # ğŸ”§ ì§ˆë¬¸ì´ ì—¬ì „íˆ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •\n",
    "                if not questions:\n",
    "                    questions = [\"ë¶„ì„ ê²°ê³¼ì—ì„œ ì§ˆë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.\"]\n",
    "                    \n",
    "            except Exception as parse_error:\n",
    "                print(f\"âš ï¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {parse_error}\")\n",
    "                summary_text = response[:500] + \"...\" if len(response) > 500 else response\n",
    "                questions = [f\"íŒŒì‹± ì˜¤ë¥˜ë¡œ ì¸í•œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(parse_error)}\"]\n",
    "            \n",
    "            return {\n",
    "                'source_file': source_file,\n",
    "                'content_type': content_type,\n",
    "                'summary': summary_text,\n",
    "                'hypothetical_questions': questions,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "            return {\n",
    "                'source_file': source_file,\n",
    "                'content_type': \"ì˜¤ë¥˜\",\n",
    "                'summary': f\"ë¶„ì„ ì‹¤íŒ¨: {str(e)}\",\n",
    "                'hypothetical_questions': [f\"ì˜¤ë¥˜ë¡œ ì¸í•œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(e)}\"],\n",
    "                'status': 'error'\n",
    "            }\n",
    "    \n",
    "    def process_batch(self, text_chunks, tables, batch_size=8):\n",
    "        \"\"\"ğŸ”§ ìˆ˜ì •ëœ ë°°ì¹˜ ì²˜ë¦¬ - íƒ€ì… í‚¤ ì¼ì¹˜\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸš€ ìˆ˜ì •ëœ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘!\")\n",
    "        print(f\"ğŸ“¦ í…ìŠ¤íŠ¸ ì²­í¬: {len(text_chunks)}ê°œ\")\n",
    "        print(f\"ğŸ“Š í…Œì´ë¸”: {len(tables)}ê°œ\") \n",
    "        print(f\"ğŸ”§ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        all_results = []\n",
    "        \n",
    "        # 1. ëª¨ë“  ì•„ì´í…œì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©\n",
    "        all_items = []\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ê°€\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            all_items.append({\n",
    "                'type': 'text',\n",
    "                'data': chunk,\n",
    "                'index': i,\n",
    "                'source_type': 'text_chunk'\n",
    "            })\n",
    "        \n",
    "        # í…Œì´ë¸” ì¶”ê°€\n",
    "        for i, table in enumerate(tables):\n",
    "            all_items.append({\n",
    "                'type': 'table', \n",
    "                'data': table,\n",
    "                'index': i,\n",
    "                'source_type': 'table'\n",
    "            })\n",
    "        \n",
    "        total_items = len(all_items)\n",
    "        print(f\"ğŸ“Š ì´ ì²˜ë¦¬í•  í•­ëª©: {total_items}ê°œ\")\n",
    "        \n",
    "        # 2. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "        processed_count = 0\n",
    "        \n",
    "        for batch_start in range(0, total_items, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, total_items)\n",
    "            current_batch = all_items[batch_start:batch_end]\n",
    "            \n",
    "            batch_num = batch_start//batch_size + 1\n",
    "            total_batches = (total_items-1)//batch_size + 1\n",
    "            \n",
    "            print(f\"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches}: {len(current_batch)}ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\")\n",
    "            \n",
    "            # ë°°ì¹˜ ë‚´ ê° ì•„ì´í…œ ì²˜ë¦¬\n",
    "            for item in current_batch:\n",
    "                try:\n",
    "                    if processed_count % 10 == 0:\n",
    "                        LLMGPUMemoryManager.cleanup()\n",
    "                    \n",
    "                    # ë°ì´í„° ì¶”ì¶œ\n",
    "                    data = item['data']\n",
    "                    content = data.get('content', '')\n",
    "                    source_file = data.get('source_file', '')\n",
    "                    \n",
    "                    # LLM ë¶„ì„ ì‹¤í–‰\n",
    "                    analysis_result = self.analyze_content(content, source_file)\n",
    "                    \n",
    "                    # ğŸ”§ ìˆ˜ì •ëœ ê²°ê³¼ êµ¬ì¡°í™” - type í‚¤ ì¼ì¹˜\n",
    "                    result = {\n",
    "                        'analysis_id': f\"{item['source_type']}_{item['index']}_{int(time.time())}\",\n",
    "                        'source_file': source_file,\n",
    "                        'type': item['type'],  # â† content_type ëŒ€ì‹  type ì‚¬ìš©\n",
    "                        'text_content': content[:1000],\n",
    "                        'analysis_result': analysis_result.get('summary', ''),\n",
    "                        'hypothetical_questions': analysis_result.get('hypothetical_questions', []),\n",
    "                        'word_count': len(content.split()) if content else 0,\n",
    "                        'char_count': len(content) if content else 0,\n",
    "                        'processed_at': datetime.now().isoformat(),\n",
    "                        'processing_batch': batch_num\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    processed_count += 1\n",
    "                    \n",
    "                    # ì§„í–‰ë¥  í‘œì‹œ\n",
    "                    if processed_count % 5 == 0:\n",
    "                        progress = (processed_count / total_items) * 100\n",
    "                        print(f\"ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({processed_count}/{total_items})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ ì•„ì´í…œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "                    # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ê²°ê³¼ì— í¬í•¨\n",
    "                    all_results.append({\n",
    "                        'analysis_id': f\"error_{item['index']}_{int(time.time())}\",\n",
    "                        'source_file': item['data'].get('source_file', ''),\n",
    "                        'type': item['type'],  # â† content_type ëŒ€ì‹  type ì‚¬ìš©\n",
    "                        'text_content': '',\n",
    "                        'analysis_result': f'ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',\n",
    "                        'hypothetical_questions': [f'ì˜¤ë¥˜ë¡œ ì¸í•œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(e)}'],\n",
    "                        'word_count': 0,\n",
    "                        'char_count': 0,\n",
    "                        'processed_at': datetime.now().isoformat(),\n",
    "                        'processing_batch': batch_num\n",
    "                    })\n",
    "                    processed_count += 1\n",
    "            \n",
    "            # ë°°ì¹˜ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            LLMGPUMemoryManager.cleanup()\n",
    "            print(f\"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ\")\n",
    "        \n",
    "        # 3. í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        total_time = time.time() - start_time\n",
    "        self.generation_stats['total_processed'] = len(all_results)\n",
    "        self.generation_stats['total_time'] = total_time\n",
    "        self.generation_stats['avg_time_per_item'] = total_time / len(all_results) if all_results else 0\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ìˆ˜ì •ëœ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“Š ì²˜ë¦¬ í†µê³„:\")\n",
    "        print(f\"   - ì´ ì²˜ë¦¬ í•­ëª©: {len(all_results)}ê°œ\")\n",
    "        print(f\"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "        print(f\"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {self.generation_stats['avg_time_per_item']:.2f}ì´ˆ/í•­ëª©\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"ë¦¬ì†ŒìŠ¤ ì •ë¦¬\"\"\"\n",
    "        try:\n",
    "            if self.model:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "            if self.tokenizer:\n",
    "                del self.tokenizer  \n",
    "                self.tokenizer = None\n",
    "            \n",
    "            LLMGPUMemoryManager.cleanup()\n",
    "            print(\"ğŸ§¹ ìˆ˜ì •ëœ LLM ë§¤ë‹ˆì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(\"âœ… ìˆ˜ì •ëœ OptimizedLLMManagerFixed í´ë˜ìŠ¤ ë¡œë”© ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ—„ï¸ 5ë‹¨ê³„: íŒŒì¼ ì €ì¥ê´€ë¦¬ì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM íƒ€ì…ë³„ ë¶„ë¦¬ ì €ì¥ ì‹œìŠ¤í…œ ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class OptimizedLLMFileManager:\n",
    "    \"\"\"LLM íŒŒì´í”„ë¼ì¸ìš© íŒŒì¼ ì €ì¥ ê´€ë¦¬ì (íƒ€ì…ë³„ ë¶„ë¦¬ ì €ì¥)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_output_directories(base_output_dir=\"./llm_analysis_output\"):\n",
    "        \"\"\"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (íƒ€ì…ë³„ ë¶„ë¦¬)\"\"\"\n",
    "        base_path = Path(base_output_dir)\n",
    "        directories = {\n",
    "            'base': base_path,\n",
    "            'markdown_text': base_path / \"markdown_text_result\",\n",
    "            'markdown_table': base_path / \"markdown_table_result\", \n",
    "            'json_text': base_path / \"json_text_result\",\n",
    "            'json_table': base_path / \"json_table_result\",\n",
    "            'analysis': base_path / \"analysis_results\",\n",
    "            'extraction': base_path / \"text_extraction_output\"\n",
    "        }\n",
    "        \n",
    "        # ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        for dir_name, dir_path in directories.items():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ“ LLM ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {base_path}\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´: {directories['markdown_text'].name}\")\n",
    "        print(f\"   ğŸ“Š í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´: {directories['markdown_table'].name}\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ JSON: {directories['json_text'].name}\")\n",
    "        print(f\"   ğŸ“Š í…Œì´ë¸” JSON: {directories['json_table'].name}\")\n",
    "        return directories\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_analysis_results_by_type(results, output_dirs, filename_prefix=\"llm_analysis\"):\n",
    "        \"\"\"íƒ€ì…ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥ (ë©”ì¸ í•¨ìˆ˜)\"\"\"\n",
    "        # íƒ€ì…ë³„ë¡œ ê²°ê³¼ ë¶„ë¦¬\n",
    "        text_results = [r for r in results if r['type'] == 'text']\n",
    "        table_results = [r for r in results if r['type'] == 'table']\n",
    "        \n",
    "        saved_files = {}\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥\n",
    "        if text_results:\n",
    "            saved_files['text_markdown'] = OptimizedLLMFileManager.save_text_results_as_markdown(\n",
    "                text_results, output_dirs, filename_prefix\n",
    "            )\n",
    "            saved_files['text_json'] = OptimizedLLMFileManager.save_text_results_as_json(\n",
    "                text_results, output_dirs, filename_prefix\n",
    "            )\n",
    "        \n",
    "        # í…Œì´ë¸” ê²°ê³¼ ì €ì¥\n",
    "        if table_results:\n",
    "            saved_files['table_markdown'] = OptimizedLLMFileManager.save_table_results_as_markdown(\n",
    "                table_results, output_dirs, filename_prefix\n",
    "            )\n",
    "            saved_files['table_json'] = OptimizedLLMFileManager.save_table_results_as_json(\n",
    "                table_results, output_dirs, filename_prefix\n",
    "            )\n",
    "        \n",
    "        return saved_files\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_text_results_as_markdown(text_results, output_dirs, filename_prefix=\"llm_text_analysis\"):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_text_{timestamp}.md\"\n",
    "            output_file = output_dirs['markdown_text'] / filename\n",
    "            \n",
    "            # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  êµ¬ì„±\n",
    "            content_parts = [\n",
    "                \"# ğŸ“ LLM í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼\\n\\n\",\n",
    "                f\"**ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                f\"**ì´ í…ìŠ¤íŠ¸ ë¶„ì„:** {len(text_results)}ê°œ\\n\\n\",\n",
    "                \"---\\n\\n\"\n",
    "            ]\n",
    "            \n",
    "            # ê° í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ì¶”ê°€\n",
    "            for i, result in enumerate(text_results, 1):\n",
    "                content_type = result.get('content_type', 'ì¼ë°˜ í…ìŠ¤íŠ¸')\n",
    "                content_parts.extend([\n",
    "                    f\"## ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„ {i}: {content_type}\\n\\n\",\n",
    "                    f\"**ğŸ“ ì›ë³¸ íŒŒì¼:** {result.get('source_file', 'unknown')}\\n\",\n",
    "                    f\"**ğŸ“‹ ì½˜í…ì¸  íƒ€ì…:** {content_type}\\n\",\n",
    "                    f\"**ğŸ•’ ì²˜ë¦¬ ì‹œê°„:** {result.get('processed_at', 'unknown')}\\n\\n\",\n",
    "                    f\"### ğŸ“„ ì›ë³¸ ë‚´ìš©\\n\",\n",
    "                    f\"```\\n{result['original_content'][:1000]}{'...' if len(result['original_content']) > 500 else ''}\\n```\\n\\n\",\n",
    "                    f\"### ğŸ” ë¶„ì„ ê²°ê³¼\\n\",\n",
    "                    f\"{result['analysis']}\\n\\n\"\n",
    "                ])\n",
    "                \n",
    "                # Hypothetical Questions ì¶”ê°€\n",
    "                questions = result.get('hypothetical_questions', [])\n",
    "                if questions:\n",
    "                    content_parts.append(\"### â“ Hypothetical Questions\\n\")\n",
    "                    for j, question in enumerate(questions, 1):\n",
    "                        content_parts.append(f\"{j}. {question}\\n\")\n",
    "                    content_parts.append(\"\\n\")\n",
    "                \n",
    "                content_parts.append(\"---\\n\\n\")\n",
    "            \n",
    "            # íŒŒì¼ ì €ì¥\n",
    "            output_file.write_text(''.join(content_parts), encoding='utf-8')\n",
    "            \n",
    "            print(f\"âœ… í…ìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´ ì €ì¥: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…ìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_table_results_as_markdown(table_results, output_dirs, filename_prefix=\"llm_table_analysis\"):\n",
    "        \"\"\"í…Œì´ë¸” ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_table_{timestamp}.md\"\n",
    "            output_file = output_dirs['markdown_table'] / filename\n",
    "            \n",
    "            # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  êµ¬ì„±\n",
    "            content_parts = [\n",
    "                \"# ğŸ“Š LLM í…Œì´ë¸” ë¶„ì„ ê²°ê³¼\\n\\n\",\n",
    "                f\"**ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                f\"**ì´ í…Œì´ë¸” ë¶„ì„:** {len(table_results)}ê°œ\\n\\n\",\n",
    "                \"---\\n\\n\"\n",
    "            ]\n",
    "            \n",
    "            # ê° í…Œì´ë¸” ë¶„ì„ ê²°ê³¼ ì¶”ê°€\n",
    "            for i, result in enumerate(table_results, 1):\n",
    "                content_type = result.get('content_type', 'ë°ì´í„° í…Œì´ë¸”')\n",
    "                content_parts.extend([\n",
    "                    f\"## ğŸ“Š í…Œì´ë¸” ë¶„ì„ {i}: {content_type}\\n\\n\",\n",
    "                    f\"**ğŸ“ ì›ë³¸ íŒŒì¼:** {result.get('source_file', 'unknown')}\\n\",\n",
    "                    f\"**ğŸ“‹ ì½˜í…ì¸  íƒ€ì…:** {content_type}\\n\",\n",
    "                    f\"**ğŸ•’ ì²˜ë¦¬ ì‹œê°„:** {result.get('processed_at', 'unknown')}\\n\\n\",\n",
    "                    f\"### ğŸ“„ ì›ë³¸ í…Œì´ë¸”\\n\",\n",
    "                    f\"```\\n{result['original_content'][:1000]}{'...' if len(result['original_content']) > 500 else ''}\\n```\\n\\n\",\n",
    "                    f\"### ğŸ” ë¶„ì„ ê²°ê³¼\\n\",\n",
    "                    f\"{result['analysis']}\\n\\n\"\n",
    "                ])\n",
    "                \n",
    "                # Hypothetical Questions ì¶”ê°€\n",
    "                questions = result.get('hypothetical_questions', [])\n",
    "                if questions:\n",
    "                    content_parts.append(\"### â“ Hypothetical Questions\\n\")\n",
    "                    for j, question in enumerate(questions, 1):\n",
    "                        content_parts.append(f\"{j}. {question}\\n\")\n",
    "                    content_parts.append(\"\\n\")\n",
    "                \n",
    "                content_parts.append(\"---\\n\\n\")\n",
    "            \n",
    "            # íŒŒì¼ ì €ì¥\n",
    "            output_file.write_text(''.join(content_parts), encoding='utf-8')\n",
    "            \n",
    "            print(f\"âœ… í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ ì €ì¥: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_text_results_as_json(text_results, output_dirs, filename_prefix=\"llm_text_analysis\"):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_text_{timestamp}.json\"\n",
    "            output_file = output_dirs['json_text'] / filename\n",
    "            \n",
    "            # JSON ë°ì´í„° êµ¬ì„±\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"generated_at\": datetime.now().isoformat(),\n",
    "                    \"content_type\": \"text_analysis\",\n",
    "                    \"total_text_analyses\": len(text_results),\n",
    "                    \"format_version\": \"1.0\",\n",
    "                    \"pipeline\": \"LLM_text_analysis\"\n",
    "                },\n",
    "                \"text_analysis_results\": text_results\n",
    "            }\n",
    "            \n",
    "            # JSON íŒŒì¼ ì €ì¥\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"âœ… í…ìŠ¤íŠ¸ JSON ì €ì¥: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…ìŠ¤íŠ¸ JSON ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_table_results_as_json(table_results, output_dirs, filename_prefix=\"llm_table_analysis\"):\n",
    "        \"\"\"í…Œì´ë¸” ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{filename_prefix}_table_{timestamp}.json\"\n",
    "            output_file = output_dirs['json_table'] / filename\n",
    "            \n",
    "            # JSON ë°ì´í„° êµ¬ì„±\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    \"generated_at\": datetime.now().isoformat(),\n",
    "                    \"content_type\": \"table_analysis\", \n",
    "                    \"total_table_analyses\": len(table_results),\n",
    "                    \"format_version\": \"1.0\",\n",
    "                    \"pipeline\": \"LLM_table_analysis\"\n",
    "                },\n",
    "                \"table_analysis_results\": table_results\n",
    "            }\n",
    "            \n",
    "            # JSON íŒŒì¼ ì €ì¥\n",
    "            with output_file.open('w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"âœ… í…Œì´ë¸” JSON ì €ì¥: {output_file.name}\")\n",
    "            return str(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…Œì´ë¸” JSON ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "print(\"âœ… LLM íƒ€ì…ë³„ ë¶„ë¦¬ ì €ì¥ ì‹œìŠ¤í…œ ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì¬ê·€ì  íŒŒì¼ íƒìƒ‰ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_main_pipeline_complete_recursive_fixed(\n",
    "    markdown_files,  # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë°›ìŒ\n",
    "    output_dir=\"./llm_analysis_output\",\n",
    "    model_name=\"google/gemma-3-4b-it\"\n",
    "):\n",
    "    \"\"\"\n",
    "    ğŸš€ ì™„ì „í•œ LLM í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ëª¨ë“  ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸš€ LLM í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ìˆ˜ì • ë²„ì „)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 1. GPU ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "        print(\"\\n1ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ìµœì í™”...\")\n",
    "        LLMGPUMemoryManager.cleanup()  \n",
    "        LLMGPUMemoryManager.print_status()\n",
    "        \n",
    "        # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "        print(\"\\n2ï¸âƒ£ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •...\")\n",
    "        output_dirs = OptimizedLLMFileManager.create_output_directories(output_dir)\n",
    "        \n",
    "        # 3. âœ… ê¸°ì¡´ ë©”ì„œë“œë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë° í…Œì´ë¸” ì¶”ì¶œ\n",
    "        print(\"\\n3ï¸âƒ£ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬...\")\n",
    "        text_processor = OptimizedTextTableProcessor()\n",
    "        all_text_chunks = []\n",
    "        all_tables = []\n",
    "        \n",
    "        for md_file in markdown_files:\n",
    "            print(f\"ğŸ“„ ì²˜ë¦¬ ì¤‘: {md_file}\")\n",
    "            \n",
    "            # íŒŒì¼ ì½ê¸°\n",
    "            try:\n",
    "                with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({md_file}): {e}\")\n",
    "                continue\n",
    "            \n",
    "            # âœ… ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©: íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬ ë¶„ê¸°\n",
    "            file_path = Path(md_file)\n",
    "            \n",
    "            # íŒŒì¼ ê²½ë¡œë¡œ í…ìŠ¤íŠ¸/í…Œì´ë¸” êµ¬ë¶„ (ex_text vs ex_table)\n",
    "            if \"ex_text\" in str(file_path) or \"text\" in file_path.name.lower():\n",
    "                # ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬\n",
    "                print(f\"ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì²˜ë¦¬: {file_path.name}\")\n",
    "                \n",
    "                # ê¸°ì¡´ ë©”ì„œë“œ: í…Œì´ë¸” ë¼ì¸ ì œê±°\n",
    "                clean_text = text_processor.extract_clean_text(content)\n",
    "                \n",
    "                if clean_text.strip():\n",
    "                    # ê¸°ì¡´ ë©”ì„œë“œ: Kiwi ê¸°ë°˜ ì²­í‚¹\n",
    "                    text_chunks = text_processor._intelligent_text_chunking(\n",
    "                        clean_text, str(md_file)\n",
    "                    )\n",
    "                    all_text_chunks.extend(text_chunks)\n",
    "                    \n",
    "            elif \"ex_table\" in str(file_path) or \"table\" in file_path.name.lower():\n",
    "                # ğŸ“Š í…Œì´ë¸” íŒŒì¼ ì²˜ë¦¬\n",
    "                print(f\"ğŸ“Š í…Œì´ë¸” íŒŒì¼ë¡œ ì²˜ë¦¬: {file_path.name}\")\n",
    "                \n",
    "                # ê¸°ì¡´ ë©”ì„œë“œì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ í…Œì´ë¸” ë°ì´í„° ìƒì„±\n",
    "                table_data = {\n",
    "                    'content': content,\n",
    "                    'source_file': str(md_file),\n",
    "                    'type': 'table',\n",
    "                    'filename': file_path.name\n",
    "                }\n",
    "                all_tables.append(table_data)\n",
    "                \n",
    "            else:\n",
    "                # ğŸ” íŒŒì¼ëª…ìœ¼ë¡œ êµ¬ë¶„ì´ ì•ˆ ë˜ëŠ” ê²½ìš°, ë‚´ìš©ìœ¼ë¡œ íŒë‹¨\n",
    "                print(f\"ğŸ” ë‚´ìš© ë¶„ì„í•˜ì—¬ ì²˜ë¦¬: {file_path.name}\")\n",
    "                \n",
    "                # í…Œì´ë¸” ë§ˆì»¤ê°€ ë§ìœ¼ë©´ í…Œì´ë¸”ë¡œ, ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬\n",
    "                table_line_count = len([line for line in content.split('\\n') if '|' in line])\n",
    "                total_lines = len(content.split('\\n'))\n",
    "                \n",
    "                if table_line_count > total_lines * 0.3:  # 30% ì´ìƒì´ í…Œì´ë¸” ë¼ì¸\n",
    "                    # í…Œì´ë¸”ë¡œ ì²˜ë¦¬\n",
    "                    table_data = {\n",
    "                        'content': content,\n",
    "                        'source_file': str(md_file),\n",
    "                        'type': 'table',\n",
    "                        'filename': file_path.name\n",
    "                    }\n",
    "                    all_tables.append(table_data)\n",
    "                else:\n",
    "                    # í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬\n",
    "                    clean_text = text_processor.extract_clean_text(content)\n",
    "                    if clean_text.strip():\n",
    "                        text_chunks = text_processor._intelligent_text_chunking(\n",
    "                            clean_text, str(md_file)\n",
    "                        )\n",
    "                        all_text_chunks.extend(text_chunks)\n",
    "        \n",
    "        print(f\"âœ… í…ìŠ¤íŠ¸ ì²­í¬: {len(all_text_chunks)}ê°œ, í…Œì´ë¸”: {len(all_tables)}ê°œ\")\n",
    "        \n",
    "        if not all_text_chunks and not all_tables:\n",
    "            print(\"âŒ ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return None\n",
    "        \n",
    "        # 4. LLM ëª¨ë¸ ë¡œë”© (ì•ˆì „í•œ ë¡œë”©)\n",
    "        print(\"\\n4ï¸âƒ£ LLM ëª¨ë¸ ë¡œë”©...\")\n",
    "        llm_generator = OptimizedLLMManager(model_name)\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë”© í™•ì¸\n",
    "        print(\"ğŸ”§ ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸...\")\n",
    "        load_success = llm_generator.load_model()\n",
    "        if not load_success:\n",
    "            print(\"âŒ LLM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨\")\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… LLM ëª¨ë¸ ë¡œë”© ì„±ê³µ!\")\n",
    "        \n",
    "        # 5. ë°°ì¹˜ ì²˜ë¦¬\n",
    "        print(\"\\n5ï¸âƒ£ LLM ë¶„ì„ ì‹œì‘...\")\n",
    "        analysis_results = llm_generator.process_batch(all_text_chunks, all_tables)\n",
    "        \n",
    "        if not analysis_results:\n",
    "            print(\"âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"âœ… ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼\")\n",
    "        \n",
    "        # 6. ê²°ê³¼ ì €ì¥ (ìˆ˜ì •ëœ ë©”ì„œë“œëª… ì‚¬ìš©)\n",
    "        print(\"\\n6ï¸âƒ£ ê²°ê³¼ ì €ì¥...\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # ë§ˆí¬ë‹¤ìš´ ì €ì¥ (ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª…ê³¼ í˜¸ì¶œ ë°©ì‹)\n",
    "        try:\n",
    "            md_saved = OptimizedLLMFileManager.save_analysis_results_as_markdown(\n",
    "                analysis_results, output_dirs, f\"llm_recursive_analysis_{timestamp}\"\n",
    "            )\n",
    "            print(f\"âœ… ë§ˆí¬ë‹¤ìš´ ì €ì¥: {md_saved}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            md_saved = None\n",
    "        \n",
    "        # JSON ì €ì¥\n",
    "        try:\n",
    "            json_saved = OptimizedLLMFileManager.save_analysis_results_as_json(\n",
    "                analysis_results, output_dirs, f\"llm_recursive_analysis_{timestamp}\"\n",
    "            )\n",
    "            print(f\"âœ… JSON ì €ì¥: {json_saved}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            json_saved = None\n",
    "        \n",
    "        # 8. ì„±ëŠ¥ í†µê³„\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"\\nğŸ¯ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "        print(f\"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ\")\n",
    "        print(f\"ğŸ“Š ì²˜ë¦¬ëœ í•­ëª©: {len(analysis_results)}ê°œ\")\n",
    "        print(f\"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_dir}\")\n",
    "        \n",
    "        return {\n",
    "            'text_chunks': all_text_chunks,\n",
    "            'tables': all_tables,\n",
    "            'analysis_results': analysis_results,\n",
    "            'saved_files': {'markdown': md_saved, 'json': json_saved}\n",
    "        }\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vlm íŒŒì´í”„ë¼ì¸ ë‹¨ë… ì‹¤í–‰í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "ğŸ“‹ execute_vlm_pipeline_only()\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ“ Path(\"./data/images\")                            âœ… pathlib í‘œì¤€ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "â”‚   â”œâ”€â”€ exists()                                        âœ… pathlib ë©”ì„œë“œ\n",
    "â”‚   â””â”€â”€ rglob('*')                                      âœ… pathlib ë©”ì„œë“œ\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ–¼ï¸ gpu_optimized_main_pipeline()                    âœ… Line 3291ì—ì„œ ì •ì˜ë¨\n",
    "â”‚   â”œâ”€â”€ AdvancedGPUManager()                            âœ… Line 2434ì—ì„œ ì •ì˜ë¨\n",
    "â”‚   â”‚   â”œâ”€â”€ cleanup_memory(intensive=True)              âœ… í´ë˜ìŠ¤ ë‚´ ë©”ì„œë“œ\n",
    "â”‚   â”‚   â””â”€â”€ print_detailed_status()                     âœ… í´ë˜ìŠ¤ ë‚´ ë©”ì„œë“œ\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ MemoryEfficientBatchProcessor()                 âœ… Line 2619ì—ì„œ ì •ì˜ë¨  \n",
    "â”‚   â”‚   â”œâ”€â”€ __init__(initial_batch_size=2)              âœ… í´ë˜ìŠ¤ ë‚´ ë©”ì„œë“œ\n",
    "â”‚   â”‚   â””â”€â”€ process_with_adaptive_batching()            âœ… í´ë˜ìŠ¤ ë‚´ ë©”ì„œë“œ\n",
    "â”‚   â”‚       â”œâ”€â”€ torch.cuda.get_device_properties()      âœ… PyTorch CUDA\n",
    "â”‚   â”‚       â”œâ”€â”€ torch.cuda.memory_allocated()           âœ… PyTorch CUDA\n",
    "â”‚   â”‚       â””â”€â”€ psutil.virtual_memory()                 âœ… psutil ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ OptimizedFileManager.create_output_directories() âœ… Line 2914ì—ì„œ ì •ì˜ë¨\n",
    "â”‚   â”‚   â””â”€â”€ Path.mkdir(parents=True, exist_ok=True)     âœ… pathlib í‘œì¤€ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ load_model()                                    âœ… Line 3230ì—ì„œ ì •ì˜ë¨ (VLMìš©)\n",
    "â”‚   â”‚   â”œâ”€â”€ Qwen2_5_VLForConditionalGeneration.from_pretrained() âœ… transformers\n",
    "â”‚   â”‚   â”œâ”€â”€ AutoProcessor.from_pretrained()             âœ… transformers  \n",
    "â”‚   â”‚   â”œâ”€â”€ torch.bfloat16                              âœ… PyTorch dtype\n",
    "â”‚   â”‚   â”œâ”€â”€ device_map=\"auto\"                           âœ… transformers ì„¤ì •\n",
    "â”‚   â”‚   â””â”€â”€ torch.cuda.empty_cache()                    âœ… PyTorch\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ process_images_with_adaptive_batching()         âœ… ì¶”ì • - ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "â”‚       â”œâ”€â”€ analyze_image()                             âœ… ì¶”ì • - ì´ë¯¸ì§€ ë¶„ì„ í•¨ìˆ˜\n",
    "â”‚       â”‚   â”œâ”€â”€ processor()                             âœ… AutoProcessor ë©”ì„œë“œ\n",
    "â”‚       â”‚   â”œâ”€â”€ model.generate()                        âœ… Transformers ëª¨ë¸ ë©”ì„œë“œ\n",
    "â”‚       â”‚   â”‚   â”œâ”€â”€ max_new_tokens=512                  âœ… ìƒì„± íŒŒë¼ë¯¸í„°\n",
    "â”‚       â”‚   â”‚   â”œâ”€â”€ temperature=0.7                     âœ… ìƒì„± íŒŒë¼ë¯¸í„°\n",
    "â”‚       â”‚   â”‚   â””â”€â”€ do_sample=True                      âœ… ìƒì„± íŒŒë¼ë¯¸í„°\n",
    "â”‚       â”‚   â””â”€â”€ processor.decode()                      âœ… AutoProcessor ë©”ì„œë“œ\n",
    "â”‚       â””â”€â”€ chunk_analysis_results()                    âœ… ì¶”ì • - ê²°ê³¼ ì²­í‚¹ í•¨ìˆ˜\n",
    "â”‚           â”œâ”€â”€ json.dumps()                            âœ… json í‘œì¤€ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "â”‚           â””â”€â”€ Path.write_text()                       âœ… pathlib ë©”ì„œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ VLM íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...\n",
      "ğŸ¬ VLM íŒŒì´í”„ë¼ì¸ ë‹¨ë… ì‹¤í–‰ ì‹œì‘\n",
      "==================================================\n",
      "\n",
      "ğŸ–¼ï¸ VLM íŒŒì´í”„ë¼ì¸: ì´ë¯¸ì§€ ë¶„ì„\n",
      "ğŸ“ ì…ë ¥: ./data/images\n",
      "ğŸ“ ì¶œë ¥: ./analysis_output\n",
      "ğŸ“· ë°œê²¬ëœ ì´ë¯¸ì§€: 46ê°œ\n",
      "============================================================\n",
      "ğŸš€ GPU ìµœì í™” MultiModal RAG ì‹œìŠ¤í…œ\n",
      "============================================================\n",
      "ğŸ” ì´ˆê¸° GPU ìƒíƒœ:\n",
      "\n",
      "============================================================\n",
      "ğŸ–¥ï¸ GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§\n",
      "============================================================\n",
      "ğŸ¯ GPU ì½”ì–´ í™œìš©ë¥ : 0%\n",
      "ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë¥ : 1%\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.3GB / 24.0GB (1.4%)\n",
      "ğŸ†“ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: 24.0GB\n",
      "ğŸ”¥ PyTorch í• ë‹¹ë¨: 0.0GB\n",
      "ğŸ“¦ PyTorch ì˜ˆì•½ë¨: 0.0GB\n",
      "âš¡ GPU í™œìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ - ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê¶Œì¥\n",
      "âœ… ë©”ëª¨ë¦¬ ì—¬ìœ  ì¶©ë¶„\n",
      "============================================================\n",
      "ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: analysis_output\n",
      "ğŸ“· ë°œê²¬ëœ ì´ë¯¸ì§€: 46ê°œ\n",
      "ğŸ”„ Qwen/Qwen2.5-VL-7B-Instruct ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4010204795040c19521329933a876bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ìµœì í™” ì™„ë£Œ (dtype: torch.bfloat16)\n",
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë””ë°”ì´ìŠ¤: GPU (NVIDIA GeForce RTX 3090)\n",
      "\n",
      "============================================================\n",
      "ğŸ–¥ï¸ GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§\n",
      "============================================================\n",
      "ğŸ¯ GPU ì½”ì–´ í™œìš©ë¥ : 0%\n",
      "ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë¥ : 0%\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 16.0GB / 24.0GB (66.8%)\n",
      "ğŸ†“ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: 8.6GB\n",
      "ğŸ”¥ PyTorch í• ë‹¹ë¨: 15.4GB\n",
      "ğŸ“¦ PyTorch ì˜ˆì•½ë¨: 15.4GB\n",
      "âš¡ GPU í™œìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ - ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê¶Œì¥\n",
      "============================================================\n",
      "ğŸ”„ GPU ìµœì í™” ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘...\n",
      "ğŸš€ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: 46ê°œ ì•„ì´í…œ\n",
      "ğŸ”§ ë°°ì¹˜ í¬ê¸° ì¡°ì •: 2 â†’ 8\n",
      "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: 1-8/46 (í¬ê¸°: 8)\n",
      "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: 9-16/46 (í¬ê¸°: 8)\n",
      "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: 17-24/46 (í¬ê¸°: 8)\n",
      "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: 25-32/46 (í¬ê¸°: 8)\n",
      "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: 33-40/46 (í¬ê¸°: 8)\n",
      "ğŸ“Š ì§„í–‰ë¥ : 87.0% (40/46) | ì²˜ë¦¬ìœ¨: 5.1ê°œ/ë¶„\n",
      "\n",
      "============================================================\n",
      "ğŸ–¥ï¸ GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§\n",
      "============================================================\n",
      "ğŸ¯ GPU ì½”ì–´ í™œìš©ë¥ : 65%\n",
      "ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë¥ : 55%\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 16.1GB / 24.0GB (67.1%)\n",
      "ğŸ†“ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: 8.5GB\n",
      "ğŸ”¥ PyTorch í• ë‹¹ë¨: 15.5GB\n",
      "ğŸ“¦ PyTorch ì˜ˆì•½ë¨: 15.5GB\n",
      "============================================================\n",
      "ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: 41-46/46 (í¬ê¸°: 6)\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½\n",
      "==================================================\n",
      "ğŸ“· ì´ ì²˜ë¦¬ ì•„ì´í…œ: 46/46\n",
      "ğŸ“¦ ì„±ê³µí•œ ë°°ì¹˜: 6\n",
      "ğŸ’¥ OOM ë°œìƒ íšŸìˆ˜: 0\n",
      "ğŸ”§ ë°°ì¹˜ í¬ê¸° ì¡°ì •: 1íšŒ\n",
      "ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ íšŸìˆ˜: 6\n",
      "âš¡ ìµœì¢… ë°°ì¹˜ í¬ê¸°: 8\n",
      "â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: 9.0ë¶„\n",
      "ğŸ“ˆ í‰ê·  ì²˜ë¦¬ìœ¨: 5.1ê°œ/ë¶„\n",
      "==================================================\n",
      "\n",
      "âœ… ë¶„ì„ ì™„ë£Œ! ì„±ê³µ: 46/46ê°œ\n",
      "ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\n",
      "âœ… ë§ˆí¬ë‹¤ìš´ ì €ì¥: gpu_optimized_analysis_20250623_064538_20250623_064538.md\n",
      "âœ… JSON ì €ì¥: gpu_optimized_analysis_20250623_064538_20250623_064538.json\n",
      "\n",
      "ğŸ“Š GPU ìµœì í™” ì²˜ë¦¬ ìš”ì•½:\n",
      "   ğŸ“· ì²˜ë¦¬ëœ ì´ë¯¸ì§€: 46ê°œ\n",
      "   â±ï¸ ì´ ì†Œìš” ì‹œê°„: 9.0ë¶„\n",
      "   âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: 5.1ê°œ/ë¶„\n",
      "   ğŸ“ ì¶œë ¥ ê²½ë¡œ: analysis_output\n",
      "\n",
      "ğŸ” ìµœì¢… GPU ìƒíƒœ:\n",
      "\n",
      "============================================================\n",
      "ğŸ–¥ï¸ GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§\n",
      "============================================================\n",
      "ğŸ¯ GPU ì½”ì–´ í™œìš©ë¥ : 20%\n",
      "ğŸ“Š ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë¥ : 15%\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 16.1GB / 24.0GB (67.1%)\n",
      "ğŸ†“ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: 8.5GB\n",
      "ğŸ”¥ PyTorch í• ë‹¹ë¨: 15.5GB\n",
      "ğŸ“¦ PyTorch ì˜ˆì•½ë¨: 15.5GB\n",
      "âš¡ GPU í™œìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ - ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê¶Œì¥\n",
      "============================================================\n",
      "\n",
      "âœ… VLM íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "ğŸ“Š ì²˜ë¦¬ëœ ì´ë¯¸ì§€: 46ê°œ\n",
      "\n",
      "ğŸ“‹ VLM íŒŒì´í”„ë¼ì¸ ê²°ê³¼: VLM íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ–¼ï¸ VLM íŒŒì´í”„ë¼ì¸ ë‹¨ë… ì‹¤í–‰\n",
    "def execute_vlm_pipeline_only():\n",
    "    \"\"\"VLM íŒŒì´í”„ë¼ì¸ë§Œ ë³„ë„ë¡œ ì‹¤í–‰ (ì´ë¯¸ì§€ ë¶„ì„)\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¬ VLM íŒŒì´í”„ë¼ì¸ ë‹¨ë… ì‹¤í–‰ ì‹œì‘\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ ì‚¬ì „ ì²´í¬\n",
    "        image_dir = Path(\"./data/images\")\n",
    "        if not image_dir.exists():\n",
    "            return {\n",
    "                'success': False,\n",
    "                'vlm_result': None,\n",
    "                'message': f'ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {image_dir}'\n",
    "            }\n",
    "        \n",
    "        # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬\n",
    "        image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "        image_files = [\n",
    "            path for path in image_dir.rglob('*')\n",
    "            if path.suffix.lower() in image_extensions and path.is_file()\n",
    "        ]\n",
    "        \n",
    "        if not image_files:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'vlm_result': None,\n",
    "                'message': f'ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŒ: {image_dir} (*.jpg, *.jpeg, *.png)'\n",
    "            }\n",
    "        \n",
    "        # VLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "        print(\"\\nğŸ–¼ï¸ VLM íŒŒì´í”„ë¼ì¸: ì´ë¯¸ì§€ ë¶„ì„\")\n",
    "        print(\"ğŸ“ ì…ë ¥: ./data/images\")\n",
    "        print(\"ğŸ“ ì¶œë ¥: ./analysis_output\")\n",
    "        print(f\"ğŸ“· ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ\")\n",
    "        \n",
    "        vlm_result = gpu_optimized_main_pipeline('./data/images', './analysis_output')\n",
    "        \n",
    "       \n",
    "        if vlm_result:\n",
    "            print(f\"\\nâœ… VLM íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "            print(f\"ğŸ“Š ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'vlm_result': vlm_result,\n",
    "                'total_images_processed': len(image_files),\n",
    "                'message': 'VLM íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ'\n",
    "            }\n",
    "        else:\n",
    "            print(\"âŒ VLM íŒŒì´í”„ë¼ì¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'vlm_result': None,\n",
    "                'message': 'VLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ VLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'success': False,\n",
    "            'vlm_result': None,\n",
    "            'message': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'\n",
    "        }\n",
    "\n",
    "# VLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "print(\"ğŸ–¼ï¸ VLM íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...\")\n",
    "vlm_pipeline_result = execute_vlm_pipeline_only()\n",
    "print(f\"\\nğŸ“‹ VLM íŒŒì´í”„ë¼ì¸ ê²°ê³¼: {vlm_pipeline_result.get('message', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm íŒŒì´í”„ë¼ì¸ ë‹¨ë… ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'ğŸ“„' (U+1F4C4) (3113715157.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mğŸ“„ íŒŒì¼ íƒìƒ‰: ./data/ex_text ë””ë ‰í† ë¦¬ì—ì„œ .md íŒŒì¼ ìŠ¤ìº”\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character 'ğŸ“„' (U+1F4C4)\n"
     ]
    }
   ],
   "source": [
    "ì›Œí¬í”Œë¡œìš° ë‹¨ê³„:\n",
    "ğŸ“„ íŒŒì¼ íƒìƒ‰: ./data/ex_text ë””ë ‰í† ë¦¬ì—ì„œ .md íŒŒì¼ ìŠ¤ìº”\n",
    "ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: íƒ€ì…ë³„ ë¶„ë¦¬ ì €ì¥ì„ ìœ„í•œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "ğŸ¤– ëª¨ë¸ ë¡œë”©: Gemma-3-1B-IT LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "ğŸ“– í…ìŠ¤íŠ¸ ì¶”ì¶œ: ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ë¶„ë¦¬\n",
    "ğŸ”„ LLM ë¶„ì„: ë°°ì¹˜ ì²˜ë¦¬ë¡œ í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ì„\n",
    "ğŸ’¾ ë¶„ë¦¬ ì €ì¥: í…ìŠ¤íŠ¸/í…Œì´ë¸” ê²°ê³¼ë¥¼ íƒ€ì…ë³„ë¡œ ë¶„ë¦¬ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì¢… ìˆ˜ì •ëœ execute_llm_pipelined_final_fixed í•¨ìˆ˜ ë¡œë”© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def execute_llm_pipeline():\n",
    "    \"\"\"ìµœì¢… ìˆ˜ì •ëœ ì„±ëŠ¥ ìµœì í™” LLM íŒŒì´í”„ë¼ì¸ - ëª¨ë“  ë¬¸ì œ í•´ê²° ë²„ì „\"\"\"\n",
    "    \n",
    "    print(\"ğŸ› ï¸ ìµœì¢… ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘! (ëª¨ë“  ë¬¸ì œ í•´ê²°)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. íŒŒì¼ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ ì‚¬ì „ ì²´í¬\n",
    "        markdown_dir = Path(\"./data/ex_text\")\n",
    "        \n",
    "        if not markdown_dir.exists():\n",
    "            return {\n",
    "                'success': False, \n",
    "                'result': None, \n",
    "                'message': f'ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {markdown_dir}'\n",
    "            }\n",
    "        \n",
    "        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬\n",
    "        markdown_files = list(markdown_dir.rglob(\"*.md\"))\n",
    "        if not markdown_files:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'result': None,\n",
    "                'message': f'ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŒ: {markdown_dir}'\n",
    "            }\n",
    "        \n",
    "        print(f\"ğŸ“„ ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬: {markdown_dir}\")\n",
    "        print(f\"ğŸ“ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(markdown_files)}ê°œ\")\n",
    "        \n",
    "        # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        output_dirs = OptimizedLLMFileManager.create_output_directories(\"./llm_analysis_output\")\n",
    "        \n",
    "        print(\"ğŸ” ìƒì„±ëœ ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n",
    "        for key, value in output_dirs.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # 3. âœ… ìˆ˜ì •ëœ LLM ë§¤ë‹ˆì € ì‚¬ìš©\n",
    "        print(\"ğŸ¤– ìˆ˜ì •ëœ LLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        llm_manager = OptimizedLLMManager()  # â† ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "        if not llm_manager.load_model():\n",
    "            return {\n",
    "                'success': False, \n",
    "                'result': None, \n",
    "                'message': 'ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨'\n",
    "            }\n",
    "        \n",
    "        # 4. âœ… ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° ì´ˆê¸°í™”\n",
    "        text_extractor = OptimizedTextTableProcessor()\n",
    "        \n",
    "        # 5. âœ… ê°œì„ ëœ ë””ë ‰í† ë¦¬ ë¶„ë¦¬ ì²˜ë¦¬\n",
    "        print(\"ğŸ“– ê°œì„ ëœ í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ë¦¬ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        \n",
    "        text_dir = Path(\"./data/ex_text\")      # í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬\n",
    "        table_dir = Path(\"./data/ex_table\")    # í…Œì´ë¸” ë””ë ‰í† ë¦¬\n",
    "        \n",
    "        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "        if 'base' in output_dirs:\n",
    "            extraction_output_dir = output_dirs['base']\n",
    "        elif 'markdown_text' in output_dirs:\n",
    "            extraction_output_dir = output_dirs['markdown_text']\n",
    "        else:\n",
    "            extraction_output_dir = list(output_dirs.values())[0]\n",
    "            \n",
    "        extraction_result = text_extractor.process_markdown_file(\n",
    "            text_dir, table_dir, extraction_output_dir\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ í™•ì¸\n",
    "        if not extraction_result:\n",
    "            return {\n",
    "                'success': False, \n",
    "                'result': None, \n",
    "                'message': 'íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨'\n",
    "            }\n",
    "        \n",
    "        all_text_chunks = extraction_result.get('text_chunks', [])\n",
    "        all_tables = extraction_result.get('tables', [])\n",
    "        \n",
    "        print(f\"ğŸ“ ì´ í…ìŠ¤íŠ¸ ì²­í¬: {len(all_text_chunks)}ê°œ\")\n",
    "        print(f\"ğŸ“Š ì´ í…Œì´ë¸”: {len(all_tables)}ê°œ\")\n",
    "        \n",
    "        # 6. âœ… ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì • ë° ìˆ˜ì •ëœ process_batch í˜¸ì¶œ\n",
    "        print(\"ğŸ¤– ìµœì í™”ëœ LLM ë¶„ì„ ì‹œì‘...\")\n",
    "        \n",
    "        total_items = len(all_text_chunks) + len(all_tables)\n",
    "        if total_items == 0:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'result': None,\n",
    "                'message': 'ì²˜ë¦¬í•  í…ìŠ¤íŠ¸/í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤'\n",
    "            }\n",
    "        \n",
    "        if total_items > 100:\n",
    "            batch_size = 8\n",
    "        elif total_items > 50:\n",
    "            batch_size = 4\n",
    "        else:\n",
    "            batch_size = 2\n",
    "            \n",
    "        print(f\"ğŸ“¦ ë™ì  ë°°ì¹˜ í¬ê¸°: {batch_size} (ì´ {total_items}ê°œ í•­ëª©)\")\n",
    "        \n",
    "        # âœ… ìˆ˜ì •ëœ process_batch ë©”ì„œë“œ í˜¸ì¶œ\n",
    "        analysis_results = llm_manager.process_batch(all_text_chunks, all_tables, batch_size)\n",
    "        \n",
    "        # 7. âœ… ìˆ˜ì •ëœ ì €ì¥ ë¡œì§ (íƒ€ì… í‚¤ ì¼ì¹˜ ì²˜ë¦¬)\n",
    "        print(\"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘...\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # ğŸ”§ ë¶„ì„ ê²°ê³¼ ë°ì´í„° êµ¬ì¡° ìˆ˜ì • (original_content, analysis í‚¤ ì¶”ê°€)\n",
    "        print(\"ğŸ”§ ë¶„ì„ ê²°ê³¼ ë°ì´í„° êµ¬ì¡° ìˆ˜ì • ì¤‘...\")\n",
    "        for result in analysis_results:\n",
    "            # original_content í‚¤ ì¶”ê°€ (ë§ˆí¬ë‹¤ìš´ ì €ì¥ì— í•„ìš”)\n",
    "            if 'original_content' not in result:\n",
    "                result['original_content'] = result.get('text_content', '')\n",
    "            \n",
    "            # âœ… type í‚¤ëŠ” ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨ (ìˆ˜ì •ëœ process_batchì—ì„œ)\n",
    "            # ê¸°ì¡´ì˜ ê°•ì œ 'text' ì„¤ì • ì œê±°\n",
    "            \n",
    "            # analysis í‚¤ ì¶”ê°€ (ë§ˆí¬ë‹¤ìš´ ì €ì¥ì— í•„ìš”)\n",
    "            if 'analysis' not in result:\n",
    "                result['analysis'] = result.get('analysis_result', '')\n",
    "        \n",
    "        print(f\"ğŸ”§ {len(analysis_results)}ê°œ ê²°ê³¼ ë°ì´í„° êµ¬ì¡° ìˆ˜ì • ì™„ë£Œ\")\n",
    "        \n",
    "        # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸\n",
    "        text_count = len([r for r in analysis_results if r.get('type') == 'text'])\n",
    "        table_count = len([r for r in analysis_results if r.get('type') == 'table'])\n",
    "        print(f\"ğŸ“Š íƒ€ì…ë³„ ë¶„í¬: í…ìŠ¤íŠ¸ {text_count}ê°œ, í…Œì´ë¸” {table_count}ê°œ\")\n",
    "        \n",
    "        # âœ… íƒ€ì…ë³„ ë¶„ë¦¬ ì €ì¥ ì‚¬ìš©\n",
    "        saved_files = OptimizedLLMFileManager.save_analysis_results_by_type(\n",
    "            analysis_results, output_dirs, f\"llm_analysis_final_{timestamp}\"\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ í†µí•©\n",
    "        md_saved = saved_files.get('text_markdown') or saved_files.get('table_markdown')\n",
    "        json_saved = saved_files.get('text_json') or saved_files.get('table_json')\n",
    "        \n",
    "        print(f\"âœ… ì €ì¥ëœ íŒŒì¼:\")\n",
    "        print(f\"   ğŸ“ ë§ˆí¬ë‹¤ìš´: {md_saved}\")\n",
    "        print(f\"   ğŸ“Š JSON: {json_saved}\")\n",
    "        \n",
    "        # ëª¨ë“  ì €ì¥ëœ íŒŒì¼ í‘œì‹œ\n",
    "        if saved_files:\n",
    "            print(f\"   ğŸ“„ ì „ì²´ ì €ì¥ íŒŒì¼:\")\n",
    "            for file_type, file_path in saved_files.items():\n",
    "                if file_path:\n",
    "                    print(f\"      {file_type}: {file_path}\")\n",
    "        \n",
    "        # âœ… ìµœì¢… ë°˜í™˜ê°’\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'total_files_processed': len(markdown_files),\n",
    "            'total_text_chunks': len(all_text_chunks),\n",
    "            'total_tables': len(all_tables),\n",
    "            'total_analysis_results': len(analysis_results),\n",
    "            'text_results_count': text_count,\n",
    "            'table_results_count': table_count,\n",
    "            'saved_files': {\n",
    "                'markdown': md_saved,\n",
    "                'json': json_saved,\n",
    "                'all_saved_files': saved_files\n",
    "            },\n",
    "            'message': 'ëª¨ë“  ë¬¸ì œ í•´ê²° ì™„ë£Œ!'\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'success': False, \n",
    "            'result': None, \n",
    "            'message': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n",
    "        if 'llm_manager' in locals():\n",
    "            if hasattr(llm_manager, 'cleanup'):\n",
    "                llm_manager.cleanup()\n",
    "            else:\n",
    "                print(\"ğŸ§¹ LLM ë§¤ë‹ˆì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (cleanup ë©”ì„œë“œ ì—†ìŒ)\")\n",
    "\n",
    "print(\"âœ… ìµœì¢… ìˆ˜ì •ëœ execute_llm_pipelined_final_fixed í•¨ìˆ˜ ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤... (ëª¨ë“  ë¬¸ì œ í•´ê²°)\n",
      "============================================================\n",
      "ğŸ› ï¸ ìµœì¢… ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘! (ëª¨ë“  ë¬¸ì œ í•´ê²°)\n",
      "============================================================\n",
      "ğŸ“„ ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬: data/ex_text\n",
      "ğŸ“ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: 9ê°œ\n",
      "ğŸ“ LLM ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: llm_analysis_output\n",
      "   ğŸ“ í…ìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´: markdown_text_result\n",
      "   ğŸ“Š í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´: markdown_table_result\n",
      "   ğŸ“ í…ìŠ¤íŠ¸ JSON: json_text_result\n",
      "   ğŸ“Š í…Œì´ë¸” JSON: json_table_result\n",
      "ğŸ” ìƒì„±ëœ ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\n",
      "   base: llm_analysis_output\n",
      "   markdown_text: llm_analysis_output/markdown_text_result\n",
      "   markdown_table: llm_analysis_output/markdown_table_result\n",
      "   json_text: llm_analysis_output/json_text_result\n",
      "   json_table: llm_analysis_output/json_table_result\n",
      "   analysis: llm_analysis_output/analysis_results\n",
      "   extraction: llm_analysis_output/text_extraction_output\n",
      "ğŸ¤– ìˆ˜ì •ëœ LLM ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ğŸ”„ google/gemma-3-4b-it ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4787dc4642a4c58a19be8b2135545d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fcee45779b40e09bc3d0030dff8f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00cb9044a04194a6200ece5072d10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece70363969344a1920ab11975803897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dda92158d64227b60c5c624420b5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ëª¨ë¸ ë¡œë”©...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e510e7cf42c49a9af4b3d750c2c967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc8a25c6f604b9fb18b3ef583695abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25721583acef4498ad18b7b4bafc87ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd8f8eb6f5a43a287e5c5a566e8aea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7147e615a045bf8a289cd0cc69da43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554ba58d9b924eb39a48a5fc5b6d3938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd934af6134f4ee5915bccfc5f72f0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… google/gemma-3-4b-it ë¡œë”© ì™„ë£Œ!\n",
      "ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
      "ğŸ”„ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...\n",
      "âœ… Kiwi ì´ˆê¸°í™” ì™„ë£Œ\n",
      "ğŸ“– ê°œì„ ëœ í…ìŠ¤íŠ¸/í…Œì´ë¸” ë¶„ë¦¬ ì²˜ë¦¬ ì‹œì‘...\n",
      "ğŸ“– í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\n",
      "ğŸ“ 9ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë°œê²¬\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.md (1/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.md\n",
      "ğŸ“ 29ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 6ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.md (2/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F.md\n",
      "ğŸ“ 25ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 5ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.md (3/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f.md\n",
      "ğŸ“ 24ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 4ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.md (4/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.06.04_[í˜„ì§€ì •ë³´]_25ë…„_6ì›”_ìºë‚˜ë‹¤_ì¤‘ì•™ì€í–‰_ì •ì±…íšŒì˜_ê²°ê³¼_ë°_ì‹œì¥_ë°˜ì‘.md\n",
      "ğŸ“ 22ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 7ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.md (5/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.05.20_[í˜„ì§€ì •ë³´]_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥_ì¡°ì •ì—_ëŒ€í•œ_ì‹œì¥ì°¸ê°€ì_í‰ê°€.md\n",
      "ğŸ“ 46ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 9ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.md (6/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.05.19_(í˜„ì§€ì •ë³´_20250516)_Moodyâ€™sç¤¾,_ë¯¸êµ­_ì‹ ìš©ë“±ê¸‰_í•˜í–¥ì¡°ì •_f.md\n",
      "ğŸ“ 5ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 3ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.md (7/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘.md\n",
      "ğŸ“ 24ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 5ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.md (8/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.05.08_(í˜„ì§€ì •ë³´_250507)_2025_5ì›”_FOMC_ì‹œì¥ë°˜ì‘_f.md\n",
      "ğŸ“ 27ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 6ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.md (9/9)\n",
      "ğŸ§  Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€.md\n",
      "ğŸ“ 19ê°œ ë¬¸ì¥ ë¶„ë¦¬ ì™„ë£Œ\n",
      "âœ… 5ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\n",
      "ğŸ“Š í…Œì´ë¸” íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\n",
      "ğŸ“Š HTML í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘: data/ex_table\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.06.19_(í˜„ì§€ì •ë³´_250618)_2025_6ì›”_FOMC_ì‹œì¥ë°˜ì‘_f-table-1.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F-table-1.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.06.12_[í˜„ì§€ì •ë³´]_ç¾_2025.5ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘_F-table-2.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.06.09_(í˜„ì§€ì •ë³´)_ç¾_2025.5ì›”_ê³ ìš©ì§€í‘œ_ë‚´ìš©_ë°_ë‰´ìš•_ê¸ˆìœµì‹œì¥_ë°˜ì‘_f-table-1.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘-table-1.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.14_[í˜„ì§€ì •ë³´]_ç¾_2025.4ì›”_ì†Œë¹„ìë¬¼ê°€_ë™í–¥_ë°_ê¸ˆìœµì‹œì¥_ë°˜ì‘-table-2.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-1.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-2.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-3.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-4.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-5.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-6.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-7.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-8.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-9.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-10.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ² HTML í…Œì´ë¸” ê°ì§€: ë“±ë¡ì¼_2025.05.07_ìµœê·¼(2025.4ì›”)ì˜_ë¯¸êµ­ê²½ì œ_ìƒí™©ê³¼_í‰ê°€-table-11.md\n",
      "   âœ… HTML í…Œì´ë¸” 1 ë³€í™˜ ì™„ë£Œ\n",
      "ğŸ“Š ì´ 17ê°œ í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Kiwi ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬ í…ìŠ¤íŠ¸/í…Œì´ë¸” ì¶”ì¶œ ì™„ë£Œ\n",
      "======================================================================\n",
      "ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: 9ê°œ\n",
      "ğŸ“Š ë¡œë“œëœ í…Œì´ë¸”: 17ê°œ\n",
      "ğŸ“ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬: 50ê°œ\n",
      "ğŸ§  Kiwi ë¶„ì„ ë¬¸ì¥: 221ê°œ\n",
      "ğŸ“ í‰ê·  ì²­í¬ ê¸¸ì´: 1004.42ì\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 1.30ì´ˆ\n",
      "======================================================================\n",
      "ğŸ¯ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì „ìš© ë¬¸ì¥ ë¶„ë¦¬ ì²­í‚¹\n",
      "âœ… ë†’ì€ ì •í™•ë„ì˜ ë¬¸ì¥ ê²½ê³„ íƒì§€\n",
      "======================================================================\n",
      "ğŸ“ ì´ í…ìŠ¤íŠ¸ ì²­í¬: 50ê°œ\n",
      "ğŸ“Š ì´ í…Œì´ë¸”: 17ê°œ\n",
      "ğŸ¤– ìµœì í™”ëœ LLM ë¶„ì„ ì‹œì‘...\n",
      "ğŸ“¦ ë™ì  ë°°ì¹˜ í¬ê¸°: 4 (ì´ 67ê°œ í•­ëª©)\n",
      "\n",
      "ğŸš€ ìˆ˜ì •ëœ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘!\n",
      "ğŸ“¦ í…ìŠ¤íŠ¸ ì²­í¬: 50ê°œ\n",
      "ğŸ“Š í…Œì´ë¸”: 17ê°œ\n",
      "ğŸ”§ ë°°ì¹˜ í¬ê¸°: 4\n",
      "==================================================\n",
      "ğŸ“Š ì´ ì²˜ë¦¬í•  í•­ëª©: 67ê°œ\n",
      "ğŸ”„ ë°°ì¹˜ 1/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 1 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 2/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 7.5% (5/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 2 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 3/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 14.9% (10/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 3 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 4/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 22.4% (15/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 4 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 5/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 29.9% (20/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 5 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 6/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 6 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 7/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 37.3% (25/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 7 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 8/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 44.8% (30/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 8 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 9/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 52.2% (35/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 9 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 10/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 59.7% (40/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 10 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 11/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 11 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 12/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 67.2% (45/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 12 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 13/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 74.6% (50/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 13 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 14/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 82.1% (55/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 14 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 15/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 89.6% (60/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 15 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 16/17: 4ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 16 ì™„ë£Œ\n",
      "ğŸ”„ ë°°ì¹˜ 17/17: 3ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“ˆ ì§„í–‰ë¥ : 97.0% (65/67)\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "âœ… ë°°ì¹˜ 17 ì™„ë£Œ\n",
      "\n",
      "ğŸ‰ ìˆ˜ì •ëœ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\n",
      "ğŸ“Š ì²˜ë¦¬ í†µê³„:\n",
      "   - ì´ ì²˜ë¦¬ í•­ëª©: 67ê°œ\n",
      "   - ì´ ì†Œìš” ì‹œê°„: 1655.55ì´ˆ\n",
      "   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: 24.71ì´ˆ/í•­ëª©\n",
      "ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹œì‘...\n",
      "ğŸ”§ ë¶„ì„ ê²°ê³¼ ë°ì´í„° êµ¬ì¡° ìˆ˜ì • ì¤‘...\n",
      "ğŸ”§ 67ê°œ ê²°ê³¼ ë°ì´í„° êµ¬ì¡° ìˆ˜ì • ì™„ë£Œ\n",
      "ğŸ“Š íƒ€ì…ë³„ ë¶„í¬: í…ìŠ¤íŠ¸ 50ê°œ, í…Œì´ë¸” 17ê°œ\n",
      "âœ… í…ìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´ ì €ì¥: llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "âœ… í…ìŠ¤íŠ¸ JSON ì €ì¥: llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "âœ… í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ ì €ì¥: llm_analysis_final_20250623_135438_table_20250623_135438.md\n",
      "âœ… í…Œì´ë¸” JSON ì €ì¥: llm_analysis_final_20250623_135438_table_20250623_135438.json\n",
      "âœ… ì €ì¥ëœ íŒŒì¼:\n",
      "   ğŸ“ ë§ˆí¬ë‹¤ìš´: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "   ğŸ“Š JSON: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "   ğŸ“„ ì „ì²´ ì €ì¥ íŒŒì¼:\n",
      "      text_markdown: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "      text_json: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "      table_markdown: llm_analysis_output/markdown_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.md\n",
      "      table_json: llm_analysis_output/json_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.json\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\n",
      "ğŸ§¹ ìˆ˜ì •ëœ LLM ë§¤ë‹ˆì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ\n",
      "\n",
      "ğŸ“‹ ìµœì¢… LLM íŒŒì´í”„ë¼ì¸ ê²°ê³¼: ëª¨ë“  ë¬¸ì œ í•´ê²° ì™„ë£Œ!\n",
      "\n",
      "ğŸ‰ ìµœì¢… ì„±ê³µ!\n",
      "   ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: 9ê°œ\n",
      "   ğŸ“ í…ìŠ¤íŠ¸ ì²­í¬: 50ê°œ\n",
      "   ğŸ“Š í…Œì´ë¸”: 17ê°œ\n",
      "   ğŸ” ë¶„ì„ ê²°ê³¼: 67ê°œ\n",
      "   ğŸ“ í…ìŠ¤íŠ¸ ê²°ê³¼: 50ê°œ\n",
      "   ğŸ“Š í…Œì´ë¸” ê²°ê³¼: 17ê°œ\n",
      "\n",
      "ğŸ’¾ ì €ì¥ëœ íŒŒì¼ë“¤:\n",
      "   ğŸ“„ markdown: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "   ğŸ“„ json: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "\n",
      "ğŸ“‚ ì „ì²´ ì €ì¥ íŒŒì¼ ëª©ë¡:\n",
      "   text_markdown: llm_analysis_output/markdown_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.md\n",
      "   text_json: llm_analysis_output/json_text_result/llm_analysis_final_20250623_135438_text_20250623_135438.json\n",
      "   table_markdown: llm_analysis_output/markdown_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.md\n",
      "   table_json: llm_analysis_output/json_table_result/llm_analysis_final_20250623_135438_table_20250623_135438.json\n",
      "\n",
      "============================================================\n",
      "ğŸ ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ğŸ§ª ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "\n",
    "print(\"ğŸ§ª ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤... (ëª¨ë“  ë¬¸ì œ í•´ê²°)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ìµœì¢… ìˆ˜ì • ë²„ì „ ì‹¤í–‰\n",
    "llm_pipeline_result_final = execute_llm_pipeline()\n",
    "\n",
    "print(f\"\\nğŸ“‹ ìµœì¢… LLM íŒŒì´í”„ë¼ì¸ ê²°ê³¼: {llm_pipeline_result_final.get('message', 'N/A')}\")\n",
    "\n",
    "if llm_pipeline_result_final.get('success'):\n",
    "    print(f\"\\nğŸ‰ ìµœì¢… ì„±ê³µ!\")\n",
    "    print(f\"   ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: {llm_pipeline_result_final.get('total_files_processed')}ê°œ\")\n",
    "    print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ì²­í¬: {llm_pipeline_result_final.get('total_text_chunks')}ê°œ\") \n",
    "    print(f\"   ğŸ“Š í…Œì´ë¸”: {llm_pipeline_result_final.get('total_tables')}ê°œ\")\n",
    "    print(f\"   ğŸ” ë¶„ì„ ê²°ê³¼: {llm_pipeline_result_final.get('total_analysis_results')}ê°œ\")\n",
    "    print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê²°ê³¼: {llm_pipeline_result_final.get('text_results_count')}ê°œ\")\n",
    "    print(f\"   ğŸ“Š í…Œì´ë¸” ê²°ê³¼: {llm_pipeline_result_final.get('table_results_count')}ê°œ\")\n",
    "    \n",
    "    saved_files = llm_pipeline_result_final.get('saved_files', {})\n",
    "    print(f\"\\nğŸ’¾ ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "    for file_type, file_path in saved_files.items():\n",
    "        if file_path and file_type != 'all_saved_files':\n",
    "            print(f\"   ğŸ“„ {file_type}: {file_path}\")\n",
    "            \n",
    "    # ëª¨ë“  ì €ì¥ íŒŒì¼ ìƒì„¸ í‘œì‹œ\n",
    "    all_saved = saved_files.get('all_saved_files', {})\n",
    "    if all_saved:\n",
    "        print(f\"\\nğŸ“‚ ì „ì²´ ì €ì¥ íŒŒì¼ ëª©ë¡:\")\n",
    "        for key, path in all_saved.items():\n",
    "            if path:\n",
    "                print(f\"   {key}: {path}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ ì‹¤í–‰ ì‹¤íŒ¨: {llm_pipeline_result_final.get('message')}\")\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ ìˆ˜ì •ëœ LLM íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant ë²¡í„° DB & Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ—„ï¸ ë²¡í„° DB: ë¶„ì„ ê²°ê³¼ë¥¼ ì„ë² ë”©í•˜ì—¬ Qdrantì— ì €ì¥\n",
    "ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ë° ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033f427f393c403b8e3b361d9e57131f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "0abe43cbb56347f19e78fb12c32baaca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b356e8149b74f3e8d79d3acd521fe16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cf704292f934c418b9cebad64e2fde7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30e4909e547446baa522d20a45fe12d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3290db6325a54c09a90c12e9ca357dab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39a895b8a115411795e34835edca0c87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9ac128c0ce47448c819b6f131cdb40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f1ea86674444193bacb88fa555911c2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c738cce6df644dd3838daee6bd590502",
      "value": "Connecting..."
     }
    },
    "46daaed28d4d4f8999fbaf7341d6a3b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "478f761b7ee842bba506d83296051b85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ed247f1c8394d74b3c86453e020f032": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_e03b8902afe141b9860d954a5aaf7f1c"
     }
    },
    "5166717845a140f6b3bc96407aab3a2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59444f7d6a0a4412a1fee4afb59e01ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7117954f4c234c6580290d4df479d5cf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b356e8149b74f3e8d79d3acd521fe16",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "5a3483fef64240319e498507056657ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "6465ed01974049ba9e931c823347376d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f1ea86674444193bacb88fa555911c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fc3a6fca6a443a28c9f3964aa851c1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7117954f4c234c6580290d4df479d5cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "785131af0d8d45c482d828adc1a14c96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d8a7301696c41bbaf6a11ec79bb74bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f848f9398764d09b021e97521166ada": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_adb5fd8f883f42e487d3c67c59b191fa"
     }
    },
    "8492c1e195d74c148a8d1deb7d177959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_1cf704292f934c418b9cebad64e2fde7",
      "style": "IPY_MODEL_99ded4beacb349f59951c7878418c36b",
      "value": true
     }
    },
    "84d65025b29542cf976deb3068cbbabd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_e00f902d50a2407a88df4c480e8d8ebd",
      "style": "IPY_MODEL_033f427f393c403b8e3b361d9e57131f",
      "tooltip": ""
     }
    },
    "8b29d68cb0b64741aacb813aa5f8ca05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0abe43cbb56347f19e78fb12c32baaca",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9c3dd7ef6fa449b8ffd0a2194a46423",
      "value": 2
     }
    },
    "8d791323e7ae4dda8d4e92b367362759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_478f761b7ee842bba506d83296051b85",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a73c41ecf01540f0b28f2362a4b57e8a",
      "value": ""
     }
    },
    "8f48f700b1dd4facaba16f74d020a18f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99225575063e4335ad2affd85b8b5d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbee42484444455bbe68b749b77e2800",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b08fe184782f43c7a3e22ae78efacfa5",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "99ded4beacb349f59951c7878418c36b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2f65e3e57aa4fefb77e75b70b9af2a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a73c41ecf01540f0b28f2362a4b57e8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8257d9bd02048f4969bc76b1a37a31e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad2a17a01db944339df8a3386ef0bc63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_785131af0d8d45c482d828adc1a14c96",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d755f6c63ab04465818d7b3020c5799e",
      "value": "â€‡2/2â€‡[00:28&lt;00:00,â€‡14.08s/it]"
     }
    },
    "adb5fd8f883f42e487d3c67c59b191fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "af1232fb89c241f3aef22c7bf0121aa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_f4851b2d697742e1aff69230206ae9e4",
      "style": "IPY_MODEL_d89bc6eb2d8440cf958074dff71e21ed",
      "value": true
     }
    },
    "aff7be0e7b674a76922729fc1c68c5a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_7d8a7301696c41bbaf6a11ec79bb74bf",
      "style": "IPY_MODEL_5a3483fef64240319e498507056657ef",
      "tooltip": ""
     }
    },
    "b08fe184782f43c7a3e22ae78efacfa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b12bd8cbf65e4c749c7b1197ae599a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_6fc3a6fca6a443a28c9f3964aa851c1a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3290db6325a54c09a90c12e9ca357dab",
      "value": ""
     }
    },
    "b18bbc866d0b4239bb0360e71851d04c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6465ed01974049ba9e931c823347376d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c1726258e9b14e2089ebdf13148cbef3",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "c1726258e9b14e2089ebdf13148cbef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c280ad0b86b147f3869196f225ebbb2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39a895b8a115411795e34835edca0c87",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5166717845a140f6b3bc96407aab3a2e",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "c738cce6df644dd3838daee6bd590502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee5fbcc075540579ad05ff632749e2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8257d9bd02048f4969bc76b1a37a31e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_46daaed28d4d4f8999fbaf7341d6a3b5",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "d755f6c63ab04465818d7b3020c5799e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d89bc6eb2d8440cf958074dff71e21ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbee42484444455bbe68b749b77e2800": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddee3f2203ae4fa092ef6bb610cf2f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2f65e3e57aa4fefb77e75b70b9af2a6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_30e4909e547446baa522d20a45fe12d1",
      "value": "Connecting..."
     }
    },
    "e00f902d50a2407a88df4c480e8d8ebd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e03b8902afe141b9860d954a5aaf7f1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "e9a636aa7a2b47d9b8dd5b0b82a49dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c280ad0b86b147f3869196f225ebbb2d",
       "IPY_MODEL_8b29d68cb0b64741aacb813aa5f8ca05",
       "IPY_MODEL_ad2a17a01db944339df8a3386ef0bc63"
      ],
      "layout": "IPY_MODEL_8f48f700b1dd4facaba16f74d020a18f"
     }
    },
    "e9c3dd7ef6fa449b8ffd0a2194a46423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4851b2d697742e1aff69230206ae9e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
