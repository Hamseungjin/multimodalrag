"""
MultiModal RAG ì‹œìŠ¤í…œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
"""
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from loguru import logger

from config import Config
from kiwipiepy import Kiwi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import time

@dataclass
class Document:
    """ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    metadata: Dict[str, Any]
    doc_id: str
    doc_type: str  # "text" or "image"

class EmbeddingModel:
    """ì„ë² ë”© ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = Config.EMBEDDING_MODEL):
        self.model_name = model_name
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            if torch.cuda.is_available():
                self.model = self.model.to('cuda')
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            return embeddings
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise

class RerankerModel:
    """ë¦¬ë­ì»¤ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = Config.RERANKER_MODEL):
        self.model_name = model_name
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """ë¦¬ë­ì»¤ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            if torch.cuda.is_available():
                self.model = self.model.to('cuda')
            logger.info("ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float]]:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„±ì„ ì¬í‰ê°€í•©ë‹ˆë‹¤."""
        try:
            if not documents:
                return []
            
            # SentenceTransformerì˜ similarity ê¸°ëŠ¥ ì‚¬ìš©
            query_embedding = self.model.encode([query], convert_to_tensor=True)
            doc_embeddings = self.model.encode(documents, convert_to_tensor=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sentence_transformers.util import cos_sim
            similarities = cos_sim(query_embedding, doc_embeddings)[0]
            
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            if hasattr(similarities, 'cpu'):
                similarities = similarities.cpu().numpy()
            else:
                similarities = np.array(similarities)
            
            # ì ìˆ˜ì™€ ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ì •ë ¬
            scored_docs = [(i, float(score)) for i, score in enumerate(similarities)]
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            return scored_docs[:top_k]
            
        except Exception as e:
            logger.error(f"ë¦¬ë­í‚¹ ì‹¤íŒ¨: {e}")
            # ì›ë³¸ ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ê¸°ë³¸ ì ìˆ˜ í• ë‹¹
            return [(i, 1.0 - (i * 0.1)) for i in range(min(top_k, len(documents)))]

class AdvancedKeywordExtractor:
    """ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œê¸° - KiwiPiepy + BGE-m3-ko + TF-IDF (ì˜ë¯¸ì  ìœ ì‚¬ë„ ë³´ì • ì¶”ê°€)"""
    
    def __init__(self, embedding_model: EmbeddingModel):
        self.embedding_model = embedding_model
        self.kiwi = Kiwi()
        self.tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 3))
        
        # ê²½ì œ/ê¸ˆìœµ ë„ë©”ì¸ ì‹œë“œ í‚¤ì›Œë“œ (ì˜ë¯¸ì  í™•ì¥ì˜ ê¸°ì¤€ì )
        self.seed_keywords = {
            "ë¬¼ê°€": ["ì†Œë¹„ìë¬¼ê°€", "CPI", "ì¸í”Œë ˆì´ì…˜", "ë¬¼ê°€ìƒìŠ¹ë¥ ", "ë””í”Œë ˆì´ì…˜", "PCE"],
            "ê³ ìš©": ["ê³ ìš©", "ì‹¤ì—…", "ì¼ìë¦¬", "ì·¨ì—…", "ì‹¤ì—…ë¥ ", "ê³ ìš©ë¥ ", "ë¹„ë†ì—…ë¶€ë¬¸", "ì„ê¸ˆ"],
            "ê¸ˆìœµ": ["ê¸ˆë¦¬", "ê¸°ì¤€ê¸ˆë¦¬", "FOMC", "ì—°ì¤€", "í†µí™”ì •ì±…", "ì–‘ì ì™„í™”", "QE", "QT"],
            "ì‹œì¥": ["ê¸ˆìœµì‹œì¥", "ì£¼ì‹", "ì±„ê¶Œ", "ì‹œì¥ë°˜ì‘", "ì¦ì‹œ", "ë‹¬ëŸ¬", "í™˜ìœ¨"],
            "ê²½ì œ": ["ê²½ì œ", "ì„±ì¥", "GDP", "ê²½ê¸°", "ê²½ì œì§€í‘œ", "ë¬´ì—­", "ìˆ˜ì¶œ", "ìˆ˜ì…", "ê²½ê¸°ì¹¨ì²´"],
            "ìˆ˜ì¹˜": ["ìƒìŠ¹", "í•˜ë½", "ì¦ê°€", "ê°ì†Œ", "ê°œì„ ", "ì•…í™”", "ì•ˆì •", "ë‘”í™”", "ê°€ì†í™”"]
        }
        
        # ëª¨ë“  ì‹œë“œ í‚¤ì›Œë“œë“¤ì˜ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°
        self.seed_embeddings = self._compute_seed_embeddings()
    
    def _compute_seed_embeddings(self) -> Dict[str, np.ndarray]:
        """ì‹œë“œ í‚¤ì›Œë“œë“¤ì˜ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•©ë‹ˆë‹¤."""
        seed_embeddings = {}
        for category, keywords in self.seed_keywords.items():
            embeddings = self.embedding_model.encode(keywords)
            seed_embeddings[category] = embeddings
        return seed_embeddings
    
    def extract_keywords(self, query: str, context_texts: List[str] = None) -> Dict[str, Any]:
        """
        ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            {
                "morphological_keywords": List[str],  # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼
                "semantic_keywords": List[str],       # ì˜ë¯¸ì  í™•ì¥ í‚¤ì›Œë“œ
                "tfidf_keywords": List[str],         # TF-IDF ê¸°ë°˜ ì¤‘ìš” ë‹¨ì–´
                "final_keywords": List[str],         # ìµœì¢… í†µí•© í‚¤ì›Œë“œ
                "sentences": List[str],              # ì •í™•í•œ ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼
                "analysis": Dict[str, Any]           # ë¶„ì„ ìƒì„¸ ì •ë³´
            }
        """
        result = {
            "morphological_keywords": [],
            "semantic_keywords": [],
            "tfidf_keywords": [],
            "final_keywords": [],
            "sentences": [],
            "analysis": {}
        }
        
        # 1. í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        morph_keywords = self._extract_morphological_keywords(query)
        result["morphological_keywords"] = morph_keywords
        
        # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥
        semantic_keywords = self._expand_keywords_semantically(query, morph_keywords)
        result["semantic_keywords"] = semantic_keywords
        
        # 3. TF-IDF ê¸°ë°˜ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ (ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
        if context_texts:
            tfidf_keywords = self._extract_tfidf_keywords(query, context_texts)
            result["tfidf_keywords"] = tfidf_keywords
        
        # 4. ë¬¸ì¥ ë¶„ë¦¬ (í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©)
        sentences = self._split_sentences_accurately(query)
        result["sentences"] = sentences
        
        # 5. ìµœì¢… í‚¤ì›Œë“œ í†µí•© ë° ì¤‘ìš”ë„ ê³„ì‚°
        final_keywords = self._integrate_keywords(
            morph_keywords, semantic_keywords, result["tfidf_keywords"]
        )
        result["final_keywords"] = final_keywords
        
        # 6. ë¶„ì„ ì •ë³´ ì¶”ê°€
        result["analysis"] = {
            "total_keywords": len(final_keywords),
            "morphological_count": len(morph_keywords),
            "semantic_count": len(semantic_keywords),
            "tfidf_count": len(result["tfidf_keywords"]),
            "dominant_categories": self._categorize_keywords(final_keywords)
        }
        
        return result
    
    def _extract_morphological_keywords(self, text: str) -> List[str]:
        """í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        try:
            # Kiwi í˜•íƒœì†Œ ë¶„ì„ - ìµœì‹  API ì‚¬ìš©
            # Method 1: morphsì™€ pos ë©”ì„œë“œ ì‚¬ìš© (ì•ˆì „í•œ ë°©ë²•)
            try:
                morphs = self.kiwi.morphs(text)
                pos_tags = self.kiwi.pos(text)
                
                for word, tag in pos_tags:
                    # ëª…ì‚¬(N), í˜•ìš©ì‚¬(V), ì˜ì–´(SL) ì¶”ì¶œ
                    if (tag.startswith('N') or tag.startswith('V') or tag == 'SL') and len(word) >= 2:
                        # ë¶ˆìš©ì–´ ì œê±°
                        if word not in ['ê²ƒ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ë•Œë¬¸', 'ë•Œë¬¸ì—', 'ë”°ë¼', 'ìœ„í•´']:
                            keywords.append(word)
            
            except (AttributeError, TypeError):
                # Method 2: analyze ë©”ì„œë“œ í˜¸í™˜ì„± ì²˜ë¦¬
                try:
                    analyzed = self.kiwi.analyze(text)
                    
                    for sentence in analyzed:
                        # sentenceê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                        if isinstance(sentence, list):
                            tokens = sentence
                        else:
                            # sentenceê°€ ê°ì²´ë¼ë©´ tokens ì†ì„± ì ‘ê·¼
                            tokens = getattr(sentence, 'tokens', sentence)
                        
                        for token in tokens:
                            # tokenì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
                            if isinstance(token, list):
                                # ê° ìš”ì†Œê°€ ì‹¤ì œ í† í°ì¸ì§€ í™•ì¸
                                for sub_token in token:
                                    word = getattr(sub_token, 'form', str(sub_token))
                                    tag = getattr(sub_token, 'tag', 'UNKNOWN')
                                    
                                    if (tag.startswith('N') or tag.startswith('V') or tag == 'SL') and len(word) >= 2:
                                        if word not in ['ê²ƒ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ë•Œë¬¸', 'ë•Œë¬¸ì—', 'ë”°ë¼', 'ìœ„í•´']:
                                            keywords.append(word)
                            else:
                                # ì¼ë°˜ì ì¸ í† í° ê°ì²´ ì²˜ë¦¬
                                word = getattr(token, 'form', str(token))
                                tag = getattr(token, 'tag', 'UNKNOWN')
                                
                                if (tag.startswith('N') or tag.startswith('V') or tag == 'SL') and len(word) >= 2:
                                    if word not in ['ê²ƒ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ë•Œë¬¸', 'ë•Œë¬¸ì—', 'ë”°ë¼', 'ìœ„í•´']:
                                        keywords.append(word)
                
                except Exception as analyze_error:
                    logger.warning(f"Kiwi analyze ë©”ì„œë“œ ì‹¤íŒ¨: {analyze_error}")
                    # Method 3: ê¸°ë³¸ ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ í´ë°±
                    return self._extract_keywords_with_regex(text)
        
        except Exception as e:
            logger.error(f"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ê·œì‹ ë°©ë²•ìœ¼ë¡œ í´ë°±
            return self._extract_keywords_with_regex(text)
        
        # ë³µí•©ì–´ ë° ì—°ì†ëœ ëª…ì‚¬ ì¶”ì¶œ
        compound_keywords = self._extract_compound_words(text)
        keywords.extend(compound_keywords)
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    def _extract_keywords_with_regex(self, text: str) -> List[str]:
        """ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨ ì‹œ í´ë°±)"""
        keywords = []
        
        # í•œê¸€ í‚¤ì›Œë“œ íŒ¨í„´
        korean_patterns = [
            r'[ê°€-í£]{2,}ì§€ìˆ˜',      # ~ì§€ìˆ˜
            r'[ê°€-í£]{2,}ë¬¼ê°€',      # ~ë¬¼ê°€  
            r'[ê°€-í£]{2,}ìƒìŠ¹ë¥ ',    # ~ìƒìŠ¹ë¥ 
            r'[ê°€-í£]{2,}í•˜ë½ë¥ ',    # ~í•˜ë½ë¥ 
            r'[ê°€-í£]{2,}ì •ì±…',      # ~ì •ì±…
            r'[ê°€-í£]{2,}ì‹œì¥',      # ~ì‹œì¥
            r'[ê°€-í£]{2,}ê³ ìš©',      # ~ê³ ìš©
            r'[ê°€-í£]{2,}ìœ¨',        # ~ìœ¨ (ì‹¤ì—…ë¥ , ì„±ì¥ë¥  ë“±)
            r'[ê°€-í£]{3,}',          # 3ê¸€ì ì´ìƒ í•œê¸€
        ]
        
        # ì˜ì–´/ì•½ì–´ íŒ¨í„´
        english_patterns = [
            r'\b[A-Z]{2,}\b',        # CPI, GDP, FOMC ë“±
            r'\b[A-Z][a-z]+\b',      # Fed ë“±
        ]
        
        all_patterns = korean_patterns + english_patterns
        
        for pattern in all_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)
        
        # ë¶ˆìš©ì–´ ì œê±°
        filtered_keywords = []
        stopwords = ['ê²ƒ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ë•Œë¬¸', 'ë•Œë¬¸ì—', 'ë”°ë¼', 'ìœ„í•´', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°']
        
        for keyword in keywords:
            if keyword not in stopwords and len(keyword.strip()) >= 2:
                filtered_keywords.append(keyword.strip())
        
        return list(set(filtered_keywords))  # ì¤‘ë³µ ì œê±°
    
    def _extract_compound_words(self, text: str) -> List[str]:
        """ë³µí•©ì–´ ë° ì—°ì†ëœ ëª…ì‚¬ ì¶”ì¶œ"""
        compound_words = []
        
        # ì •ê·œì‹ìœ¼ë¡œ ë³µí•©ì–´ íŒ¨í„´ ì°¾ê¸°
        patterns = [
            r'[ê°€-í£]+ì§€ìˆ˜',  # ~ì§€ìˆ˜
            r'[ê°€-í£]+ê³„ì¶œì§€ìˆ˜',  # ~ê³„ì¶œì§€ìˆ˜
            r'[ê°€-í£]+ë¬¼ê°€',  # ~ë¬¼ê°€
            r'[ê°€-í£]+ìƒìŠ¹ë¥ ',  # ~ìƒìŠ¹ë¥ 
            r'[ê°€-í£]+ê³ ìš©',  # ~ê³ ìš©
            r'[ê°€-í£]{2,}ìœ¨',  # ~ìœ¨ (ì‹¤ì—…ë¥ , ì„±ì¥ë¥  ë“±)
            r'[A-Z]{2,}',  # ì˜ì–´ ì•½ì–´ (CPI, GDP ë“±)
            r'[ê°€-í£]+ì‹œì¥',  # ~ì‹œì¥
            r'[ê°€-í£]+ì •ì±…',  # ~ì •ì±…
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            compound_words.extend(matches)
        
        return compound_words
    
    def _expand_keywords_semantically(self, query: str, base_keywords: List[str]) -> List[str]:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ í™•ì¥"""
        if not base_keywords:
            return []
        
        expanded_keywords = []
        
        # ì¿¼ë¦¬ ì „ì²´ì˜ ì„ë² ë”©
        query_embedding = self.embedding_model.encode([query])
        
        # ê° ì‹œë“œ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        for category, seed_embeddings in self.seed_embeddings.items():
            similarities = cosine_similarity(query_embedding, seed_embeddings)[0]
            
            # ìœ ì‚¬ë„ê°€ 0.5 ì´ìƒì¸ í‚¤ì›Œë“œë“¤ ì¶”ê°€
            for i, similarity in enumerate(similarities):
                if similarity > 0.5:
                    keyword = self.seed_keywords[category][i]
                    if keyword not in base_keywords:
                        expanded_keywords.append(keyword)
        
        # ê¸°ì¡´ í‚¤ì›Œë“œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í™•ì¥
        if base_keywords:
            keyword_embeddings = self.embedding_model.encode(base_keywords)
            
            for category, seed_embeddings in self.seed_embeddings.items():
                for keyword_emb in keyword_embeddings:
                    similarities = cosine_similarity([keyword_emb], seed_embeddings)[0]
                    
                    for i, similarity in enumerate(similarities):
                        if similarity > 0.6:  # ë” ë†’ì€ ì„ê³„ê°’
                            keyword = self.seed_keywords[category][i]
                            if keyword not in base_keywords and keyword not in expanded_keywords:
                                expanded_keywords.append(keyword)
        
        return expanded_keywords[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    
    def _extract_tfidf_keywords(self, query: str, context_texts: List[str]) -> List[str]:
        """TF-IDF ê¸°ë°˜ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ"""
        if not context_texts:
            return []
        
        try:
            # ëª¨ë“  í…ìŠ¤íŠ¸ (ì¿¼ë¦¬ + ì»¨í…ìŠ¤íŠ¸) ê²°í•©
            all_texts = [query] + context_texts
            
            # TF-IDF ë²¡í„°í™”
            tfidf_matrix = self.tfidf.fit_transform(all_texts)
            feature_names = self.tfidf.get_feature_names_out()
            
            # ì¿¼ë¦¬(ì²« ë²ˆì§¸ ë¬¸ì„œ)ì˜ TF-IDF ì ìˆ˜ 
            query_tfidf = tfidf_matrix[0].toarray()[0]
            
            # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            word_scores = [(feature_names[i], score) for i, score in enumerate(query_tfidf) if score > 0]
            word_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜ (í•œê¸€ í‚¤ì›Œë“œë§Œ)
            tfidf_keywords = []
            for word, score in word_scores[:15]:
                if any(ord('ê°€') <= ord(char) <= ord('í£') for char in word) or word.isupper():
                    tfidf_keywords.append(word)
            
            return tfidf_keywords[:8]  # ìµœëŒ€ 8ê°œ
            
        except Exception as e:
            logger.warning(f"TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _split_sentences_accurately(self, text: str) -> List[str]:
        """ì •í™•í•œ ë¬¸ì¥ ë¶„ë¦¬ (í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©)"""
        sentences = []
        
        # Kiwiì˜ ë¬¸ì¥ ë¶„ë¦¬ ê¸°ëŠ¥ ì‚¬ìš©
        try:
            # Method 1: split_into_sents ë©”ì„œë“œ ì‹œë„
            try:
                split_result = self.kiwi.split_into_sents(text)
                # split_resultì˜ êµ¬ì¡°ì— ë”°ë¼ ì²˜ë¦¬
                if hasattr(split_result, '__iter__'):
                    for sent in split_result:
                        if hasattr(sent, 'text'):
                            sentences.append(sent.text.strip())
                        elif isinstance(sent, str):
                            sentences.append(sent.strip())
                        else:
                            sentences.append(str(sent).strip())
                else:
                    sentences = [str(split_result).strip()]
            
            except (AttributeError, TypeError) as e:
                logger.warning(f"Kiwi split_into_sents ì‹¤íŒ¨: {e}")
                # Method 2: ë‹¤ë¥¸ Kiwi ë©”ì„œë“œ ì‹œë„
                try:
                    # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” ë‹¤ë¥¸ ë©”ì„œë“œëª… ì‚¬ìš©
                    if hasattr(self.kiwi, 'split_sents'):
                        split_result = self.kiwi.split_sents(text)
                        sentences = [sent.strip() for sent in split_result if sent.strip()]
                    else:
                        raise AttributeError("Kiwi ë¬¸ì¥ ë¶„ë¦¬ ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
                except Exception:
                    # Method 3: ì •ê·œì‹ ê¸°ë°˜ í´ë°±
                    sentences = self._split_sentences_with_regex(text)
            
        except Exception as e:
            logger.warning(f"Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            # í´ë°±: ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬
            sentences = self._split_sentences_with_regex(text)
        
        # ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬
        cleaned_sentences = []
        for sent in sentences:
            if sent and len(sent.strip()) > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                cleaned_sentences.append(sent.strip())
        
        return cleaned_sentences if cleaned_sentences else [text]  # ìµœì†Œí•œ ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” ë°˜í™˜
    
    def _split_sentences_with_regex(self, text: str) -> List[str]:
        """ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬ (í´ë°± ë©”ì„œë“œ)"""
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ íŒ¨í„´
        sentence_patterns = [
            r'[.!?]+\s+',           # ì¼ë°˜ì ì¸ ë¬¸ì¥ ì¢…ë£Œ
            r'[.!?]+$',             # ë¬¸ì¥ ë
            r'[.]\s*\n',            # ì¤„ë°”ê¿ˆê³¼ í•¨ê»˜ ëë‚˜ëŠ” ë¬¸ì¥
            r'[ë‹¤ìš”ìŒë‹ˆê¹Œ]\.\s*',   # í•œêµ­ì–´ ì–´ë¯¸ + ë§ˆì¹¨í‘œ
        ]
        
        sentences = [text]  # ì‹œì‘ì€ ì „ì²´ í…ìŠ¤íŠ¸
        
        for pattern in sentence_patterns:
            new_sentences = []
            for sent in sentences:
                split_sents = re.split(pattern, sent)
                new_sentences.extend([s.strip() for s in split_sents if s.strip()])
            sentences = new_sentences
        
        return [sent for sent in sentences if len(sent) > 5]
    
    def _integrate_keywords(self, morph_keywords: List[str], semantic_keywords: List[str], 
                          tfidf_keywords: List[str]) -> List[str]:
        """ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ í†µí•©í•˜ê³  ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬"""
        keyword_scores = {}
        
        # í˜•íƒœì†Œ ë¶„ì„ í‚¤ì›Œë“œ (ê¸°ë³¸ ì ìˆ˜ 1.0)
        for keyword in morph_keywords:
            keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 1.0
        
        # ì˜ë¯¸ì  í™•ì¥ í‚¤ì›Œë“œ (ì ìˆ˜ 0.8)
        for keyword in semantic_keywords:
            keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 0.8
        
        # TF-IDF í‚¤ì›Œë“œ (ì ìˆ˜ 0.6)
        for keyword in tfidf_keywords:
            keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 0.6
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
        
        return [keyword for keyword, score in sorted_keywords if score >= 0.5]
    
    def _categorize_keywords(self, keywords: List[str]) -> Dict[str, int]:
        """í‚¤ì›Œë“œë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        categories = {category: 0 for category in self.seed_keywords.keys()}
        
        for keyword in keywords:
            for category, seed_words in self.seed_keywords.items():
                if keyword in seed_words or any(seed in keyword for seed in seed_words):
                    categories[category] += 1
                    break
        
        return categories

class AnswerGenerator:
    """ë‹µë³€ ìƒì„± LLM í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = Config.LLM_MODEL):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self._load_model()
    
    def _check_tensor_health(self, tensor: torch.Tensor, name: str = "tensor") -> bool:
        """í…ì„œì˜ ìˆ˜ì¹˜ì  ê±´ì „ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            if tensor is None:
                logger.error(f"{name}ì´ Noneì…ë‹ˆë‹¤")
                return False
            
            # NaN í™•ì¸
            if torch.isnan(tensor).any():
                nan_count = torch.isnan(tensor).sum().item()
                logger.error(f"{name}ì— NaN ê°’ì´ {nan_count}ê°œ ìˆìŠµë‹ˆë‹¤")
                return False
            
            # Inf í™•ì¸
            if torch.isinf(tensor).any():
                inf_count = torch.isinf(tensor).sum().item()
                logger.error(f"{name}ì— ë¬´í•œëŒ€ ê°’ì´ {inf_count}ê°œ ìˆìŠµë‹ˆë‹¤")
                return False
            
            # ë§¤ìš° í° ê°’ í™•ì¸ (overflow ìœ„í—˜)
            max_val = torch.abs(tensor).max().item()
            if max_val > 1e6:
                logger.warning(f"{name}ì— ë§¤ìš° í° ê°’ì´ ìˆìŠµë‹ˆë‹¤: {max_val:.2e}")
                return False
            
            # ë§¤ìš° ì‘ì€ ê°’ í™•ì¸ (underflow ìœ„í—˜)
            min_val = torch.abs(tensor).min().item()
            if min_val > 0 and min_val < 1e-10:
                logger.warning(f"{name}ì— ë§¤ìš° ì‘ì€ ê°’ì´ ìˆìŠµë‹ˆë‹¤: {min_val:.2e}")
                # ì‘ì€ ê°’ì€ ê²½ê³ ë§Œ í•˜ê³  í†µê³¼
            
            # í…ì„œ ëª¨ì–‘ í™•ì¸
            if tensor.numel() == 0:
                logger.error(f"{name}ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return False
                
            logger.debug(f"{name} ê±´ì „ì„± í™•ì¸ ì™„ë£Œ - ëª¨ì–‘: {tensor.shape}, ë²”ìœ„: [{tensor.min().item():.6f}, {tensor.max().item():.6f}]")
            return True
            
        except Exception as e:
            logger.error(f"{name} ê±´ì „ì„± í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _safe_gpu_memory_cleanup(self):
        """ì•ˆì „í•œ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if torch.cuda.is_available():
                # ëª¨ë“  GPUì— ëŒ€í•´ ë©”ëª¨ë¦¬ ì •ë¦¬
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
        except Exception as e:
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _monitor_gpu_memory(self, step: str = "unknown"):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        if not torch.cuda.is_available():
            return
        
        try:
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                max_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                
                if allocated > max_memory * 0.95:  # 95% ì´ìƒ ì‚¬ìš© ì‹œ ê²½ê³ 
                    logger.warning(f"[{step}] GPU {i} ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜: {allocated:.1f}GB/{max_memory:.1f}GB")
                else:
                    logger.info(f"[{step}] GPU {i} ë©”ëª¨ë¦¬: {allocated:.1f}GB í• ë‹¹, {cached:.1f}GB ì˜ˆì•½")
                    
        except Exception as e:
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _load_model(self):
        """LLM ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"LLM ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë”© (ì•ˆì „ì„± í–¥ìƒ)
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True  # ë¹ ë¥¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            )
            
            # GPU ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¡œë”©
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                total_memory = 0
                
                # ëª¨ë“  GPU ë©”ëª¨ë¦¬ í™•ì¸
                for i in range(gpu_count):
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    total_memory += gpu_memory
                    logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)} - {gpu_memory:.1f}GB")
                
                logger.info(f"ì´ GPU ê°œìˆ˜: {gpu_count}, ì´ ë©”ëª¨ë¦¬: {total_memory:.1f}GB")
                
                # ê³µí†µ ë¡œë”© ì„¤ì • (Gemma-3 í˜¸í™˜)
                model_kwargs = {
                    "torch_dtype": torch.float16,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True,
                }
                
                # Gemma ëª¨ë¸ì˜ Float16 í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
                if "gemma" in self.model_name.lower() and Config.FORCE_FLOAT32_FOR_GEMMA:
                    logger.warning("ğŸ”§ Gemma ëª¨ë¸ ê°ì§€ - Float16 í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ Float32 ì‚¬ìš©")
                    model_kwargs["torch_dtype"] = torch.float32
                
                if gpu_count >= 2 and total_memory >= 40:  # ë©€í‹° GPU & ì¶©ë¶„í•œ ë©”ëª¨ë¦¬
                    logger.info("ë©€í‹° GPU í™˜ê²½ - ìë™ ë¶„ì‚° ì„¤ì • ì‚¬ìš©")
                    model_kwargs.update({
                        "device_map": "auto",  # ìë™ ë¶„ì‚°ìœ¼ë¡œ ë³€ê²½
                        "max_memory": {0: "22GB", 1: "22GB"}  # ê° GPUë³„ ë©”ëª¨ë¦¬ ì œí•œ
                    })
                    
                    # Float32 ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ì œí•œ ì¡°ì •
                    if model_kwargs["torch_dtype"] == torch.float32:
                        logger.info("Float32 ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì œí•œ ì¡°ì •")
                        model_kwargs["max_memory"] = {0: "18GB", 1: "18GB"}
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        **model_kwargs
                    )
                elif gpu_count >= 1 and total_memory >= 32:  # ë‹¨ì¼ GPU & ì¶©ë¶„í•œ ë©”ëª¨ë¦¬
                    logger.info("ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ - float16 ìµœì  ì„¤ì • ì‚¬ìš©")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float32,
                        device_map="auto",
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        max_memory={0: "40GB"}
                    )
                elif gpu_count >= 1 and total_memory >= 40:  # ì œí•œì  GPU ë©”ëª¨ë¦¬
                    logger.info("ì œí•œì  GPU ë©”ëª¨ë¦¬ - ìµœì í™”ëœ ì„¤ì • ì‚¬ìš©")
                    gpu_memory_per_device = total_memory / gpu_count
                    max_memory_per_gpu = f"{gpu_memory_per_device-4:.0f}GB"
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        max_memory={i: max_memory_per_gpu for i in range(gpu_count)}
                    )
                else:  # ë©”ëª¨ë¦¬ ë¶€ì¡±
                    logger.warning(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (ì´ {total_memory:.1f}GB) - CPU ì‚¬ìš© ê¶Œì¥")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
                        device_map="cpu",
                        trust_remote_code=True,
                        low_cpu_mem_usage=True
                    )
            else:
                # CPU ì „ìš©
                logger.info("GPU ì—†ìŒ - CPU ì‚¬ìš©")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # í† í¬ë‚˜ì´ì € íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì„¤ì • (ì¶”ë¡  ìµœì í™”)
            self.model.eval()
            
            # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ëª¨ë¸ í›„ì²˜ë¦¬
            if hasattr(self.model.config, 'torch_dtype'):
                logger.info(f"ëª¨ë¸ ë°ì´í„° íƒ€ì…: {self.model.config.torch_dtype}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ëª¨ë“  GPU)
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    logger.info(f"GPU {i} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ - í• ë‹¹: {allocated:.1f}GB, ì˜ˆì•½: {cached:.1f}GB")
            
            logger.info("LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"LLM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def generate_answer(self, query: str, context: List[str], sources: List[Dict] = None, 
                       has_relevant_docs: bool = True, max_length: int = 1024) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        import time
        start_time = time.time()
        
        try:
            if not has_relevant_docs or not context:
                # ë¬¸ì„œì™€ ì—°ê´€ ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ìœ ë„ ë‹µë³€
                return self._generate_guidance_answer(query)
            
            # ë” ì§§ì€ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í† í° ì œí•œ ê³ ë ¤)
            context_with_sources = []
            total_length = 0
            max_context_length = 600  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¤„ì„
            
            for i, (ctx, source) in enumerate(zip(context[:2], sources[:2])):  # ìµœëŒ€ 2ê°œ ë¬¸ì„œë§Œ
                file_name = source.get('metadata', {}).get('file_name', f'ë¬¸ì„œ{i+1}')
                # ê° ì»¨í…ìŠ¤íŠ¸ë¥¼ 250ìë¡œ ì œí•œ (ë” ì§§ê²Œ)
                truncated_ctx = ctx[:250] + "..." if len(ctx) > 250 else ctx
                doc_text = f"[{file_name}] {truncated_ctx}"
                
                if total_length + len(doc_text) > max_context_length:
                    break
                
                context_with_sources.append(doc_text)
                total_length += len(doc_text)
            
            context_text = "\n".join(context_with_sources)
            
            # ë”ìš± ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
            prompt = f"ë¬¸ì„œ: {context_text}\nì§ˆë¬¸: {query}\në‹µë³€:"
            
            logger.info(f"LLMì— ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")
            logger.debug(f"í”„ë¡¬í”„íŠ¸ ì‹œì‘ 200ì: '{prompt[:200]}'")
            
            try:
                # ë§¤ìš° ì•ˆì „í•œ í† í°í™” ì„¤ì •
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=200,   # ë”ìš± ì§§ê²Œ ì„¤ì •
                    padding=False,
                    add_special_tokens=True
                )
                
                logger.debug(f"í† í°í™” ê²°ê³¼ - input_ids shape: {inputs['input_ids'].shape}")
                
                # ì…ë ¥ ê¸¸ì´ ê²€ì¦ ë° ì•ˆì „ ì¥ì¹˜
                input_length = inputs['input_ids'].shape[1]
                logger.debug(f"ì‹¤ì œ ì…ë ¥ í† í° ê¸¸ì´: {input_length}")
                
                # ë¹ˆ ì…ë ¥ ê²€ì¦
                if input_length < 5:
                    logger.error("ì…ë ¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
                    return self._create_context_summary(query, context, sources)
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° í…ì„œ ì´ë™
                self._monitor_gpu_memory("ìƒì„± ì „")
                self._safe_gpu_memory_cleanup()
                
                # ì…ë ¥ í…ì„œ ê±´ì „ì„± í™•ì¸
                if not self._check_tensor_health(inputs['input_ids'], "input_ids"):
                    logger.error("ì…ë ¥ í…ì„œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤")
                    return self._create_context_summary(query, context, sources)
                
                # Gemma-3-12b-it ì „ìš©: ë¡œì§“ ì‚¬ì „ ê²€ì‚¬
                if "gemma" in self.model_name.lower():
                    logger.info("ğŸ” Gemma ëª¨ë¸ ê°ì§€ - ë¡œì§“ ì‚¬ì „ ê²€ì‚¬ ì‹¤í–‰")
                    try:
                        with torch.no_grad():
                            # ë¡œì§“ë§Œ ë¨¼ì € í™•ì¸ (generate ì „)
                            test_outputs = self.model(**inputs)
                            test_logits = test_outputs.logits
                            
                            # ë¡œì§“ ìƒíƒœ ìƒì„¸ ë¡œê¹…
                            # Float16 í˜¸í™˜ì„±ì„ ìœ„í•´ ì•ˆì „í•œ ì—°ì‚° ì‚¬ìš©
                            if test_logits.dtype == torch.float16:
                                logger.debug("Float16 ë¡œì§“ ê°ì§€ - float32ë¡œ ë³€í™˜í•˜ì—¬ í†µê³„ ê³„ì‚°")
                                logits_f32 = test_logits.float()
                                logits_min = logits_f32.min().item()
                                logits_max = logits_f32.max().item()
                                logits_mean = logits_f32.mean().item()
                                logits_std = logits_f32.std().item()
                            else:
                                logits_min = test_logits.min().item()
                                logits_max = test_logits.max().item()
                                logits_mean = test_logits.mean().item()
                                logits_std = test_logits.std().item()
                            
                            # NaN ì²´í¬ ì¶”ê°€
                            if any(x != x for x in [logits_min, logits_max, logits_mean, logits_std]):  # NaN ì²´í¬
                                logger.error("ğŸš¨ ë¡œì§“ í†µê³„ì— NaN ê°’ ë°œê²¬!")
                                logger.debug(f"ì›ë³¸ ë¡œì§“ dtype: {test_logits.dtype}, device: {test_logits.device}")
                                logger.debug(f"ë¡œì§“ í…ì„œ ìœ íš¨ì„±: min_valid={torch.isfinite(test_logits).all()}")
                                risk_score += 3
                            else:
                                logger.info(f"ğŸ“Š ë¡œì§“ í†µê³„: ë²”ìœ„[{logits_min:.3f}, {logits_max:.3f}], í‰ê· : {logits_mean:.3f}, í‘œì¤€í¸ì°¨: {logits_std:.3f}")
                            
                            # ìœ„í—˜ ì‹ í˜¸ ê°ì§€
                            risk_score = 0
                            if abs(logits_max) > 50:
                                logger.warning(f"âš ï¸ ë¡œì§“ ìµœëŒ€ê°’ ìœ„í—˜: {logits_max:.3f}")
                                risk_score += 1
                            if abs(logits_min) < -50:
                                logger.warning(f"âš ï¸ ë¡œì§“ ìµœì†Œê°’ ìœ„í—˜: {logits_min:.3f}")
                                risk_score += 1
                            if logits_std > 20:
                                logger.warning(f"âš ï¸ ë¡œì§“ ë¶„ì‚° í¼: {logits_std:.3f}")
                                risk_score += 1
                            
                            # ì•ˆì •í™”ëœ softmax í…ŒìŠ¤íŠ¸
                            try:
                                # Float16 í˜¸í™˜ì„±ì„ ìœ„í•´ float32ë¡œ ìºìŠ¤íŒ…
                                if test_logits.dtype == torch.float16:
                                    logger.debug("Float16 ê°ì§€ - float32ë¡œ ìºìŠ¤íŒ…í•˜ì—¬ softmax í…ŒìŠ¤íŠ¸")
                                    test_logits_f32 = test_logits.float()
                                    stable_logits = test_logits_f32 - test_logits_f32.max(dim=-1, keepdim=True).values
                                    test_probs = torch.softmax(stable_logits, dim=-1)
                                else:
                                    stable_logits = test_logits - test_logits.max(dim=-1, keepdim=True).values
                                    test_probs = torch.softmax(stable_logits, dim=-1)
                                
                                if torch.isnan(test_probs).any():
                                    logger.error("âŒ ì•ˆì •í™”ëœ softmaxì—ì„œ NaN ë°œìƒ!")
                                    risk_score += 3
                                else:
                                    logger.debug("âœ… Softmax ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
                                    
                            except Exception as softmax_error:
                                logger.error(f"âŒ Softmax í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {softmax_error}")
                                # Float16 ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                                if "Half" in str(softmax_error) or "float16" in str(softmax_error).lower():
                                    logger.warning("ğŸ”„ Float16 í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€ - ëª¨ë¸ ì •ë°€ë„ ì¡°ì • í•„ìš”")
                                risk_score += 3
                            
                            # ìœ„í—˜ë„ ê¸°ë°˜ ìƒì„± ì „ëµ ê²°ì •
                            if risk_score >= 3:
                                logger.warning(f"ğŸš¨ ë†’ì€ ìœ„í—˜ë„ ({risk_score}) - ê·¹ë„ë¡œ ë³´ìˆ˜ì  ìƒì„± ëª¨ë“œ")
                                generation_mode = "ultra_safe"
                            elif risk_score >= 1:
                                logger.warning(f"âš ï¸ ì¤‘ê°„ ìœ„í—˜ë„ ({risk_score}) - ì•ˆì „ ìƒì„± ëª¨ë“œ")
                                generation_mode = "safe"
                            else:
                                logger.info("âœ… ë‚®ì€ ìœ„í—˜ë„ - ì¼ë°˜ ìƒì„± ëª¨ë“œ")
                                generation_mode = "normal"
                                
                            logger.info(f"ğŸ¯ ì„ íƒëœ ìƒì„± ëª¨ë“œ: {generation_mode}")
                            
                    except Exception as logits_check_error:
                        logger.error(f"âŒ ë¡œì§“ ì‚¬ì „ ê²€ì‚¬ ì‹¤íŒ¨: {logits_check_error}")
                        generation_mode = "ultra_safe"
                else:
                    generation_mode = "normal"
                
                # í…ì„œ ë””ë°”ì´ìŠ¤ ì´ë™
                if torch.cuda.is_available() and inputs['input_ids'].device.type == 'cpu':
                    try:
                        inputs = {k: v.to('cuda') for k, v in inputs.items()}
                        logger.debug("âœ… í…ì„œë¥¼ GPUë¡œ ì´ë™ ì™„ë£Œ")
                    except Exception as device_error:
                        logger.warning(f"âŒ GPUë¡œ í…ì„œ ì´ë™ ì‹¤íŒ¨: {device_error}, CPU ëª¨ë“œ ì‚¬ìš©")

                with torch.no_grad():
                    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ê°œì„ ëœ ìƒì„±
                    try:
                        # íŒ¨ë”© í† í° ì„¤ì •
                        if self.tokenizer.pad_token is None:
                            self.tokenizer.pad_token = self.tokenizer.eos_token
                        
                        # ìœ„í—˜ë„ë³„ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
                        if generation_mode == "ultra_safe":
                            logger.info("ğŸ”’ ê·¹ë„ë¡œ ë³´ìˆ˜ì ì¸ ìƒì„± ì„¤ì • ì ìš©")
                            gen_kwargs = {
                                "input_ids": inputs['input_ids'],
                                "attention_mask": inputs['attention_mask'],
                                "max_new_tokens": 80,  # 30 â†’ 80ìœ¼ë¡œ ì¦ê°€
                                "do_sample": False,  # Greedy only
                                "pad_token_id": self.tokenizer.pad_token_id,
                                "eos_token_id": self.tokenizer.eos_token_id,
                                "early_stopping": True
                            }
                        elif generation_mode == "safe":
                            logger.info("ğŸ›¡ï¸ ì•ˆì „í•œ ìƒì„± ì„¤ì • ì ìš©")
                            gen_kwargs = {
                                "input_ids": inputs['input_ids'],
                                "attention_mask": inputs['attention_mask'],
                                "max_new_tokens": 100,  # 40 â†’ 100ìœ¼ë¡œ ì¦ê°€
                                "do_sample": True,
                                "temperature": 1.0,  # ì•ˆì •ì ì¸ ì˜¨ë„
                                "top_p": 0.95,
                                "top_k": 50,
                                "pad_token_id": self.tokenizer.pad_token_id,
                                "eos_token_id": self.tokenizer.eos_token_id,
                                "early_stopping": True
                            }
                        else:  # normal
                            logger.info("ğŸŒŸ ì¼ë°˜ ìƒì„± ì„¤ì • ì ìš©")
                            gen_kwargs = {
                                "input_ids": inputs['input_ids'],
                                "attention_mask": inputs['attention_mask'],
                                "max_new_tokens": 120,  # 50 â†’ 120ìœ¼ë¡œ ì¦ê°€
                                "do_sample": True,
                                "temperature": 1.2,
                                "top_p": 0.9,
                                "top_k": 40,
                                "pad_token_id": self.tokenizer.pad_token_id,
                                "eos_token_id": self.tokenizer.eos_token_id,
                                "repetition_penalty": 1.02
                            }
                        
                        # ì²« ë²ˆì§¸ ì‹œë„
                        logger.debug("ì²« ë²ˆì§¸ ìƒì„± ì‹œë„ ì‹œì‘")
                        logger.debug(f"ìƒì„± íŒŒë¼ë¯¸í„°: {gen_kwargs}")
                        outputs = self.model.generate(**gen_kwargs)
                        logger.debug("ì²« ë²ˆì§¸ ìƒì„± ì‹œë„ ì„±ê³µ")
                        
                    except Exception as greedy_error:
                        logger.warning(f"ì²« ë²ˆì§¸ ìƒì„± ì‹œë„ ì‹¤íŒ¨: {greedy_error}")
                        
                        # ì˜¤ë¥˜ ìƒì„¸ ë¶„ì„
                        if "probability tensor" in str(greedy_error):
                            logger.error("ğŸš¨ í™•ë¥  í…ì„œ ìˆ˜ì¹˜ ë¶ˆì•ˆì •ì„± ê°ì§€!")
                            logger.debug("ë¬¸ì œ í•´ê²° ì‹œë„: ë¡œì§“ ì•ˆì •í™” ì ìš©")
                        elif "cuda" in str(greedy_error).lower():
                            logger.error("ğŸ® CUDA ê´€ë ¨ ì˜¤ë¥˜ ê°ì§€")
                        elif "memory" in str(greedy_error).lower():
                            logger.error("ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë ¨ ì˜¤ë¥˜ ê°ì§€")
                        
                        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        self._safe_gpu_memory_cleanup()
                        
                        try:
                            # ë‘ ë²ˆì§¸ ì‹œë„: ê·¹ë„ë¡œ ë‹¨ìˆœí•œ ì„¤ì •
                            logger.debug("ë‘ ë²ˆì§¸ ìƒì„± ì‹œë„ ì‹œì‘ (Greedy only)")
                            fallback_kwargs = {
                                "input_ids": inputs['input_ids'],
                                "max_new_tokens": 60,  # 25 â†’ 60ìœ¼ë¡œ ì¦ê°€
                                "do_sample": False,     # Greedy only
                                "pad_token_id": self.tokenizer.pad_token_id,
                                "eos_token_id": self.tokenizer.eos_token_id
                            }
                            logger.debug(f"ë°±ì—… ìƒì„± íŒŒë¼ë¯¸í„°: {fallback_kwargs}")
                            outputs = self.model.generate(**fallback_kwargs)
                            logger.debug("ë‘ ë²ˆì§¸ ìƒì„± ì‹œë„ ì„±ê³µ")
                            
                        except Exception as second_error:
                            logger.error(f"ë‘ ë²ˆì§¸ ìƒì„± ì‹œë„ë„ ì‹¤íŒ¨: {second_error}")
                            logger.error(f"ëª¨ë¸ ìƒíƒœ - device: {next(self.model.parameters()).device}, dtype: {next(self.model.parameters()).dtype}")
                            
                            # ìµœì¢… ë°±ì—…: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
                            logger.info("ğŸ”„ ëª¨ë“  ìƒì„± ì‹œë„ ì‹¤íŒ¨ - ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´")
                            return self._create_context_summary(query, context, sources)
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë” ì•ˆì „í•œ ë°©ì‹)
                try:
                    input_ids = inputs['input_ids'][0]
                    output_ids = outputs[0]
                    
                    # ì…ë ¥ í† í° ì œê±°í•˜ì—¬ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    generated_ids = output_ids[len(input_ids):]
                    
                    # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
                    logger.debug(f"ì…ë ¥ í† í° ê¸¸ì´: {len(input_ids)}, ì¶œë ¥ í† í° ê¸¸ì´: {len(output_ids)}")
                    logger.debug(f"ìƒì„±ëœ í† í° ê¸¸ì´: {len(generated_ids)}")
                    
                    if len(generated_ids) > 0:
                        logger.debug(f"ìƒì„±ëœ ì²« 5ê°œ í† í°: {generated_ids[:5].tolist()}")
                        
                        # íŒ¨ë”© í† í°ê³¼ 0 í† í° ì œê±°
                        clean_tokens = []
                        for token in generated_ids:
                            token_id = token.item()
                            if token_id not in [0, self.tokenizer.pad_token_id]:
                                clean_tokens.append(token)
                            elif token_id == self.tokenizer.eos_token_id:
                                break  # EOS í† í°ì—ì„œ ì¤‘ì§€
                        
                        if clean_tokens:
                            clean_tensor = torch.stack(clean_tokens)
                            raw_answer = self.tokenizer.decode(clean_tensor, skip_special_tokens=True).strip()
                            logger.debug(f"ì •ë¦¬ëœ í† í° ê°œìˆ˜: {len(clean_tokens)}")
                        else:
                            raw_answer = ""
                            logger.warning("ì •ë¦¬ëœ í† í°ì´ ì—†ìŠµë‹ˆë‹¤")
                    else:
                        raw_answer = ""
                        logger.warning("ìƒì„±ëœ í† í°ì´ ì—†ìŠµë‹ˆë‹¤!")
                    
                    logger.info(f"LLM ì›ë³¸ ë‹µë³€: '{raw_answer}' (ê¸¸ì´: {len(raw_answer)})")
                    
                    # ë‹µë³€ í›„ì²˜ë¦¬
                    answer = self._clean_generated_answer(raw_answer, query) if raw_answer else ""
                    
                    # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    if not answer or len(answer.strip()) < 5:
                        logger.warning(f"ìƒì„±ëœ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ: '{answer}'")
                        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš”ì•½ ë‹µë³€ìœ¼ë¡œ ëŒ€ì²´
                        return self._create_context_summary(query, context, sources)
                    
                    elapsed_time = time.time() - start_time
                    
                    # ë‹µë³€ í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Gemma ëª¨ë¸ ì „ìš©)
                    if "gemma" in self.model_name.lower() and answer:
                        self._log_answer_quality_metrics(raw_answer, answer, generated_ids, elapsed_time)
                    logger.info(f"LLM ë‹µë³€ ìƒì„± ì„±ê³µ (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ), ë‹µë³€ ê¸¸ì´: {len(answer)}")
                    
                    # ì„±ê³µ ë©”íŠ¸ë¦­ ë¡œê¹…
                    self._log_success_metrics(query, answer, elapsed_time, generation_mode if 'generation_mode' in locals() else 'unknown')
                    
                    self._monitor_gpu_memory("ìƒì„± ì™„ë£Œ")
                    
                    return answer
                    
                except Exception as extraction_error:
                    logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {extraction_error}")
                    return self._create_context_summary(query, context, sources)
                
            except Exception as tokenization_error:
                logger.error(f"í† í°í™” ì¤‘ ì˜¤ë¥˜: {tokenization_error}")
                return self._create_context_summary(query, context, sources)
                
        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ): {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _clean_generated_answer(self, answer: str, query: str) -> str:
        """ìƒì„±ëœ ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ê°œì„ í•©ë‹ˆë‹¤."""
        if not answer:
            return ""
        
        # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        patterns_to_remove = [
            "ë‹µë³€:", "ì‘ë‹µ:", "Answer:", "Response:",
            "ì§ˆë¬¸:", "Question:", "Q:", "A:",
            "ì°¸ê³  ë¬¸ì„œ:", "ì°¸ê³ :", "ì¶œì²˜:",
            "[ì¶œì²˜:", "í•œêµ­ì–´ ë‹µë³€:", "ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:",
            "ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€ë“œë¦½ë‹ˆë‹¤:", "ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤:"
        ]
        
        cleaned_answer = answer
        for pattern in patterns_to_remove:
            if pattern in cleaned_answer:
                parts = cleaned_answer.split(pattern, 1)
                if len(parts) > 1 and len(parts[1].strip()) > 20:
                    cleaned_answer = parts[1].strip()
        
        # ë°˜ë³µì ì¸ ë¬¸ì¥ ì œê±°
        sentences = cleaned_answer.split('.')
        unique_sentences = []
        seen_sentences = set()
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:
                # ë¬¸ì¥ ìœ ì‚¬ì„± ê²€ì‚¬ (ê°„ë‹¨í•œ ë²„ì „)
                sentence_key = sentence.lower()[:50]  # ì²« 50ìë¡œ ì¤‘ë³µ ê²€ì‚¬
                if sentence_key not in seen_sentences:
                    unique_sentences.append(sentence)
                    seen_sentences.add(sentence_key)
        
        cleaned_answer = '. '.join(unique_sentences)
        if cleaned_answer and not cleaned_answer.endswith('.'):
            cleaned_answer += '.'
        
        # ìµœì¢… ê²€ì¦
        if len(cleaned_answer.strip()) < 20:
            return answer  # ì›ë³¸ ë°˜í™˜
        
        return cleaned_answer.strip()
    
    def _create_context_summary(self, query: str, context: List[str], sources: List[Dict]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ì—†ì´ ìš”ì•½ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
            query_lower = query.lower()
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ë‹µë³€ ìƒì„±
            if "ì†Œë¹„ìë¬¼ê°€" in query or "cpi" in query_lower or "ì¸í”Œë ˆì´ì…˜" in query_lower:
                # ì†Œë¹„ìë¬¼ê°€ ê´€ë ¨ íŠ¹í™” ë‹µë³€
                answer = self._generate_cpi_summary(context, sources)
            elif "ê³ ìš©" in query or "ì‹¤ì—…" in query or "ì¼ìë¦¬" in query:
                # ê³ ìš© ê´€ë ¨ íŠ¹í™” ë‹µë³€
                answer = self._generate_employment_summary(context, sources)
            else:
                # ì¼ë°˜ì ì¸ ê²½ì œ ë¶„ì„ ë‹µë³€
                answer = self._generate_general_summary(query, context, sources)
            
            return answer
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}' ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì°¸ê³  ë¬¸ì„œë¥¼ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    def _generate_cpi_summary(self, context: List[str], sources: List[Dict]) -> str:
        """ì†Œë¹„ìë¬¼ê°€ ê´€ë ¨ íŠ¹í™” ìš”ì•½ ìƒì„± (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        return self._generate_keyword_based_summary("ì†Œë¹„ìë¬¼ê°€", ["CPI", "ì†Œë¹„ìë¬¼ê°€", "ì¸í”Œë ˆì´ì…˜", "ë¬¼ê°€"], context, sources)
    
    def _generate_employment_summary(self, context: List[str], sources: List[Dict]) -> str:
        """ê³ ìš© ê´€ë ¨ íŠ¹í™” ìš”ì•½ ìƒì„± (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        return self._generate_keyword_based_summary("ë¯¸êµ­ ê³ ìš©ì§€í‘œ", ["ê³ ìš©", "ì‹¤ì—…", "ì¼ìë¦¬", "ì·¨ì—…", "ì„ê¸ˆ", "ì‹¤ì—…ë¥ "], context, sources)
    
    def _generate_general_summary(self, query: str, context: List[str], sources: List[Dict]) -> str:
        """ì¼ë°˜ì ì¸ ê²½ì œ ë¶„ì„ ìš”ì•½ ìƒì„± (ê³ ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        query_keywords = self._extract_query_keywords(query, context)
        
        if query_keywords:
            return self._generate_keyword_based_summary(query, query_keywords, context, sources)
        else:
            # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°± (ê°œì„ ëœ ë²„ì „)
            key_points = []
            for i, (ctx, source) in enumerate(zip(context[:3], sources[:3])):
                file_name = source.get('metadata', {}).get('file_name', f'ë¬¸ì„œ{i+1}')
                
                # ë” ì˜ë¯¸ìˆëŠ” ìš”ì•½ ìƒì„±
                sentences = ctx.split('.')[:3]  # ì²« 3ë¬¸ì¥
                summary = '. '.join(sentence.strip() for sentence in sentences if len(sentence.strip()) > 10)
                if len(summary) > 200:
                    summary = summary[:200] + "..."
                
                key_points.append(f"â€¢ **{file_name}**: {summary}")
            
            return f""""{query}"ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ğŸ“‹ **ì£¼ìš” ë‚´ìš©**:
{chr(10).join(key_points)}

ğŸ’¡ **ì°¸ê³ **: ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ(ì˜ˆ: 'CPI', 'ì‹¤ì—…ë¥ ', 'ê¸ˆë¦¬' ë“±)ë¥¼ í¬í•¨í•œ ì§ˆë¬¸ì„ í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì—ëŠ” í•œêµ­ì€í–‰ì´ ë¶„ì„í•œ ìµœì‹  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê²ƒì…ë‹ˆë‹¤."""
    
    def _extract_query_keywords(self, query: str, context: List[str] = None) -> List[str]:
        """
        ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        if not hasattr(self, 'keyword_extractor'):
            # ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
            embedding_model = EmbeddingModel()
            self.keyword_extractor = AdvancedKeywordExtractor(embedding_model)
        
        # ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ ìˆ˜í–‰
        extraction_result = self.keyword_extractor.extract_keywords(query, context)
        
        # ìƒì„¸ ë¶„ì„ ì •ë³´ ì €ì¥ (ë””ë²„ê¹… ë° ì¶”í›„ ì‚¬ìš©)
        self._last_keyword_analysis = extraction_result
        
        # ë¡œê·¸ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ê³¼ì • ê¸°ë¡
        logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ - ì§ˆë¬¸: {query[:30]}...")
        logger.info(f"  í˜•íƒœì†Œ ë¶„ì„: {extraction_result['morphological_keywords']}")
        logger.info(f"  ì˜ë¯¸ì  í™•ì¥: {extraction_result['semantic_keywords']}")
        logger.info(f"  TF-IDF: {extraction_result['tfidf_keywords']}")
        logger.info(f"  ìµœì¢… í‚¤ì›Œë“œ: {extraction_result['final_keywords']}")
        
        return extraction_result["final_keywords"]
    
    def _generate_keyword_based_summary(self, topic: str, keywords: List[str], context: List[str], sources: List[Dict]) -> str:
        """
        ê³ ê¸‰ í‚¤ì›Œë“œ ì‹œìŠ¤í…œì„ í™œìš©í•œ ì •êµí•œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ë° ë‹µë³€ ìƒì„±
        """
        relevant_passages = []
        
        # í‚¤ì›Œë“œ ë¶„ì„ ì •ë³´ í™œìš© (ìˆëŠ” ê²½ìš°)
        if hasattr(self, '_last_keyword_analysis'):
            analysis = self._last_keyword_analysis
            all_keywords = analysis.get('final_keywords', keywords)
            morphological_keywords = analysis.get('morphological_keywords', [])
            semantic_keywords = analysis.get('semantic_keywords', [])
        else:
            all_keywords = keywords
            morphological_keywords = keywords
            semantic_keywords = []
        
        for ctx in context[:3]:
            # ì •í™•í•œ ë¬¸ì¥ ë¶„ë¦¬ (í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ìˆë‹¤ë©´ í™œìš©)
            if hasattr(self, 'keyword_extractor'):
                sentences = self.keyword_extractor._split_sentences_accurately(ctx)
            else:
                sentences = [s.strip() for s in ctx.split('.') if s.strip()]
            
            keyword_sentences = []
            
            for i, sentence in enumerate(sentences):
                if len(sentence) < 10:
                    continue
                
                # ë‹¤ì¸µì  í‚¤ì›Œë“œ ë§¤ì¹­
                sentence_score = 0
                
                # 1. í˜•íƒœì†Œ ë¶„ì„ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 1.0)
                for keyword in morphological_keywords:
                    if keyword in sentence:
                        sentence_score += 1.0
                
                # 2. ì˜ë¯¸ì  í™•ì¥ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.8)
                for keyword in semantic_keywords:
                    if keyword in sentence:
                        sentence_score += 0.8
                
                # 3. ì „ì²´ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.6)
                for keyword in all_keywords:
                    if keyword in sentence and keyword not in morphological_keywords and keyword not in semantic_keywords:
                        sentence_score += 0.6
                
                # ì„ê³„ê°’ ì´ìƒì¸ ë¬¸ì¥ë§Œ ì„ íƒ
                if sentence_score >= 0.8:
                    # ì•ë’¤ ë¬¸ë§¥ í¬í•¨í•˜ì—¬ ì™„ì „í•œ ì •ë³´ êµ¬ì„±
                    start_idx = max(0, i - 1)
                    end_idx = min(len(sentences), i + 2)
                    
                    context_sentence = " ".join(sentences[start_idx:end_idx])
                    
                    # ì¤‘ë³µ ë°©ì§€ ë° í’ˆì§ˆ ê²€ì¦
                    if (context_sentence not in keyword_sentences and 
                        len(context_sentence) > 20 and
                        any(keyword in context_sentence for keyword in all_keywords)):
                        keyword_sentences.append((context_sentence, sentence_score))
            
            if keyword_sentences:
                # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 2ê°œë§Œ ì„ íƒ
                keyword_sentences.sort(key=lambda x: x[1], reverse=True)
                relevant_passages.extend([passage for passage, score in keyword_sentences[:2]])
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
        unique_passages = []
        for passage in relevant_passages:
            # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
            if (passage not in unique_passages and 
                len(passage) > 30 and
                any(keyword in passage for keyword in all_keywords)):
                unique_passages.append(passage)
        
        if unique_passages:
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±
            passages_text = "\n\n".join([f"ğŸ“Š {passage}" for passage in unique_passages[:3]])
            
            # ê³ ê¸‰ íŠ¸ë Œë“œ ë¶„ì„ (í‚¤ì›Œë“œ ë¶„ì„ ì •ë³´ í™œìš©)
            combined_text = " ".join(unique_passages)
            trend_analysis = self._analyze_trend_from_text_advanced(combined_text, all_keywords)
            
            # í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¶”ê°€
            category_info = self._get_keyword_category_info(all_keywords)
            
            return f"""{topic} ê´€ë ¨ ì£¼ìš” ì •ë³´:

{passages_text}

{trend_analysis}

{category_info}

ìœ„ ì •ë³´ëŠ” í•œêµ­ì€í–‰ì´ ë¶„ì„í•œ ìµœì‹  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê²ƒì…ë‹ˆë‹¤."""
        
        else:
            # í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°ì—ë„ ê°œì„ ëœ ë‹µë³€
            return f"""{topic}ì— ëŒ€í•œ ì •ë³´ê°€ ê²€ìƒ‰ë˜ì—ˆìœ¼ë‚˜, êµ¬ì²´ì ì¸ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤.

ğŸ’¡ **ê²€ìƒ‰ ê°œì„  ì œì•ˆ:**
- ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš© (ì˜ˆ: "ì†Œë¹„ìë¬¼ê°€ìƒìŠ¹ë¥ ", "ê³ ìš©ì§€í‘œ", "ê¸ˆë¦¬ì¸ìƒ" ë“±)
- íŠ¹ì • ì‹œê¸°ë‚˜ ì§€ì—­ ëª…ì‹œ
- ê²½ì œì§€í‘œì˜ ì •í™•í•œ ëª…ì¹­ ì‚¬ìš©

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì—ëŠ” ê´€ë ¨ ë¶„ì„ê³¼ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë‹ˆ, ì•„ë˜ ì°¸ê³  ë¬¸ì„œë¥¼ í™•ì¸í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤."""

    def _analyze_trend_from_text_advanced(self, text: str, keywords: List[str]) -> str:
        """í‚¤ì›Œë“œ ì •ë³´ë¥¼ í™œìš©í•œ ê³ ê¸‰ íŠ¸ë Œë“œ ë¶„ì„"""
        if not text:
            return ""
        
        # í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ì ìš© íŠ¸ë Œë“œ ë¶„ì„
        trend_keywords = {
            "ìƒìŠ¹": ["ìƒìŠ¹", "ì¦ê°€", "í™•ëŒ€", "ê°•í™”", "ê°œì„ ", "í˜¸ì¡°", "ê¸‰ì¦"],
            "í•˜ë½": ["í•˜ë½", "ê°ì†Œ", "ì¶•ì†Œ", "ì•…í™”", "ë¶€ì§„", "í•˜íšŒ", "ê¸‰ê°"],
            "ì•ˆì •": ["ì•ˆì •", "ìœ ì§€", "ë™ê²°", "ë³´í•©", "íš¡ë³´"],
            "ë³€ë™": ["ë³€ë™", "ë“±ë½", "í˜¼ì¡°", "ë³€í™”"]
        }
        
        trend_scores = {}
        
        for trend_type, trend_words in trend_keywords.items():
            score = 0
            for word in trend_words:
                if word in text:
                    # í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥ë„ ê³ ë ¤
                    count = text.count(word)
                    # í•µì‹¬ í‚¤ì›Œë“œì™€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€
                    for keyword in keywords:
                        if keyword in text and abs(text.find(keyword) - text.find(word)) < 50:
                            score += count * 1.5
                        else:
                            score += count
            trend_scores[trend_type] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íŠ¸ë Œë“œ ë°˜í™˜
        if not any(trend_scores.values()):
            return "ğŸ“‹ **ë¶„ì„**: í˜„ì¬ ë°ì´í„°ë¡œëŠ” ëª…í™•í•œ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
        
        dominant_trend = max(trend_scores, key=trend_scores.get)
        
        trend_messages = {
            "ìƒìŠ¹": "ğŸ“ˆ **ë¶„ì„**: ì „ë°˜ì ìœ¼ë¡œ ìƒìŠ¹ì„¸ ë˜ëŠ” ê°œì„  íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.",
            "í•˜ë½": "ğŸ“‰ **ë¶„ì„**: ì „ë°˜ì ìœ¼ë¡œ í•˜ë½ì„¸ ë˜ëŠ” ë‘”í™” íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.",
            "ì•ˆì •": "ğŸ“Š **ë¶„ì„**: ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì¸ íë¦„ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ë³€ë™": "ğŸ”„ **ë¶„ì„**: ë‹¤ì–‘í•œ ë³€ë™ ìš”ì¸ë“¤ì´ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        }
        
        return trend_messages.get(dominant_trend, "ğŸ“‹ **ë¶„ì„**: ë‹¤ì–‘í•œ ìš”ì¸ë“¤ì´ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.")
    
    def _get_keyword_category_info(self, keywords: List[str]) -> str:
        """í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì œê³µ"""
        if hasattr(self, '_last_keyword_analysis'):
            categories = self._last_keyword_analysis.get('analysis', {}).get('dominant_categories', {})
            
            category_names = {
                "ë¬¼ê°€": "ğŸ’° ë¬¼ê°€",
                "ê³ ìš©": "ğŸ‘¥ ê³ ìš©", 
                "ê¸ˆìœµ": "ğŸ¦ ê¸ˆìœµ",
                "ì‹œì¥": "ğŸ“ˆ ì‹œì¥",
                "ê²½ì œ": "ğŸŒ ê²½ì œ",
                "ìˆ˜ì¹˜": "ğŸ“Š ìˆ˜ì¹˜"
            }
            
            active_categories = []
            for category, count in categories.items():
                if count > 0:
                    active_categories.append(f"{category_names.get(category, category)}: {count}ê°œ í‚¤ì›Œë“œ")
            
            if active_categories:
                return f"ğŸ” **í‚¤ì›Œë“œ ë¶„ì•¼**: {', '.join(active_categories)}"
        
        return ""
    
    def _analyze_trend_from_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        if not text:
            return ""
        
        # ê¸ì •ì /ìƒìŠ¹ í‚¤ì›Œë“œ
        positive_keywords = ["ìƒìŠ¹", "ì¦ê°€", "ê°œì„ ", "ê°•í™”", "í™•ëŒ€", "í˜¸ì¡°"]
        # ë¶€ì •ì /í•˜ë½ í‚¤ì›Œë“œ  
        negative_keywords = ["í•˜ë½", "ê°ì†Œ", "ì•…í™”", "ì¶•ì†Œ", "ë¶€ì§„", "í•˜íšŒ"]
        # ì¤‘ì„±/ì•ˆì • í‚¤ì›Œë“œ
        neutral_keywords = ["ì•ˆì •", "ìœ ì§€", "ë™ê²°", "ë³´í•©"]
        
        positive_count = sum(1 for keyword in positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text)
        neutral_count = sum(1 for keyword in neutral_keywords if keyword in text)
        
        if positive_count > negative_count and positive_count > neutral_count:
            return "ğŸ“ˆ **ë¶„ì„**: ì „ë°˜ì ìœ¼ë¡œ ìƒìŠ¹ì„¸ ë˜ëŠ” ê°œì„  íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
        elif negative_count > positive_count and negative_count > neutral_count:
            return "ğŸ“‰ **ë¶„ì„**: ì „ë°˜ì ìœ¼ë¡œ í•˜ë½ì„¸ ë˜ëŠ” ë‘”í™” íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
        elif neutral_count > 0:
            return "ğŸ“Š **ë¶„ì„**: ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì¸ íë¦„ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        else:
            return "ğŸ“‹ **ë¶„ì„**: ë‹¤ì–‘í•œ ìš”ì¸ë“¤ì´ ë³µí•©ì ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤."

    def _generate_guidance_answer(self, query: str) -> str:
        """ë¬¸ì„œì™€ ì—°ê´€ ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ìœ ë„ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        guidance_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ "{query}"ì€ í˜„ì¬ ì‹œìŠ¤í…œì— ì €ì¥ëœ í•œêµ­ì€í–‰ ë‰´ìŠ¤ ë°ì´í„°ì™€ ì§ì ‘ì ì¸ ì—°ê´€ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ğŸ¦ **ë³¸ ì‹œìŠ¤í…œì—ì„œ ì œê³µ ê°€ëŠ¥í•œ ì •ë³´:**
- ë¯¸êµ­ ì†Œë¹„ìë¬¼ê°€ ë™í–¥ ë° ê¸ˆìœµì‹œì¥ ë°˜ì‘
- ë¯¸êµ­ ê³ ìš©ì§€í‘œ ë‚´ìš© ë° ë‰´ìš• ê¸ˆìœµì‹œì¥ ë°˜ì‘  
- ê°ì¢… ê²½ì œì§€í‘œ ë¶„ì„ ë° í•´ì„
- ê¸ˆìœµì‹œì¥ ë™í–¥ ë° ì „ë§
- í†µí™”ì •ì±… ê´€ë ¨ ì •ë³´

ğŸ’¡ **ì¶”ì²œ ì§ˆë¬¸ ì˜ˆì‹œ:**
- "ë¯¸êµ­ ì†Œë¹„ìë¬¼ê°€ ìƒìŠ¹ë¥ ì€ ì–´ë–»ê²Œ ë³€í™”í•˜ê³  ìˆë‚˜ìš”?"
- "ìµœê·¼ ë¯¸êµ­ ê³ ìš©ì§€í‘œê°€ ê¸ˆìœµì‹œì¥ì— ë¯¸ì¹œ ì˜í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "í•œêµ­ì€í–‰ì´ ë¶„ì„í•œ ë¯¸êµ­ ê²½ì œ ë™í–¥ì€ ì–´ë– í•œê°€ìš”?"
- "CPI ìƒìŠ¹ë¥ ì´ ê¸ˆìœµì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
- "ë¯¸êµ­ ì—°ë°©ì¤€ë¹„ì œë„ì˜ í†µí™”ì •ì±… ë³€í™”ëŠ” ì–´ë–¤ê°€ìš”?"

ìœ„ì™€ ê°™ì€ í•œêµ­ì€í–‰ ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê¸ˆìœµ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ì •í™•í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
        
        return guidance_answer

    def _log_answer_quality_metrics(self, raw_answer: str, cleaned_answer: str, generated_ids: torch.Tensor, elapsed_time: float):
        """ë‹µë³€ í’ˆì§ˆ ê´€ë ¨ ìƒì„¸ ë©”íŠ¸ë¦­ì„ ë¡œê¹…í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            raw_length = len(raw_answer)
            cleaned_length = len(cleaned_answer)
            token_count = len(generated_ids)
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ì§€í‘œ
            sentence_count = len([s for s in cleaned_answer.split('.') if s.strip()])
            word_count = len(cleaned_answer.split())
            avg_word_length = sum(len(word) for word in cleaned_answer.split()) / max(word_count, 1)
            
            # í† í° íš¨ìœ¨ì„±
            chars_per_token = raw_length / max(token_count, 1)
            tokens_per_second = token_count / max(elapsed_time, 0.001)
            
            # ì •ë¦¬ íš¨ê³¼
            cleaning_ratio = cleaned_length / max(raw_length, 1)
            
            logger.info(f"ğŸ“Š ë‹µë³€ í’ˆì§ˆ ë©”íŠ¸ë¦­:")
            logger.info(f"   ğŸ“ ê¸¸ì´: ì›ë³¸ {raw_length}ì â†’ ì •ë¦¬í›„ {cleaned_length}ì (ì •ë¦¬ìœ¨: {cleaning_ratio:.2f})")
            logger.info(f"   ğŸ”¤ í† í°: {token_count}ê°œ, ë¬¸ì¥: {sentence_count}ê°œ, ë‹¨ì–´: {word_count}ê°œ")
            logger.info(f"   ğŸ“ˆ íš¨ìœ¨ì„±: {chars_per_token:.1f}ì/í† í°, {tokens_per_second:.1f}í† í°/ì´ˆ")
            logger.info(f"   ğŸ“ í‰ê·  ë‹¨ì–´ ê¸¸ì´: {avg_word_length:.1f}ì")
            
            # í† í° ë¶„í¬ ë¶„ì„ (ì²˜ìŒ/ë ëª‡ ê°œ)
            if len(generated_ids) > 0:
                first_tokens = generated_ids[:3].tolist()
                last_tokens = generated_ids[-3:].tolist()
                logger.debug(f"ğŸ¯ í† í° íŒ¨í„´: ì‹œì‘ {first_tokens} ... ë {last_tokens}")
                
                # 0 í† í° ë¹„ìœ¨ (íŒ¨ë”© ì²´í¬)
                zero_token_ratio = (generated_ids == 0).sum().item() / len(generated_ids)
                if zero_token_ratio > 0:
                    logger.warning(f"âš ï¸ 0 í† í° ë¹„ìœ¨: {zero_token_ratio:.2%}")
            
        except Exception as e:
            logger.debug(f"í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")

    def _log_success_metrics(self, query: str, answer: str, elapsed_time: float, generation_mode: str):
        """ì„±ê³µì ì¸ ë‹µë³€ ìƒì„±ì— ëŒ€í•œ ë©”íŠ¸ë¦­ì„ ë¡œê¹…í•©ë‹ˆë‹¤."""
        try:
            # ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­ë„ ê°„ë‹¨ ë¶„ì„
            query_words = set(query.lower().split())
            answer_words = set(answer.lower().split())
            word_overlap = len(query_words & answer_words) / max(len(query_words), 1)
            
            # ë‹µë³€ íŠ¹ì„±
            is_korean = any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in answer)
            has_numbers = any(char.isdigit() for char in answer)
            has_punctuation = any(char in '.,!?;:' for char in answer)
            
            # ì„±ëŠ¥ ì¹´í…Œê³ ë¦¬
            if elapsed_time < 2:
                speed_category = "ë§¤ìš°ë¹ ë¦„"
            elif elapsed_time < 5:
                speed_category = "ë¹ ë¦„"
            elif elapsed_time < 10:
                speed_category = "ë³´í†µ"
            else:
                speed_category = "ëŠë¦¼"
            
            logger.info(f"âœ… ì„±ê³µ ë©”íŠ¸ë¦­ ìš”ì•½:")
            logger.info(f"   ğŸš€ ì„±ëŠ¥: {speed_category} ({elapsed_time:.2f}ì´ˆ)")
            logger.info(f"   ğŸ¯ ìƒì„±ëª¨ë“œ: {generation_mode}")
            logger.info(f"   ğŸ” ë‹¨ì–´ ê²¹ì¹¨: {word_overlap:.2%}")
            logger.info(f"   ğŸŒ ì–¸ì–´íŠ¹ì„±: í•œêµ­ì–´={is_korean}, ìˆ«ì={has_numbers}, êµ¬ë‘ì ={has_punctuation}")
            
            # ì„±ê³µ íŒ¨í„´ ì¶”ì ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ë¡œê·¸
            success_data = {
                "timestamp": time.time(),
                "model": self.model_name,
                "generation_mode": generation_mode,
                "elapsed_time": elapsed_time,
                "query_length": len(query),
                "answer_length": len(answer),
                "word_overlap": word_overlap,
                "speed_category": speed_category
            }
            logger.debug(f"ğŸ“Š ì„±ê³µ ë°ì´í„°: {success_data}")
            
        except Exception as e:
            logger.debug(f"ì„±ê³µ ë©”íŠ¸ë¦­ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")

    def _log_model_state(self, step: str = "unknown"):
        """ëª¨ë¸ ìƒíƒœë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤."""
        try:
            logger.debug(f"[{step}] ëª¨ë¸ ìƒíƒœ ì²´í¬")
            
            # ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸
            model_device = next(self.model.parameters()).device
            logger.debug(f"[{step}] ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model_device}")
            
            # ëª¨ë¸ ë°ì´í„° íƒ€ì… í™•ì¸
            model_dtype = next(self.model.parameters()).dtype
            logger.debug(f"[{step}] ëª¨ë¸ ë°ì´í„° íƒ€ì…: {model_dtype}")
            
            # ëª¨ë¸ í‰ê°€ ëª¨ë“œ í™•ì¸
            logger.debug(f"[{step}] ëª¨ë¸ í‰ê°€ ëª¨ë“œ: {not self.model.training}")
            
            # íŒŒë¼ë¯¸í„° ê±´ì „ì„± ê°„ë‹¨ ì²´í¬
            params = list(self.model.parameters())
            if params:
                first_param = params[0]
                if torch.isnan(first_param).any() or torch.isinf(first_param).any():
                    logger.error(f"[{step}] ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— NaN ë˜ëŠ” Inf ê°’ ë°œê²¬!")
                    return False
                    
            return True
            
        except Exception as e:
            logger.error(f"[{step}] ëª¨ë¸ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

class VectorStore:
    """Qdrant ë²¡í„° ìŠ¤í† ì–´ í´ë˜ìŠ¤"""
    
    def __init__(self, collection_name: str = Config.COLLECTION_NAME):
        self.collection_name = collection_name
        self.client = QdrantClient(url=Config.QDRANT_URL)
        self._setup_collection()
    
    def _setup_collection(self):
        """ì»¬ë ‰ì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            collections = self.client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name not in collection_names:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=1024,  # BGE-M3 ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            else:
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ì´ë¯¸ ì¡´ì¬")
                
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def add_documents(self, documents: List[Document], embeddings: np.ndarray):
        """ë¬¸ì„œë“¤ì„ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            points = []
            for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
                point = PointStruct(
                    id=len(points),
                    vector=embedding.tolist(),
                    payload={
                        "content": doc.content,
                        "doc_id": doc.doc_id,
                        "doc_type": doc.doc_type,
                        **doc.metadata
                    }
                )
                points.append(point)
            
            batch_size = 100
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch
                )
            
            logger.info(f"{len(documents)}ê°œ ë¬¸ì„œ ë²¡í„° ìŠ¤í† ì–´ ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def search(self, query_embedding: np.ndarray, top_k: int = 10, section_types: List[str] = None) -> List[Dict]:
        """ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # section_type í•„í„° ì„¤ì • (ê¸°ë³¸ê°’: analysisì™€ image_summaryë§Œ)
            if section_types is None:
                section_types = ["analysis", "image_summary"]
            
            # í•„í„° ì¡°ê±´ ìƒì„±
            filter_condition = {
                "must": [
                    {
                        "key": "section_type",
                        "match": {
                            "any": section_types
                        }
                    }
                ]
            }
            
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                limit=top_k,
                with_payload=True,
                score_threshold=Config.SIMILARITY_THRESHOLD,
                query_filter=filter_condition
            )
            
            return [
                {
                    "content": result.payload["content"],
                    "score": result.score,
                    "metadata": {k: v for k, v in result.payload.items() if k != "content"}
                }
                for result in results
            ]
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_questions(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict]:
        """ì§ˆë¬¸ ì„¹ì…˜ë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        return self.search(query_embedding, top_k, section_types=["questions"])

class DocumentLoader:
    """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ë¡œë” í´ë˜ìŠ¤"""
    
    @staticmethod
    def load_markdown_files(directory: Path, doc_type: str) -> List[Document]:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  í•„ìš”í•œ ì„¹ì…˜ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        documents = []
        
        if not directory.exists():
            logger.warning(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
            return documents
        
        # .md íŒŒì¼ë§Œ ì„ íƒ (Zone.Identifier ë“± ì œì™¸)
        markdown_files = [f for f in directory.glob("*.md") 
                         if not f.name.endswith(".mdZone.Identifier")]
        logger.info(f"{directory}ì—ì„œ {len(markdown_files)}ê°œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë°œê²¬")
        
        for file_path in markdown_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if doc_type == "text":
                    # í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì„¹ì…˜ ì¶”ì¶œ
                    extracted_docs = DocumentLoader._extract_text_analysis_sections(content, file_path)
                elif doc_type == "image":
                    # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì—ì„œ í•„ìš”í•œ ì„¹ì…˜ ì¶”ì¶œ
                    extracted_docs = DocumentLoader._extract_image_analysis_sections(content, file_path)
                else:
                    logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” doc_type: {doc_type}")
                    continue
                
                documents.extend(extracted_docs)
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ {file_path}: {e}")
                continue
        
        logger.info(f"{len(documents)}ê°œ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ")
        return documents
    
    @staticmethod
    def _extract_text_analysis_sections(content: str, file_path: Path) -> List[Document]:
        """í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ğŸ” ë¶„ì„ ê²°ê³¼ì™€ â“ Hypothetical Questions ì¶”ì¶œ"""
        documents = []
        
        # ê° í…ìŠ¤íŠ¸ ë¶„ì„ ë¸”ë¡ì„ ë¶„ë¦¬
        sections = content.split("## ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„")
        
        for i, section in enumerate(sections[1:], 1):  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
            try:
                # ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                analysis_start = section.find("### ğŸ” ë¶„ì„ ê²°ê³¼")
                questions_start = section.find("### â“ Hypothetical Questions")
                
                if analysis_start != -1 and questions_start != -1:
                    # ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                    analysis_content = section[analysis_start:questions_start].strip()
                    analysis_content = analysis_content.replace("### ğŸ” ë¶„ì„ ê²°ê³¼", "").strip()
                    
                    if analysis_content:
                        doc_id = f"{file_path.stem}_analysis_{i}"
                        metadata = {
                            "file_path": str(file_path),
                            "file_name": file_path.name,
                            "section_type": "analysis",
                            "section_number": i
                        }
                        
                        documents.append(Document(
                            content=analysis_content,
                            metadata=metadata,
                            doc_id=doc_id,
                            doc_type="text"
                        ))
                    
                    # Hypothetical Questions ì¶”ì¶œ
                    next_section = section.find("---", questions_start)
                    if next_section == -1:
                        next_section = len(section)
                    
                    questions_content = section[questions_start:next_section].strip()
                    questions_content = questions_content.replace("### â“ Hypothetical Questions", "").strip()
                    
                    if questions_content:
                        doc_id = f"{file_path.stem}_questions_{i}"
                        metadata = {
                            "file_path": str(file_path),
                            "file_name": file_path.name,
                            "section_type": "questions",
                            "section_number": i
                        }
                        
                        documents.append(Document(
                            content=questions_content,
                            metadata=metadata,
                            doc_id=doc_id,
                            doc_type="text"
                        ))
                        
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ (ì„¹ì…˜ {i}): {e}")
                continue
        
        return documents
    
    @staticmethod
    def _extract_image_analysis_sections(content: str, file_path: Path) -> List[Document]:
        """ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì—ì„œ ğŸ“‹ Image Summaryì™€ â“ Hypothetical Questions ì¶”ì¶œ"""
        documents = []
        
        # ê° ì´ë¯¸ì§€ ë¶„ì„ ë¸”ë¡ì„ ë¶„ë¦¬
        sections = content.split("## ğŸ–¼ï¸ ì´ë¯¸ì§€")
        
        for i, section in enumerate(sections[1:], 1):  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
            try:
                # ì„¹ì…˜ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                image_filename = ""
                lines = section.split('\n')
                for line in lines[:5]:  # ì²˜ìŒ 5ì¤„ì—ì„œ íŒŒì¼ëª… ì°¾ê¸°
                    if '.png' in line and 'ì´ë¯¸ì§€' in line:
                        # "## ğŸ–¼ï¸ ì´ë¯¸ì§€ 1: íŒŒì¼ëª….png" í˜•íƒœì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                        parts = line.split(': ')
                        if len(parts) > 1:
                            image_filename = parts[1].strip()
                        break
                
                # íŒŒì¼ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ íƒ€ì… íŒë³„ (table vs picture)
                image_type = "table" if "-table-" in image_filename else "picture"
                
                # Image Summary ì¶”ì¶œ
                summary_start = section.find("### ğŸ“‹ Image Summary")
                questions_start = section.find("### â“ Hypothetical Questions")
                
                if summary_start != -1 and questions_start != -1:
                    # Image Summary ì¶”ì¶œ
                    summary_content = section[summary_start:questions_start].strip()
                    summary_content = summary_content.replace("### ğŸ“‹ Image Summary", "").strip()
                    
                    if summary_content:
                        doc_id = f"{file_path.stem}_summary_{image_type}_{i}"
                        metadata = {
                            "file_path": str(file_path),
                            "file_name": file_path.name,
                            "section_type": "image_summary",
                            "image_type": image_type,  # "table" or "picture"
                            "image_filename": image_filename,  # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ëª…
                            "section_number": i
                        }
                        
                        documents.append(Document(
                            content=summary_content,
                            metadata=metadata,
                            doc_id=doc_id,
                            doc_type="image"
                        ))
                    
                    # Hypothetical Questions ì¶”ì¶œ
                    next_section = section.find("---", questions_start)
                    if next_section == -1:
                        next_section = len(section)
                    
                    questions_content = section[questions_start:next_section].strip()
                    questions_content = questions_content.replace("### â“ Hypothetical Questions", "").strip()
                    
                    if questions_content:
                        doc_id = f"{file_path.stem}_questions_{image_type}_{i}"
                        metadata = {
                            "file_path": str(file_path),
                            "file_name": file_path.name,
                            "section_type": "questions",
                            "image_type": image_type,  # "table" or "picture"
                            "image_filename": image_filename,  # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ëª…
                            "section_number": i
                        }
                        
                        documents.append(Document(
                            content=questions_content,
                            metadata=metadata,
                            doc_id=doc_id,
                            doc_type="image"
                        ))
                        
            except Exception as e:
                logger.error(f"ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ (ì„¹ì…˜ {i}): {e}")
                continue
        
        return documents

class RAGSystem:
    """í†µí•© RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.embedding_model = EmbeddingModel()
        self.reranker = RerankerModel()
        self.answer_generator = AnswerGenerator()
        self.vector_store = VectorStore()
        
    def search_and_answer(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # 1. ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_model.encode([query])
            
            # 2. ë²¡í„° ê²€ìƒ‰ (analysisì™€ image_summaryë§Œ)
            search_results = self.vector_store.search(
                query_embedding[0], 
                top_k=Config.TOP_K_RETRIEVAL
            )
            
            # 3. ë¬¸ì„œ ì—°ê´€ì„± íŒë‹¨ (ë™ì  ì„ê³„ê°’ + í‚¤ì›Œë“œ ë³´ì •)
            has_relevant_docs = self._check_document_relevance(search_results, query)
            
            if not search_results or not has_relevant_docs:
                # ë¬¸ì„œì™€ ì—°ê´€ ì—†ëŠ” ì§ˆë¬¸ ì²˜ë¦¬
                answer = self.answer_generator.generate_answer(
                    query, [], sources=[], has_relevant_docs=False
                )
                return {
                    "answer": answer,
                    "sources": [],
                    "confidence": 0.0,
                    "query_analysis": "",
                    "related_questions": []
                }
            
            # 4. ì§ˆë¬¸ ì˜ë¯¸ ë¶„ì„
            query_analysis = self._analyze_query_intent(query)
            
            # 5. ê´€ë ¨ ì§ˆë¬¸ë“¤ ê²€ìƒ‰ (questions ì„¹ì…˜ì—ì„œ)
            related_questions = self._get_related_questions(query_embedding[0], search_results)
            
            # 6. ë¦¬ë­í‚¹
            documents = [result["content"] for result in search_results]
            reranked_indices = self.reranker.rerank(query, documents, top_k=Config.TOP_K_RERANK)
            
            # 7. ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = []
            sources = []
            for idx, score in reranked_indices:
                result = search_results[idx]
                context.append(result["content"])
                sources.append({
                    "content": result["content"][:200] + "...",
                    "rerank_score": float(score),
                    "vector_score": float(result["score"]),
                    "metadata": result["metadata"]
                })
            
            # 8. ë‹µë³€ ìƒì„± (ì¶œì²˜ ì •ë³´ í¬í•¨)
            answer = self.answer_generator.generate_answer(
                query, context, sources=sources, has_relevant_docs=True
            )
            
            # 9. ê°œì„ ëœ ì‹ ë¢°ë„ ê³„ì‚°
            if reranked_indices:
                # ë¦¬ë­í‚¹ ì ìˆ˜ë“¤ì„ ì •ê·œí™”í•˜ì—¬ 0-1 ë²”ìœ„ë¡œ ë³€í™˜
                rerank_scores = [abs(float(score)) for _, score in reranked_indices]
                vector_scores = [float(sources[i]["vector_score"]) for i in range(len(sources))]
                
                # ë²¡í„° ìœ ì‚¬ë„ì™€ ë¦¬ë­í‚¹ ì ìˆ˜ë¥¼ ê²°í•©
                # ë²¡í„° ì ìˆ˜ëŠ” ì´ë¯¸ 0-1 ë²”ìœ„, ë¦¬ë­í‚¹ ì ìˆ˜ëŠ” ì •ê·œí™”
                if rerank_scores:
                    max_rerank = max(rerank_scores) if max(rerank_scores) > 0 else 1.0
                    normalized_rerank = [score / max_rerank for score in rerank_scores]
                    
                    # ê°€ì¤‘ í‰ê·  ê³„ì‚° (ë²¡í„° ìœ ì‚¬ë„ 60%, ë¦¬ë­í‚¹ 40%)
                    combined_scores = []
                    for i, (vec_score, rerank_score) in enumerate(zip(vector_scores, normalized_rerank)):
                        combined_score = (vec_score * 0.6) + (rerank_score * 0.4)
                        combined_scores.append(combined_score)
                    
                    # ìƒìœ„ ê²°ê³¼ë“¤ì˜ í‰ê· ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
                    confidence = np.mean(combined_scores[:3])  # ìƒìœ„ 3ê°œ í‰ê· 
                else:
                    confidence = np.mean(vector_scores[:3])  # ë²¡í„° ì ìˆ˜ë§Œ ì‚¬ìš©
            else:
                confidence = 0.0
            
            # ì‹ ë¢°ë„ë¥¼ 0-1 ë²”ìœ„ë¡œ ì œí•œ
            confidence = max(0.0, min(1.0, float(confidence)))
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "query_analysis": query_analysis,
                "related_questions": related_questions
            }
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0,
                "query_analysis": "",
                "related_questions": []
            }
    
    def _check_document_relevance(self, search_results: List[Dict], query: str = None) -> bool:
        """
        ë™ì  ì„ê³„ê°’ê³¼ í‚¤ì›Œë“œ ë³´ì •ì„ ì‚¬ìš©í•œ ê°œì„ ëœ ë¬¸ì„œ ì—°ê´€ì„± íŒë‹¨
        """
        if not search_results:
            return False
        
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        max_score = max(result["score"] for result in search_results)
        top_scores = [result["score"] for result in search_results[:3]]
        avg_score = np.mean(top_scores)
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚°
        dynamic_threshold = self._calculate_dynamic_threshold(query, search_results)
        dynamic_avg_threshold = dynamic_threshold * 0.8
        
        # í‚¤ì›Œë“œ ë³´ì • ê³„ì‚°
        keyword_boost = self._calculate_keyword_boost(query, search_results)
        
        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ë³´ì • ê³„ì‚°
        semantic_boost = self._calculate_semantic_boost(query, search_results)
        
        # ì¢…í•© ë³´ì •ê°’ ê³„ì‚°
        total_boost = keyword_boost + semantic_boost
        
        # ë³´ì •ëœ ì ìˆ˜ ê³„ì‚°
        adjusted_max_score = max_score + total_boost
        adjusted_avg_score = avg_score + total_boost
        
        # 1. ìµœê³  ì ìˆ˜ ê²€ì¦ (ë™ì  ì„ê³„ê°’ ì‚¬ìš©)
        if adjusted_max_score < dynamic_threshold:
            logger.info(f"ìµœê³  ì ìˆ˜ {adjusted_max_score:.3f} (í‚¤ì›Œë“œ: +{keyword_boost:.3f}, ì˜ë¯¸: +{semantic_boost:.3f}) < ë™ì  ì„ê³„ê°’ {dynamic_threshold:.3f}")
            return False
        
        # 2. í‰ê·  ì ìˆ˜ ê²€ì¦ (ë™ì  ì„ê³„ê°’ ì‚¬ìš©)
        if adjusted_avg_score < dynamic_avg_threshold:
            logger.info(f"í‰ê·  ì ìˆ˜ {adjusted_avg_score:.3f} (í‚¤ì›Œë“œ: +{keyword_boost:.3f}, ì˜ë¯¸: +{semantic_boost:.3f}) < ë™ì  í‰ê·  ì„ê³„ê°’ {dynamic_avg_threshold:.3f}")
            return False
        
        # 3. ì¶”ê°€ ê²€ì¦: í‚¤ì›Œë“œ ë§¤ì¹­ë¥ 
        keyword_match_rate = self._calculate_keyword_match_rate(query, search_results)
        if keyword_match_rate < 0.1 and adjusted_max_score < 0.6:  # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë§¤ìš° ë‚®ê³  ì ìˆ˜ë„ ë³´í†µì¸ ê²½ìš°
            logger.info(f"í‚¤ì›Œë“œ ë§¤ì¹­ë¥  {keyword_match_rate:.3f}ì´ ë„ˆë¬´ ë‚®ìŒ (ì ìˆ˜: {adjusted_max_score:.3f})")
            return False
        
        logger.info(f"ë¬¸ì„œ ì—°ê´€ì„± í™•ì¸ë¨ - ìµœê³ : {adjusted_max_score:.3f}, í‰ê· : {adjusted_avg_score:.3f}, "
                   f"ë™ì  ì„ê³„ê°’: {dynamic_threshold:.3f}, í‚¤ì›Œë“œ ë§¤ì¹­: {keyword_match_rate:.3f}, "
                   f"ë³´ì •(í‚¤ì›Œë“œ: +{keyword_boost:.3f}, ì˜ë¯¸: +{semantic_boost:.3f})")
        return True
    
    def _calculate_dynamic_threshold(self, query: str, search_results: List[Dict]) -> float:
        """
        ì§ˆë¬¸ê³¼ ê²€ìƒ‰ ê²°ê³¼ì˜ íŠ¹ì„±ì— ë”°ë¼ ë™ì  ì„ê³„ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        base_threshold = Config.SIMILARITY_THRESHOLD  # 0.45
        
        if not query:
            return base_threshold
        
        # 1. ì§ˆë¬¸ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        query_length = len(query.strip())
        if query_length < 10:  # ë§¤ìš° ì§§ì€ ì§ˆë¬¸
            length_adjustment = 0.05  # ì„ê³„ê°’ ìƒìŠ¹ (ë” ì—„ê²©)
        elif query_length > 50:  # ê¸´ ì§ˆë¬¸
            length_adjustment = -0.03  # ì„ê³„ê°’ í•˜ë½ (ë” ê´€ëŒ€)
        else:
            length_adjustment = 0.0
        
        # 2. í‚¤ì›Œë“œ ë°€ë„ì— ë”°ë¥¸ ì¡°ì •
        economic_keywords = self._count_economic_keywords(query)
        if economic_keywords >= 3:  # ê²½ì œ í‚¤ì›Œë“œê°€ ë§ìœ¼ë©´
            keyword_adjustment = -0.08  # ë” ê´€ëŒ€í•˜ê²Œ
        elif economic_keywords >= 1:
            keyword_adjustment = -0.04  # ì•½ê°„ ê´€ëŒ€í•˜ê²Œ
        else:
            keyword_adjustment = 0.05   # ë” ì—„ê²©í•˜ê²Œ
        
        # 3. ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì— ë”°ë¥¸ ì¡°ì •
        if search_results:
            score_variance = np.var([result["score"] for result in search_results[:5]])
            if score_variance > 0.1:  # ì ìˆ˜ ë¶„ì‚°ì´ í¬ë©´ (í’ˆì§ˆì´ ì¼ê´€ë˜ì§€ ì•ŠìŒ)
                variance_adjustment = 0.03  # ë” ì—„ê²©í•˜ê²Œ
            else:
                variance_adjustment = -0.02  # ë” ê´€ëŒ€í•˜ê²Œ
        else:
            variance_adjustment = 0.0
        
        # 4. ë„ë©”ì¸ë³„ ì¡°ì •
        domain_adjustment = self._get_domain_adjustment(query)
        
        # ìµœì¢… ì„ê³„ê°’ ê³„ì‚°
        dynamic_threshold = base_threshold + length_adjustment + keyword_adjustment + variance_adjustment + domain_adjustment
        
        # ì„ê³„ê°’ ë²”ìœ„ ì œí•œ (0.25 ~ 0.65)
        dynamic_threshold = max(0.25, min(0.65, dynamic_threshold))
        
        logger.debug(f"ë™ì  ì„ê³„ê°’ ê³„ì‚°: ê¸°ë³¸ {base_threshold:.3f} + ê¸¸ì´ {length_adjustment:.3f} + "
                    f"í‚¤ì›Œë“œ {keyword_adjustment:.3f} + ë¶„ì‚° {variance_adjustment:.3f} + "
                    f"ë„ë©”ì¸ {domain_adjustment:.3f} = {dynamic_threshold:.3f}")
        
        return dynamic_threshold
    
    def _calculate_keyword_boost(self, query: str, search_results: List[Dict]) -> float:
        """
        í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë„ì— ë”°ë¥¸ ì ìˆ˜ ë³´ì •ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        if not query or not search_results:
            return 0.0
        
        # ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ì‹œìŠ¤í…œ í™œìš©)
        try:
            if hasattr(self, 'answer_generator') and hasattr(self.answer_generator, '_extract_query_keywords'):
                keywords = self.answer_generator._extract_query_keywords(query)
            else:
                # í´ë°±: ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_basic_keywords(query)
        except:
            keywords = self._extract_basic_keywords(query)
        
        if not keywords:
            return 0.0
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë„ ê³„ì‚°
        total_matches = 0
        total_keywords = len(keywords)
        
        for result in search_results[:3]:  # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ í™•ì¸
            content = result.get("content", "").lower()
            matches = sum(1 for keyword in keywords if keyword.lower() in content)
            total_matches += matches
        
        # ë§¤ì¹­ë¥  ê³„ì‚°
        match_rate = total_matches / (total_keywords * 3) if total_keywords > 0 else 0.0
        
        # ë³´ì •ê°’ ê³„ì‚° (ìµœëŒ€ 0.15ê¹Œì§€)
        keyword_boost = min(0.15, match_rate * 0.3)
        
        logger.debug(f"í‚¤ì›Œë“œ ë³´ì •: {keywords} â†’ ë§¤ì¹­ë¥  {match_rate:.3f} â†’ ë³´ì • +{keyword_boost:.3f}")
        
        return keyword_boost
    
    def _calculate_keyword_match_rate(self, query: str, search_results: List[Dict]) -> float:
        """
        í‚¤ì›Œë“œ ë§¤ì¹­ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        if not query or not search_results:
            return 0.0
        
        try:
            if hasattr(self, 'answer_generator') and hasattr(self.answer_generator, '_extract_query_keywords'):
                keywords = self.answer_generator._extract_query_keywords(query)
            else:
                keywords = self._extract_basic_keywords(query)
        except:
            keywords = self._extract_basic_keywords(query)
        
        if not keywords:
            return 0.0
        
        # ìƒìœ„ ë¬¸ì„œë“¤ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
        matched_keywords = set()
        for result in search_results[:5]:
            content = result.get("content", "").lower()
            for keyword in keywords:
                if keyword.lower() in content:
                    matched_keywords.add(keyword)
        
        match_rate = len(matched_keywords) / len(keywords)
        return match_rate
    
    def _count_economic_keywords(self, query: str) -> int:
        """ê²½ì œ/ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤."""
        economic_terms = [
            "ì†Œë¹„ìë¬¼ê°€", "cpi", "ì¸í”Œë ˆì´ì…˜", "ê³ ìš©", "ì‹¤ì—…", "ì¼ìë¦¬", "ê¸ˆë¦¬", "ê¸°ì¤€ê¸ˆë¦¬",
            "fomc", "ì—°ì¤€", "í†µí™”ì •ì±…", "ì–‘ì ì™„í™”", "qe", "ê¸ˆìœµì‹œì¥", "ì£¼ì‹", "ì±„ê¶Œ",
            "ê²½ì œ", "ì„±ì¥", "gdp", "ê²½ê¸°", "ìƒìŠ¹ë¥ ", "í•˜ë½", "ì¦ê°€", "ê°ì†Œ"
        ]
        
        query_lower = query.lower()
        count = sum(1 for term in economic_terms if term in query_lower)
        return count
    
    def _get_domain_adjustment(self, query: str) -> float:
        """ë„ë©”ì¸ë³„ ì„ê³„ê°’ ì¡°ì •ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        query_lower = query.lower()
        
        # í•µì‹¬ ê²½ì œ ìš©ì–´ê°€ ìˆìœ¼ë©´ ë” ê´€ëŒ€í•˜ê²Œ
        core_terms = ["ì†Œë¹„ìë¬¼ê°€", "cpi", "ê³ ìš©ì§€í‘œ", "ì‹¤ì—…ë¥ ", "ê¸ˆë¦¬", "fomc"]
        if any(term in query_lower for term in core_terms):
            return -0.06
        
        # ì¼ë°˜ ê²½ì œ ìš©ì–´ê°€ ìˆìœ¼ë©´ ì•½ê°„ ê´€ëŒ€í•˜ê²Œ
        general_terms = ["ê²½ì œ", "ì‹œì¥", "ì •ì±…", "ì§€í‘œ", "ìƒìŠ¹", "í•˜ë½"]
        if any(term in query_lower for term in general_terms):
            return -0.03
        
        # ê²½ì œì™€ ë¬´ê´€í•œ ìš©ì–´ê°€ ìˆìœ¼ë©´ ë” ì—„ê²©í•˜ê²Œ
        unrelated_terms = ["ìš”ë¦¬", "ì—¬í–‰", "ê²Œì„", "ì˜í™”", "ìŠ¤í¬ì¸ ", "ë‚ ì”¨"]
        if any(term in query_lower for term in unrelated_terms):
            return 0.10
        
        return 0.0
    
    def _extract_basic_keywords(self, query: str) -> List[str]:
        """ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ (í´ë°±ìš©)"""
        basic_keywords = []
        economic_terms = [
            "ì†Œë¹„ìë¬¼ê°€", "CPI", "ì¸í”Œë ˆì´ì…˜", "ê³ ìš©", "ì‹¤ì—…", "ì¼ìë¦¬", "ê¸ˆë¦¬", 
            "ê¸°ì¤€ê¸ˆë¦¬", "FOMC", "ì—°ì¤€", "í†µí™”ì •ì±…", "ê²½ì œ", "ì„±ì¥", "GDP"
        ]
        
        for term in economic_terms:
            if term in query or term.lower() in query.lower():
                basic_keywords.append(term)
        
        return basic_keywords
    
    def _calculate_semantic_boost(self, query: str, search_results: List[Dict]) -> float:
        """
        ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ê°€ ë³´ì •ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        if not query or not search_results:
            return 0.0
        
        try:
            # BGE-m3-ko ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ê¸°ì¡´ ì‹œìŠ¤í…œ í™œìš©)
            query_embedding = self.embedding_model.encode([query])
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš© ì¶”ì¶œ
            doc_contents = [result.get("content", "")[:300] for result in search_results[:3]]  # ìƒìœ„ 3ê°œ, 300ì ì œí•œ
            
            if not any(doc_contents):
                return 0.0
            
            # ë¬¸ì„œ ë‚´ìš©ë“¤ì˜ ì„ë² ë”© ê³„ì‚°
            doc_embeddings = self.embedding_model.encode(doc_contents)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
            
            # ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶„ì„
            max_similarity = max(similarities)
            avg_similarity = np.mean(similarities)
            
            # ê²½ì œ/ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” ë³´ì •
            domain_boost = self._calculate_domain_semantic_boost(query, doc_contents)
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ë³´ì •ê°’ ê³„ì‚°
            # ë†’ì€ ì˜ë¯¸ì  ìœ ì‚¬ë„ì¼ìˆ˜ë¡ ë” í° ë³´ì •
            similarity_boost = 0.0
            
            if max_similarity > 0.8:  # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„
                similarity_boost = 0.10
            elif max_similarity > 0.6:  # ë†’ì€ ìœ ì‚¬ë„
                similarity_boost = 0.06
            elif max_similarity > 0.4:  # ì¤‘ê°„ ìœ ì‚¬ë„
                similarity_boost = 0.03
            
            # í‰ê·  ìœ ì‚¬ë„ë„ ê³ ë ¤ (ì¼ê´€ì„± ë³´ë„ˆìŠ¤)
            if avg_similarity > 0.5 and max_similarity > 0.6:
                similarity_boost += 0.02  # ì¼ê´€ì„± ë³´ë„ˆìŠ¤
            
            # ìµœì¢… ì˜ë¯¸ì  ë³´ì •ê°’ (ìµœëŒ€ 0.12)
            semantic_boost = min(0.12, similarity_boost + domain_boost)
            
            logger.debug(f"ì˜ë¯¸ì  ë³´ì •: ìµœê³  ìœ ì‚¬ë„ {max_similarity:.3f}, í‰ê·  {avg_similarity:.3f}, "
                        f"ë„ë©”ì¸ ë³´ì • +{domain_boost:.3f} â†’ ì´ ë³´ì • +{semantic_boost:.3f}")
            
            return semantic_boost
            
        except Exception as e:
            logger.warning(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ë³´ì • ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_domain_semantic_boost(self, query: str, doc_contents: List[str]) -> float:
        """
        ê²½ì œ/ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” ì˜ë¯¸ì  ë³´ì •ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        # ê²½ì œ/ê¸ˆìœµ ë„ë©”ì¸ í•µì‹¬ ê°œë…ë“¤
        economic_concepts = {
            "inflation": ["ì¸í”Œë ˆì´ì…˜", "ë¬¼ê°€ìƒìŠ¹", "ì†Œë¹„ìë¬¼ê°€", "CPI"],
            "employment": ["ê³ ìš©", "ì‹¤ì—…", "ì¼ìë¦¬", "ì·¨ì—…", "ë…¸ë™ì‹œì¥"],
            "monetary_policy": ["ê¸ˆë¦¬", "í†µí™”ì •ì±…", "FOMC", "ì—°ì¤€", "ê¸°ì¤€ê¸ˆë¦¬"],
            "market": ["ì‹œì¥", "ê¸ˆìœµì‹œì¥", "ì£¼ì‹", "ì±„ê¶Œ", "íˆ¬ì"],
            "growth": ["ì„±ì¥", "GDP", "ê²½ì œì„±ì¥", "ê²½ê¸°", "íšŒë³µ"]
        }
        
        query_lower = query.lower()
        combined_docs = " ".join(doc_contents).lower()
        
        concept_matches = 0
        total_concepts = len(economic_concepts)
        
        for concept_name, keywords in economic_concepts.items():
            # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ëª¨ë‘ì—ì„œ ê°œë…ì´ ë°œê²¬ë˜ë©´ ë§¤ì¹­
            query_has_concept = any(keyword in query_lower for keyword in keywords)
            doc_has_concept = any(keyword in combined_docs for keyword in keywords)
            
            if query_has_concept and doc_has_concept:
                concept_matches += 1
        
        # ê°œë… ë§¤ì¹­ë¥ ì— ë”°ë¥¸ ë³´ì •
        match_rate = concept_matches / total_concepts
        
        if match_rate >= 0.4:  # 40% ì´ìƒ ë§¤ì¹­
            return 0.04
        elif match_rate >= 0.2:  # 20% ì´ìƒ ë§¤ì¹­
            return 0.02
        else:
            return 0.0
    
    def _analyze_query_intent(self, query: str) -> str:
        """ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        query_lower = query.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜
        if any(keyword in query for keyword in ["ì†Œë¹„ìë¬¼ê°€", "cpi", "ì¸í”Œë ˆì´ì…˜"]):
            return "ğŸ’° ì†Œë¹„ìë¬¼ê°€ ê´€ë ¨ ì§ˆë¬¸: ë¯¸êµ­ì˜ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI) ë™í–¥, ì¸í”Œë ˆì´ì…˜ ì••ë ¥, ë¬¼ê°€ ë³€í™” ë“±ì— ëŒ€í•´ ë¬¸ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤."
        
        elif any(keyword in query for keyword in ["ê³ ìš©", "ì‹¤ì—…", "ì¼ìë¦¬", "ì·¨ì—…"]):
            return "ğŸ‘¥ ê³ ìš©ì§€í‘œ ê´€ë ¨ ì§ˆë¬¸: ë¯¸êµ­ì˜ ê³ ìš© ìƒí™©, ì‹¤ì—…ë¥ , ì¼ìë¦¬ ì°½ì¶œ ë“± ë…¸ë™ì‹œì¥ ë™í–¥ì— ëŒ€í•´ ë¬¸ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤."
        
        elif any(keyword in query for keyword in ["ê¸ˆë¦¬", "ê¸°ì¤€ê¸ˆë¦¬", "fomc", "ì—°ì¤€", "í†µí™”ì •ì±…"]):
            return "ğŸ¦ í†µí™”ì •ì±… ê´€ë ¨ ì§ˆë¬¸: ë¯¸êµ­ ì—°ë°©ì¤€ë¹„ì œë„(Fed)ì˜ ê¸ˆë¦¬ ì •ì±…, FOMC íšŒì˜ ê²°ê³¼, í†µí™”ì •ì±… ë°©í–¥ ë“±ì— ëŒ€í•´ ë¬¸ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤."
        
        elif any(keyword in query for keyword in ["ê¸ˆìœµì‹œì¥", "ì£¼ì‹", "ì±„ê¶Œ", "ì‹œì¥ë°˜ì‘"]):
            return "ğŸ“ˆ ê¸ˆìœµì‹œì¥ ê´€ë ¨ ì§ˆë¬¸: ê²½ì œì§€í‘œ ë°œí‘œì— ë”°ë¥¸ ê¸ˆìœµì‹œì¥ ë°˜ì‘, ì£¼ì‹ì‹œì¥ ë° ì±„ê¶Œì‹œì¥ ë™í–¥ì— ëŒ€í•´ ë¬¸ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤."
        
        elif any(keyword in query for keyword in ["ê²½ì œ", "ì„±ì¥", "gdp", "ê²½ê¸°"]):
            return "ğŸŒ ê²½ì œì „ë°˜ ê´€ë ¨ ì§ˆë¬¸: ë¯¸êµ­ì˜ ê²½ì œ ì„±ì¥, GDP, ê²½ê¸° ì „ë§ ë“± ê±°ì‹œê²½ì œ ìƒí™©ì— ëŒ€í•´ ë¬¸ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤."
        
        else:
            return "â“ ì¼ë°˜ì ì¸ ê²½ì œ ê´€ë ¨ ì§ˆë¬¸: í•œêµ­ì€í–‰ì´ ë¶„ì„í•œ ë¯¸êµ­ ê²½ì œ ê´€ë ¨ ì •ë³´ì— ëŒ€í•´ ë¬¸ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤."
    
    def _get_related_questions(self, query_embedding: np.ndarray, search_results: List[Dict]) -> List[str]:
        """ì—°ê´€ì„± ë†’ì€ ì§ˆë¬¸ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # ê°€ì¥ ì—°ê´€ì„± ë†’ì€ ë¬¸ì„œì˜ íŒŒì¼ëª… ì°¾ê¸°
            if not search_results:
                return []
            
            top_result = search_results[0]
            file_name = top_result["metadata"].get("file_name", "")
            
            # questions ì„¹ì…˜ ê²€ìƒ‰
            question_results = self.vector_store.search_questions(query_embedding, top_k=3)
            
            # ê°™ì€ íŒŒì¼ì˜ ì§ˆë¬¸ë“¤ ìš°ì„  ì„ íƒ
            related_questions = []
            
            # 1. ê°™ì€ íŒŒì¼ì˜ ì§ˆë¬¸ë“¤
            for result in question_results:
                if result["metadata"].get("file_name") == file_name:
                    questions_text = result["content"]
                    # ê°œë³„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¦¬
                    questions = self._parse_questions(questions_text)
                    related_questions.extend(questions[:3])  # ìµœëŒ€ 3ê°œ
            
            # 2. ë‹¤ë¥¸ íŒŒì¼ì˜ ì§ˆë¬¸ë“¤ (ê°™ì€ íŒŒì¼ ì§ˆë¬¸ì´ ë¶€ì¡±í•œ ê²½ìš°)
            if len(related_questions) < 3:
                for result in question_results:
                    if result["metadata"].get("file_name") != file_name:
                        questions_text = result["content"]
                        questions = self._parse_questions(questions_text)
                        remaining_slots = 3 - len(related_questions)
                        related_questions.extend(questions[:remaining_slots])
                        if len(related_questions) >= 3:
                            break
            
            return related_questions[:3]  # ìµœëŒ€ 3ê°œ ë°˜í™˜
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ì§ˆë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_questions(self, questions_text: str) -> List[str]:
        """ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ ì§ˆë¬¸ë“¤ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        questions = []
        lines = questions_text.split('\n')
        
        for line in lines:
            line = line.strip()
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ì°¾ê¸° (ì˜ˆ: "1. ì§ˆë¬¸ë‚´ìš©?")
            if line and (line[0].isdigit() or line.startswith('-')):
                # ìˆ«ìì™€ ì  ì œê±°
                question = line.split('.', 1)[-1].strip()
                if question and question.endswith('?'):
                    questions.append(question)
        
        return questions 