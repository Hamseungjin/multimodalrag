"""
Gemma-3-12b-it ê¸°ë°˜ ë¡œì»¬ í•©ì„± Q&A ìƒì„±ê¸°
API ì—†ì´ ì™„ì „ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” í”Œë¼ì´íœ  ì›Œí¬í”Œë¡œìš°ìš© ë°ì´í„° ìƒì„±
"""
import torch
import json
import time
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
from config import Config

class LocalSyntheticQAGenerator:
    """Gemma-3-12b-it ê¸°ë°˜ ë¡œì»¬ í•©ì„± Q&A ìƒì„±ê¸°"""
    
    def __init__(self, model_name: str = Config.LLM_MODEL):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.domain_prompts = {
            "economics": {
                "system": "ë‹¹ì‹ ì€ ê²½ì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì€í–‰ ë‰´ìŠ¤ì™€ ê²½ì œ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì‹¤ì ì´ê³  êµ¬ì²´ì ì¸ ê²½ì œ ê´€ë ¨ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": [
                    "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI)", "ê³ ìš©ë¥ ", "ì‹¤ì—…ë¥ ", "GDP ì„±ì¥ë¥ ", "ê¸°ì¤€ê¸ˆë¦¬", 
                    "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "í†µí™”ì •ì±…", "ì¬ì •ì •ì±…", "ê²½ì œì„±ì¥"
                ],
                "question_types": [
                    "ìµœê·¼ ë™í–¥ ì§ˆë¬¸", "ì›ì¸ ë¶„ì„ ì§ˆë¬¸", "ì „ë§ ì§ˆë¬¸", "ë¹„êµ ë¶„ì„ ì§ˆë¬¸", "ì •ì±… ì˜í–¥ ì§ˆë¬¸"
                ]
            },
            "finance": {
                "system": "ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸ˆìœµì‹œì¥, íˆ¬ì, ìì‚°ê´€ë¦¬ì— ëŒ€í•œ ì‹¤ìš©ì ì¸ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": [
                    "ì£¼ì‹íˆ¬ì", "ì±„ê¶Œíˆ¬ì", "í¬íŠ¸í´ë¦¬ì˜¤", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "ìì‚°ë°°ë¶„",
                    "ê¸ˆìœµìƒí’ˆ", "íˆ¬ìì „ëµ", "ì‹œì¥ë¶„ì„", "í™˜ìœ¨", "ê¸ˆë¦¬"
                ],
                "question_types": [
                    "íˆ¬ì ì¡°ì–¸ ì§ˆë¬¸", "ìœ„í—˜ë„ ë¶„ì„ ì§ˆë¬¸", "ìƒí’ˆ ë¹„êµ ì§ˆë¬¸", "ì‹œì¥ ì „ë§ ì§ˆë¬¸", "ì „ëµ ìˆ˜ë¦½ ì§ˆë¬¸"
                ]
            },
            "healthcare": {
                "system": "ë‹¹ì‹ ì€ ì˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜í•™ ì •ë³´, ê±´ê°•ê´€ë¦¬, ì§ˆë³‘ì— ëŒ€í•œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": [
                    "ì§ˆë³‘ ì˜ˆë°©", "ê±´ê°•ê²€ì§„", "ë§Œì„±ì§ˆí™˜", "ì‘ê¸‰ì²˜ì¹˜", "ì˜ì–‘ê´€ë¦¬",
                    "ìš´ë™ìš”ë²•", "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬", "ìˆ˜ë©´ê±´ê°•", "ì •ì‹ ê±´ê°•", "ë…¸ì¸ê±´ê°•"
                ],
                "question_types": [
                    "ì¦ìƒ ê´€ë ¨ ì§ˆë¬¸", "ì˜ˆë°©ë²• ì§ˆë¬¸", "ì¹˜ë£Œë²• ì§ˆë¬¸", "ìƒí™œìŠµê´€ ì§ˆë¬¸", "ê±´ê°•ê´€ë¦¬ ì§ˆë¬¸"
                ]
            },
            "legal": {
                "system": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë²•ë¥  ì •ë³´, ê¶Œë¦¬êµ¬ì œ, ë²•ì  ì ˆì°¨ì— ëŒ€í•œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": [
                    "ë¯¼ë²•", "í˜•ë²•", "ìƒë²•", "ë…¸ë™ë²•", "ë¶€ë™ì‚°ë²•",
                    "ê³„ì•½ë²•", "ì†Œì†¡ì ˆì°¨", "ë²•ë¥ ìƒë‹´", "ê¶Œë¦¬êµ¬ì œ", "ë²•ì ì±…ì„"
                ],
                "question_types": [
                    "ë²•ì  ê¶Œë¦¬ ì§ˆë¬¸", "ì ˆì°¨ ì•ˆë‚´ ì§ˆë¬¸", "ê³„ì•½ ê´€ë ¨ ì§ˆë¬¸", "ë¶„ìŸí•´ê²° ì§ˆë¬¸", "ë²•ì  ì˜ë¬´ ì§ˆë¬¸"
                ]
            }
        }
        
        self.load_model()
    
    def load_model(self):
        """Gemma ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"Gemma ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            if not torch.cuda.is_available():
                self.model = self.model.to(self.device)
            
            self.model.eval()
            logger.info("âœ… Gemma ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Gemma ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def generate_qa_pairs_by_domain(self, domain: str, count: int = 10, 
                                   context_data: List[str] = None) -> List[Dict[str, Any]]:
        """ë„ë©”ì¸ë³„ í•©ì„± Q&A ìŒ ìƒì„±"""
        if domain not in self.domain_prompts:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸: {domain}")
        
        logger.info(f"ğŸ“ {domain} ë„ë©”ì¸ Q&A {count}ê°œ ìƒì„± ì‹œì‘")
        
        domain_config = self.domain_prompts[domain]
        qa_pairs = []
        
        for i in range(count):
            try:
                # ëœë¤í•˜ê²Œ ì£¼ì œì™€ ì§ˆë¬¸ ìœ í˜• ì„ íƒ
                import random
                topic = random.choice(domain_config["topics"])
                question_type = random.choice(domain_config["question_types"])
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self._create_qa_generation_prompt(domain, topic, question_type, context_data)
                
                # Q&A ìƒì„±
                generated_text = self._generate_text(prompt)
                
                # ê²°ê³¼ íŒŒì‹±
                qa_pair = self._parse_qa_from_text(generated_text, domain, topic, question_type)
                
                if qa_pair:
                    qa_pairs.append(qa_pair)
                    logger.debug(f"ìƒì„± ì™„ë£Œ {i+1}/{count}: {qa_pair['question'][:50]}...")
                else:
                    logger.warning(f"Q&A íŒŒì‹± ì‹¤íŒ¨ {i+1}/{count}")
                
            except Exception as e:
                logger.error(f"Q&A ìƒì„± ì‹¤íŒ¨ {i+1}/{count}: {e}")
                continue
        
        logger.info(f"âœ… {domain} ë„ë©”ì¸ Q&A ìƒì„± ì™„ë£Œ: {len(qa_pairs)}ê°œ")
        return qa_pairs
    
    def _create_qa_generation_prompt(self, domain: str, topic: str, question_type: str, 
                                   context_data: List[str] = None) -> str:
        """ë„ë©”ì¸ë³„ Q&A ìƒì„± í”„ë¡¬í”„íŠ¸ ì‘ì„±"""
        domain_config = self.domain_prompts[domain]
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        prompt = f"""<start_of_turn>user
{domain_config['system']}

ì£¼ì œ: {topic}
ì§ˆë¬¸ ìœ í˜•: {question_type}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ 1ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

**ì§ˆë¬¸:** [êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸]
**ë‹µë³€:** [ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€, 2-3ë¬¸ì¥]

"""
        
        # ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if context_data and len(context_data) > 0:
            sample_context = context_data[0][:500]  # 500ìë¡œ ì œí•œ
            prompt += f"""
ì°¸ê³  ì •ë³´:
{sample_context}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
        
        prompt += "<end_of_turn>\n<start_of_turn>model\n"
        
        return prompt
    
    def _generate_text(self, prompt: str, max_length: int = 512) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            # í† í°í™”
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # ìƒì„± ì„¤ì •
            generation_config = {
                "max_new_tokens": max_length,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 50,
                "do_sample": True,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.1
            }
            
            # ìƒì„± ì‹¤í–‰
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **generation_config
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def _parse_qa_from_text(self, generated_text: str, domain: str, topic: str, 
                           question_type: str) -> Optional[Dict[str, Any]]:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ Q&A íŒŒì‹±"""
        try:
            # ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ì¶œ íŒ¨í„´
            question_patterns = [
                r"\*\*ì§ˆë¬¸:\*\*\s*(.+?)(?=\*\*ë‹µë³€:\*\*|\n\n|\Z)",
                r"ì§ˆë¬¸:\s*(.+?)(?=ë‹µë³€:|\n\n|\Z)",
                r"Q:\s*(.+?)(?=A:|\n\n|\Z)"
            ]
            
            answer_patterns = [
                r"\*\*ë‹µë³€:\*\*\s*(.+?)(?=\n\n|\Z)",
                r"ë‹µë³€:\s*(.+?)(?=\n\n|\Z)",
                r"A:\s*(.+?)(?=\n\n|\Z)"
            ]
            
            question = None
            answer = None
            
            # ì§ˆë¬¸ ì¶”ì¶œ
            for pattern in question_patterns:
                match = re.search(pattern, generated_text, re.DOTALL | re.IGNORECASE)
                if match:
                    question = match.group(1).strip()
                    break
            
            # ë‹µë³€ ì¶”ì¶œ
            for pattern in answer_patterns:
                match = re.search(pattern, generated_text, re.DOTALL | re.IGNORECASE)
                if match:
                    answer = match.group(1).strip()
                    break
            
            # í’ˆì§ˆ ê²€ì¦
            if question and answer and len(question) > 10 and len(answer) > 20:
                qa_pair = {
                    "question": question,
                    "answer": answer,
                    "domain": domain,
                    "topic": topic,
                    "question_type": question_type,
                    "generated_at": time.time(),
                    "quality_score": self._calculate_qa_quality(question, answer),
                    "source": "local_gemma"
                }
                
                # í’ˆì§ˆ ì„ê³„ê°’ í™•ì¸
                if qa_pair["quality_score"] >= Config.QUALITY_THRESHOLD:
                    return qa_pair
            
            return None
            
        except Exception as e:
            logger.error(f"Q&A íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_qa_quality(self, question: str, answer: str) -> float:
        """Q&A í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_score = 0.0
        
        # 1. ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (30%)
        question_len = len(question)
        answer_len = len(answer)
        
        if 15 <= question_len <= 100:
            quality_score += 0.15
        if 30 <= answer_len <= 300:
            quality_score += 0.15
        
        # 2. êµ¬ì¡° ê¸°ë°˜ ì ìˆ˜ (25%)
        if question.endswith('?') or 'ì–´ë–»' in question or 'ë¬´ì—‡' in question:
            quality_score += 0.1
        if '. ' in answer or 'ë‹¤.' in answer:  # ë¬¸ì¥ êµ¬ì¡°
            quality_score += 0.15
        
        # 3. ì»¨í…ì¸  ê¸°ë°˜ ì ìˆ˜ (25%)
        question_words = len(question.split())
        answer_words = len(answer.split())
        
        if 3 <= question_words <= 20:
            quality_score += 0.1
        if 10 <= answer_words <= 50:
            quality_score += 0.15
        
        # 4. ë°˜ë³µ/ë…¸ì´ì¦ˆ ê²€ì‚¬ (20%)
        if not self._has_repetition(question) and not self._has_repetition(answer):
            quality_score += 0.2
        
        return min(1.0, quality_score)
    
    def _has_repetition(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ ë‚´ ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬"""
        words = text.split()
        if len(words) < 4:
            return False
        
        # ì—°ì† ë‹¨ì–´ ë°˜ë³µ ê²€ì‚¬
        for i in range(len(words) - 2):
            if words[i] == words[i + 1] == words[i + 2]:
                return True
        
        return False
    
    def batch_generate_qa_pairs(self, domains: List[str], count_per_domain: int = 20,
                               save_path: Optional[Path] = None) -> Dict[str, List[Dict[str, Any]]]:
        """ì—¬ëŸ¬ ë„ë©”ì¸ì— ëŒ€í•´ ë°°ì¹˜ë¡œ Q&A ìƒì„±"""
        all_qa_pairs = {}
        
        for domain in domains:
            logger.info(f"ğŸ¯ {domain} ë„ë©”ì¸ ë°°ì¹˜ ìƒì„± ì‹œì‘")
            
            qa_pairs = self.generate_qa_pairs_by_domain(domain, count_per_domain)
            all_qa_pairs[domain] = qa_pairs
            
            logger.info(f"âœ… {domain}: {len(qa_pairs)}ê°œ ìƒì„± ì™„ë£Œ")
        
        # ê²°ê³¼ ì €ì¥
        if save_path:
            self._save_qa_pairs(all_qa_pairs, save_path)
        
        # í†µê³„ ì¶œë ¥
        total_count = sum(len(pairs) for pairs in all_qa_pairs.values())
        avg_quality = sum(
            sum(pair["quality_score"] for pair in pairs) / len(pairs)
            for pairs in all_qa_pairs.values() if pairs
        ) / len(all_qa_pairs)
        
        logger.info(f"ğŸŠ ë°°ì¹˜ ìƒì„± ì™„ë£Œ: ì´ {total_count}ê°œ, í‰ê·  í’ˆì§ˆ {avg_quality:.3f}")
        
        return all_qa_pairs
    
    def _save_qa_pairs(self, qa_pairs_dict: Dict[str, List[Dict[str, Any]]], save_path: Path):
        """Q&A ìŒì„ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(qa_pairs_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ Q&A ë°ì´í„° ì €ì¥: {save_path}")
            
        except Exception as e:
            logger.error(f"Q&A ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def cleanup_model(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("ğŸ§¹ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜ë“¤
def generate_economics_qa_sample():
    """ê²½ì œ ë„ë©”ì¸ Q&A ìƒì„± ì˜ˆì‹œ"""
    generator = LocalSyntheticQAGenerator()
    
    try:
        qa_pairs = generator.generate_qa_pairs_by_domain("economics", count=5)
        
        print("ğŸ“Š ìƒì„±ëœ ê²½ì œ Q&A ìƒ˜í”Œ:")
        print("=" * 50)
        
        for i, qa in enumerate(qa_pairs, 1):
            print(f"\n{i}. ì£¼ì œ: {qa['topic']} | ìœ í˜•: {qa['question_type']}")
            print(f"ì§ˆë¬¸: {qa['question']}")
            print(f"ë‹µë³€: {qa['answer']}")
            print(f"í’ˆì§ˆ: {qa['quality_score']:.3f}")
            print("-" * 40)
        
        return qa_pairs
        
    finally:
        generator.cleanup_model()

def generate_multi_domain_qa():
    """ë‹¤ì¤‘ ë„ë©”ì¸ Q&A ìƒì„± ì˜ˆì‹œ"""
    generator = LocalSyntheticQAGenerator()
    
    try:
        domains = ["economics", "healthcare", "legal"]
        qa_results = generator.batch_generate_qa_pairs(
            domains, 
            count_per_domain=3,
            save_path=Path("generated_qa_multi_domain.json")
        )
        
        # ë„ë©”ì¸ë³„ ê²°ê³¼ ì¶œë ¥
        for domain, qa_pairs in qa_results.items():
            print(f"\nğŸ¯ {domain.upper()} ë„ë©”ì¸ ({len(qa_pairs)}ê°œ):")
            for qa in qa_pairs[:2]:  # ê° ë„ë©”ì¸ë‹¹ 2ê°œì”©ë§Œ ì¶œë ¥
                print(f"Q: {qa['question'][:60]}...")
                print(f"A: {qa['answer'][:80]}...")
                print()
        
        return qa_results
        
    finally:
        generator.cleanup_model()

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logger.add("synthetic_qa_generation.log", rotation="10 MB")
    
    print("ğŸš€ Gemma-3-12b-it ê¸°ë°˜ ë¡œì»¬ í•©ì„± Q&A ìƒì„±ê¸°")
    print("1. ê²½ì œ ë„ë©”ì¸ ìƒ˜í”Œ ìƒì„±")
    # generate_economics_qa_sample()
    
    print("\n2. ë‹¤ì¤‘ ë„ë©”ì¸ ë°°ì¹˜ ìƒì„±")
    # generate_multi_domain_qa()
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì‹¤ì œ ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.") 