"""
ì ì‘í˜• RAG ì»´í¬ë„ŒíŠ¸ë“¤ - í”Œë¼ì´íœ  ì›Œí¬í”Œë¡œìš° ì§€ì›
ê°œì„ ì‚¬í•­:
1. ì ì‘í˜• í‚¤ì›Œë“œ ì¶”ì¶œ + ìºì‹±
2. ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° (ë²¡í„°/ë¦¬ë­í‚¹)
3. ìŠ¤ë§ˆíŠ¸ ì„ê³„ê°’ ê³„ì‚°
4. í”Œë¼ì´íœ  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
"""
import os
import json
import hashlib
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from loguru import logger
import re
import random
import uuid
from kiwipiepy import Kiwi
from common_constants import (
    DOMAIN_SEED_KEYWORDS, SUPPORTED_DOMAINS, MAX_CACHE_SIZE, 
    CACHE_EXPIRY_SECONDS, ECONOMIC_TERMS, OPTIMAL_WEIGHTS
)

# ì „ì—­ ìºì‹œ ë³€ìˆ˜ë“¤
KEYWORD_CACHE = {}
WEIGHT_CACHE = {}
THRESHOLD_CACHE = {}
DOMAIN_DETECTION_CACHE = {}  # LLM ë„ë©”ì¸ ê°ì§€ ê²°ê³¼ ìºì‹œ

class DomainDetector:
    """ë„ë©”ì¸ ìë™ ê°ì§€ í´ë˜ìŠ¤ - í‚¤ì›Œë“œ ë§¤ì¹­ + LLM ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ"""
    
    def __init__(self, llm_model=None):
        self.llm_model = llm_model
        self.use_llm_detection = llm_model is not None
        
        # LLM ë„ë©”ì¸ ê°ì§€ìš© í”„ë¡¬í”„íŠ¸
        self.domain_detection_prompt = """ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë„ë©”ì¸ì„ ì •í™•íˆ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ë„ë©”ì¸ ì˜µì…˜:
- economics: ê²½ì œ, ê¸ˆìœµì •ì±…, ë¬¼ê°€, ê³ ìš©, GDP ë“± ê±°ì‹œê²½ì œ
- finance: íˆ¬ì, ì£¼ì‹, ì±„ê¶Œ, ìì‚°ê´€ë¦¬, ê¸ˆìœµìƒí’ˆ ë“±
- healthcare: ì˜ë£Œ, ê±´ê°•, ì§ˆë³‘, ì¹˜ë£Œ, ë³‘ì› ë“±
- technology: ITê¸°ìˆ , ì†Œí”„íŠ¸ì›¨ì–´, AI, ì»´í“¨í„° ë“±
- legal: ë²•ë¥ , ê³„ì•½, ì†Œì†¡, ê¶Œë¦¬ì˜ë¬´ ë“±
- general: ìœ„ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ë‚´ìš©

ì‘ë‹µ í˜•ì‹: ë„ë©”ì¸ëª…ë§Œ ì •í™•íˆ ë°˜í™˜ (ì˜ˆ: economics)"""
    
    def detect_domain(self, text: str, use_llm: bool = True) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ë„ë©”ì¸ì„ ìë™ ê°ì§€ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)"""
        
        # 1. ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in DOMAIN_DETECTION_CACHE:
            cached_data = DOMAIN_DETECTION_CACHE[cache_key]
            if time.time() - cached_data['timestamp'] < 7200:  # 2ì‹œê°„ ìºì‹œ
                logger.debug(f"ë„ë©”ì¸ ìºì‹œ íˆíŠ¸: {cached_data['domain']}")
                return cached_data['domain']
        
        # 2. LLM ê¸°ë°˜ ê°ì§€ ì‹œë„ (ìš°ì„ ìˆœìœ„)
        if self.use_llm_detection and use_llm:
            try:
                llm_domain = self._detect_domain_with_llm(text)
                if llm_domain and llm_domain in SUPPORTED_DOMAINS:
                    self._save_domain_to_cache(cache_key, llm_domain, method="llm")
                    return llm_domain
                else:
                    logger.warning(f"LLM ë„ë©”ì¸ ê°ì§€ ê²°ê³¼ ë¬´íš¨: {llm_domain}")
            except Exception as e:
                logger.warning(f"LLM ë„ë©”ì¸ ê°ì§€ ì‹¤íŒ¨, í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ fallback: {e}")
        
        # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€ (fallback)
        keyword_domain = self._detect_domain_with_keywords(text)
        self._save_domain_to_cache(cache_key, keyword_domain, method="keyword")
        
        return keyword_domain
    
    def _detect_domain_with_llm(self, text: str) -> Optional[str]:
        """LLMì„ ì‚¬ìš©í•œ ë„ë©”ì¸ ê°ì§€"""
        if not self.llm_model:
            return None
        
        try:
            prompt = self.domain_detection_prompt.format(text=text[:500])  # ê¸¸ì´ ì œí•œ
            
            # LLM ì¶”ë¡  ì‹¤í–‰
            response = self._generate_llm_response(prompt)
            
            # ì‘ë‹µì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
            domain = self._parse_domain_response(response)
            
            logger.debug(f"LLM ë„ë©”ì¸ ê°ì§€: '{text[:50]}...' â†’ {domain}")
            return domain
            
        except Exception as e:
            logger.error(f"LLM ë„ë©”ì¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _generate_llm_response(self, prompt: str) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        try:
            # Gemma ëª¨ë¸ ì‚¬ìš© (ì´ë¯¸ ë¡œë“œëœ ê²½ìš°)
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            if hasattr(self.llm_model, 'generate'):
                # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©
                tokenizer = getattr(self.llm_model, 'tokenizer', None)
                if not tokenizer:
                    # í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ë¡œë“œ
                    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-12b-it")
                
                inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
                
                with torch.no_grad():
                    outputs = self.llm_model.generate(
                        inputs.input_ids,
                        max_new_tokens=50,
                        temperature=0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í™•ë³´
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id
                    )
                
                response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                return response.strip()
            else:
                # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ fallback
                logger.warning("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ, í‚¤ì›Œë“œ ë°©ì‹ ì‚¬ìš©")
                return ""
                
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def _parse_domain_response(self, response: str) -> Optional[str]:
        """LLM ì‘ë‹µì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        response = response.lower().strip()
        
        # ì§ì ‘ ë„ë©”ì¸ëª…ì´ í¬í•¨ëœ ê²½ìš°
        for domain in SUPPORTED_DOMAINS:
            if domain in response:
                return domain
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤í•‘
        korean_domain_map = {
            "ê²½ì œ": "economics",
            "ê¸ˆìœµ": "finance", 
            "ì˜ë£Œ": "healthcare",
            "ê±´ê°•": "healthcare",
            "ê¸°ìˆ ": "technology",
            "ë²•ë¥ ": "legal",
            "ì¼ë°˜": "general"
        }
        
        for korean, english in korean_domain_map.items():
            if korean in response:
                return english
        
        return None
    
    def _detect_domain_with_keywords(self, text: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë„ë©”ì¸ ê°ì§€ (ë‹¨ìˆœ fallback)"""
        text_lower = text.lower()
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì²´í¬
        if any(word in text_lower for word in ["ê²½ì œ", "ë¬¼ê°€", "ì¸í”Œë ˆì´ì…˜", "GDP", "ê³ ìš©", "ì‹¤ì—…"]):
            return "economics"
        elif any(word in text_lower for word in ["íˆ¬ì", "ì£¼ì‹", "ì±„ê¶Œ", "ìì‚°", "ìˆ˜ìµë¥ "]):
            return "finance" 
        elif any(word in text_lower for word in ["ì˜ë£Œ", "ê±´ê°•", "ì§ˆë³‘", "ì¹˜ë£Œ", "ë³‘ì›"]):
            return "healthcare"
        elif any(word in text_lower for word in ["ê¸°ìˆ ", "ì†Œí”„íŠ¸ì›¨ì–´", "AI", "ì»´í“¨í„°"]):
            return "technology"
        elif any(word in text_lower for word in ["ë²•ë¥ ", "ë²•", "íŒê²°", "ì†Œì†¡", "ë³€í˜¸ì‚¬"]):
            return "legal"
        else:
            return "general"
    
    def _save_domain_to_cache(self, cache_key: str, domain: str, method: str):
        """ë„ë©”ì¸ ê°ì§€ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        global DOMAIN_DETECTION_CACHE
        
        # ìºì‹œ í¬ê¸° ê´€ë¦¬
        if len(DOMAIN_DETECTION_CACHE) >= MAX_CACHE_SIZE:
            oldest_items = sorted(DOMAIN_DETECTION_CACHE.items(), 
                                key=lambda x: x[1]['timestamp'])
            for i in range(len(oldest_items) // 4):
                del DOMAIN_DETECTION_CACHE[oldest_items[i][0]]
        
        DOMAIN_DETECTION_CACHE[cache_key] = {
            'domain': domain,
            'method': method,
            'timestamp': time.time()
        }
        
        logger.debug(f"ë„ë©”ì¸ ìºì‹œ ì €ì¥: {domain} (method: {method})")

class AdaptiveKeywordExtractor:
    """ì ì‘í˜• í‚¤ì›Œë“œ ì¶”ì¶œê¸° - Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜"""
    
    def __init__(self, embedding_model, llm_model=None):
        self.embedding_model = embedding_model
        self.domain_detector = DomainDetector(llm_model)
        self.tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 3))
        
        # Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        try:
            self.kiwi = Kiwi()
            self.use_kiwi = True
            logger.info("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Kiwi ë¡œë“œ ì‹¤íŒ¨, ì •ê·œì‹ ë°©ì‹ ì‚¬ìš©: {e}")
            self.kiwi = None
            self.use_kiwi = False
        
        # ë„ë©”ì¸ë³„ ì‹œë“œ í‚¤ì›Œë“œ (ê³µí†µ ìƒìˆ˜ ì‚¬ìš©)
        self.domain_seed_keywords = DOMAIN_SEED_KEYWORDS
        
        self.domain_embeddings_cache = {}
        self._initialize_domain_embeddings()
    
    def _initialize_domain_embeddings(self):
        """ë„ë©”ì¸ë³„ ì‹œë“œ í‚¤ì›Œë“œ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°"""
        for domain, categories in self.domain_seed_keywords.items():
            if domain not in self.domain_embeddings_cache:
                self.domain_embeddings_cache[domain] = {}
            
            for category, keywords in categories.items():
                try:
                    embeddings = self.embedding_model.encode(keywords)
                    self.domain_embeddings_cache[domain][category] = embeddings
                except Exception as e:
                    logger.warning(f"ë„ë©”ì¸ {domain} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
    
    def extract_keywords_adaptive(self, query: str, context_texts: List[str] = None, 
                                 domain: str = None) -> Dict[str, Any]:
        """ì ì‘í˜• í‚¤ì›Œë“œ ì¶”ì¶œ"""
        detected_domain = domain or self.domain_detector.detect_domain(query)
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(f"{detected_domain}:{query}".encode()).hexdigest()
        if cache_key in KEYWORD_CACHE:
            cached_data = KEYWORD_CACHE[cache_key]
            if time.time() - cached_data['timestamp'] < 3600:  # 1ì‹œê°„ ìºì‹œ
                return cached_data['keywords']
        
        # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ì¶”ì¶œ
        result = self._extract_fresh_keywords(query, context_texts, detected_domain)
        
        # ìºì‹œì— ì €ì¥
        if len(KEYWORD_CACHE) >= MAX_CACHE_SIZE:
            # ì˜¤ë˜ëœ ìºì‹œ ì œê±°
            oldest_items = sorted(KEYWORD_CACHE.items(), 
                                key=lambda x: x[1]['timestamp'])
            for i in range(len(oldest_items) // 4):
                del KEYWORD_CACHE[oldest_items[i][0]]
        
        KEYWORD_CACHE[cache_key] = {
            'keywords': result,
            'timestamp': time.time()
        }
        
        return result
    
    def _extract_fresh_keywords(self, query: str, context_texts: List[str], 
                               domain: str) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        result = {
            "domain": domain,
            "basic_keywords": [],
            "semantic_keywords": [],
            "tfidf_keywords": [],
            "final_keywords": []
        }
        
        try:
            # 1. ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
            basic_keywords = self._extract_basic_keywords(query)
            result["basic_keywords"] = basic_keywords
            
            # 2. ë„ë©”ì¸ë³„ ì˜ë¯¸ì  í™•ì¥
            semantic_keywords = self._expand_keywords_by_domain(query, basic_keywords, domain)
            result["semantic_keywords"] = semantic_keywords
            
            # 3. TF-IDF í‚¤ì›Œë“œ
            if context_texts:
                tfidf_keywords = self._extract_tfidf_keywords(query, context_texts)
                result["tfidf_keywords"] = tfidf_keywords
            
            # 4. ìµœì¢… í†µí•©
            final_keywords = self._integrate_keywords_adaptive(
                basic_keywords, semantic_keywords, result["tfidf_keywords"], domain
            )
            result["final_keywords"] = final_keywords
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            result["final_keywords"] = self._extract_basic_keywords(query)
        
        return result
    
    def _extract_basic_keywords(self, query: str) -> List[str]:
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì •ë°€ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        if self.use_kiwi and self.kiwi:
            try:
                # Kiwi í˜•íƒœì†Œ ë¶„ì„
                tokens = self.kiwi.analyze(query)
                
                for token_info in tokens[0][0]:  # ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
                    morph = token_info[0]  # í˜•íƒœì†Œ
                    pos = token_info[1]    # í’ˆì‚¬
                    
                    # ëª…ì‚¬, ê³ ìœ ëª…ì‚¬, ì˜ì–´, í•œìì–´ ì¶”ì¶œ
                    if pos in ['NNG', 'NNP', 'SL', 'SH'] and len(morph) >= 2:
                        keywords.append(morph)
                    
                    # ë³µí•©ëª…ì‚¬ ì²˜ë¦¬ (ì—°ì†ëœ ëª…ì‚¬ë“¤ ê²°í•©)
                    elif pos in ['NNG', 'NNP'] and len(morph) >= 1:
                        keywords.append(morph)
                
                # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
                keywords = self._filter_quality_keywords(list(set(keywords)))
                
                logger.debug(f"Kiwi í‚¤ì›Œë“œ ì¶”ì¶œ: {keywords}")
                
            except Exception as e:
                logger.warning(f"Kiwi ë¶„ì„ ì‹¤íŒ¨, ì •ê·œì‹ ë°©ì‹ìœ¼ë¡œ fallback: {e}")
                keywords = self._extract_keywords_regex(query)
        else:
            # Kiwi ì‚¬ìš© ë¶ˆê°€ì‹œ ì •ê·œì‹ ë°©ì‹
            keywords = self._extract_keywords_regex(query)
        
        return keywords
    
    def _extract_keywords_regex(self, query: str) -> List[str]:
        """ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (fallback)"""
        keywords = []
        
        # í•œê¸€ í‚¤ì›Œë“œ (2ê¸€ì ì´ìƒ)
        korean_pattern = r'[ê°€-í£]{2,}'
        korean_words = re.findall(korean_pattern, query)
        keywords.extend(korean_words)
        
        # ì˜ì–´ ëŒ€ë¬¸ì ì•½ì–´
        english_pattern = r'[A-Z]{2,}'
        english_words = re.findall(english_pattern, query)
        keywords.extend(english_words)
        
        # ìˆ«ì+í•œê¸€ ì¡°í•© (ì˜ˆ: "3ë¶„ê¸°", "2023ë…„")
        number_korean_pattern = r'\d+[ê°€-í£]+'
        number_korean = re.findall(number_korean_pattern, query)
        keywords.extend(number_korean)
        
        return list(set(keywords))
    
    def _filter_quality_keywords(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ í’ˆì§ˆ í•„í„°ë§"""
        filtered = []
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ë•Œë¬¸', 'ìœ„í•´'}
        
        for keyword in keywords:
            # ê¸¸ì´ ì¡°ê±´ (2-15ì)
            if not (2 <= len(keyword) <= 15):
                continue
            
            # ë¶ˆìš©ì–´ ì œê±°
            if keyword in stopwords:
                continue
                
            # ë‹¨ìˆœ ë°˜ë³µ ë¬¸ì ì œê±° (ì˜ˆ: "ã…‹ã…‹ã…‹", "...")
            if len(set(keyword)) <= 2 and len(keyword) > 3:
                continue
            
            # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì œì™¸ (ë‹¨, ë…„ë„ëŠ” í¬í•¨)
            if keyword.isdigit() and not (1900 <= int(keyword) <= 2100):
                continue
            
            filtered.append(keyword)
        
        return filtered
    
    def _expand_keywords_by_domain(self, query: str, base_keywords: List[str], 
                                  domain: str) -> List[str]:
        """ë„ë©”ì¸ë³„ ì˜ë¯¸ì  í‚¤ì›Œë“œ í™•ì¥"""
        if not base_keywords or domain not in self.domain_embeddings_cache:
            return []
        
        expanded_keywords = []
        
        try:
            query_embedding = self.embedding_model.encode([query])
            domain_embeddings = self.domain_embeddings_cache[domain]
            
            for category, seed_embeddings in domain_embeddings.items():
                similarities = cosine_similarity(query_embedding, seed_embeddings)[0]
                threshold = 0.5 if domain == "economics" else 0.55
                
                for i, similarity in enumerate(similarities):
                    if similarity > threshold:
                        seed_keywords = self.domain_seed_keywords[domain][category]
                        keyword = seed_keywords[i]
                        if keyword not in base_keywords and keyword not in expanded_keywords:
                            expanded_keywords.append(keyword)
            
        except Exception as e:
            logger.warning(f"ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ í™•ì¥ ì‹¤íŒ¨: {e}")
        
        return expanded_keywords[:8]
    
    def _extract_tfidf_keywords(self, query: str, context_texts: List[str]) -> List[str]:
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not context_texts:
            return []
        
        try:
            all_texts = [query] + context_texts
            tfidf_matrix = self.tfidf.fit_transform(all_texts)
            feature_names = self.tfidf.get_feature_names_out()
            query_tfidf = tfidf_matrix[0].toarray()[0]
            
            word_scores = [(feature_names[i], score) 
                          for i, score in enumerate(query_tfidf) if score > 0]
            word_scores.sort(key=lambda x: x[1], reverse=True)
            
            tfidf_keywords = []
            for word, score in word_scores[:15]:
                if any(ord('ê°€') <= ord(char) <= ord('í£') for char in word) or word.isupper():
                    tfidf_keywords.append(word)
            
            return tfidf_keywords[:6]
            
        except Exception as e:
            logger.warning(f"TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _integrate_keywords_adaptive(self, basic_keywords: List[str], semantic_keywords: List[str], 
                                   tfidf_keywords: List[str], domain: str) -> List[str]:
        """ë„ë©”ì¸ë³„ ì ì‘í˜• í‚¤ì›Œë“œ í†µí•©"""
        keyword_scores = {}
        
        # ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜
        if domain == "economics":
            basic_weight, semantic_weight, tfidf_weight = 1.0, 0.9, 0.7
        elif domain == "finance":
            basic_weight, semantic_weight, tfidf_weight = 1.0, 0.8, 0.8
        else:
            basic_weight, semantic_weight, tfidf_weight = 1.0, 0.6, 0.5
        
        for keyword in basic_keywords:
            keyword_scores[keyword] = keyword_scores.get(keyword, 0) + basic_weight
        
        for keyword in semantic_keywords:
            keyword_scores[keyword] = keyword_scores.get(keyword, 0) + semantic_weight
        
        for keyword in tfidf_keywords:
            keyword_scores[keyword] = keyword_scores.get(keyword, 0) + tfidf_weight
        
        sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
        min_score = 0.5 if domain == "economics" else 0.4
        
        return [keyword for keyword, score in sorted_keywords if score >= min_score]

class AdaptiveWeightCalculator:
    """ì ì‘í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°ê¸° - ë²¡í„°/ë¦¬ë­í‚¹ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •"""
    
    def __init__(self):
        self.optimal_weights = OPTIMAL_WEIGHTS
    
    def calculate_adaptive_weights(self, query: str, search_results: List[Dict], 
                                 domain: str = "general") -> Tuple[float, float]:
        """ì ì‘í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        # ìºì‹œ í™•ì¸
        cache_key = f"{domain}:{len(search_results)}:{len(query)}"
        if cache_key in WEIGHT_CACHE:
            cached_weights = WEIGHT_CACHE[cache_key]
            if time.time() - cached_weights['timestamp'] < 1800:  # 30ë¶„ ìºì‹œ
                return cached_weights['vector'], cached_weights['rerank']
        
        # ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„
        query_features = self._analyze_query_features(query, search_results)
        
        # ë„ë©”ì¸ë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜
        base_weights = self.optimal_weights.get(domain, self.optimal_weights["general"])
        vector_weight = base_weights["vector"]
        rerank_weight = base_weights["rerank"]
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¡°ì •
        if query_features["is_factual"]:
            vector_weight += 0.1
            rerank_weight -= 0.1
        elif query_features["is_analytical"]:
            vector_weight -= 0.05
            rerank_weight += 0.05
        
        # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì— ë”°ë¥¸ ì¡°ì •
        if query_features["result_quality"] > 0.8:
            vector_weight += 0.05
        elif query_features["result_quality"] < 0.5:
            rerank_weight += 0.1
            vector_weight -= 0.1
        
        # ì •ê·œí™”
        total = vector_weight + rerank_weight
        vector_weight /= total
        rerank_weight /= total
        
        # ìºì‹œì— ì €ì¥
        WEIGHT_CACHE[cache_key] = {
            'vector': vector_weight,
            'rerank': rerank_weight,
            'timestamp': time.time()
        }
        
        logger.debug(f"ì ì‘í˜• ê°€ì¤‘ì¹˜: ë²¡í„° {vector_weight:.3f}, ë¦¬ë­í‚¹ {rerank_weight:.3f}")
        
        return vector_weight, rerank_weight
    
    def _analyze_query_features(self, query: str, search_results: List[Dict]) -> Dict[str, Any]:
        """ì¿¼ë¦¬ì™€ ê²€ìƒ‰ ê²°ê³¼ì˜ íŠ¹ì„± ë¶„ì„"""
        features = {}
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        factual_indicators = ["ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””", "ëˆ„êµ¬", "ì–¼ë§ˆ", "ëª‡"]
        analytical_indicators = ["ì™œ", "ì–´ë–»ê²Œ", "ë¶„ì„", "ë¹„êµ", "í‰ê°€", "ì „ë§"]
        
        features["is_factual"] = any(indicator in query for indicator in factual_indicators)
        features["is_analytical"] = any(indicator in query for indicator in analytical_indicators)
        
        # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
        if search_results:
            scores = [result.get("score", 0) for result in search_results[:5]]
            features["result_quality"] = np.mean(scores)
            features["score_variance"] = np.var(scores)
        else:
            features["result_quality"] = 0.0
            features["score_variance"] = 0.0
        
        return features

class SmartThresholdCalculator:
    """ìŠ¤ë§ˆíŠ¸ ì„ê³„ê°’ ê³„ì‚°ê¸°"""
    
    def calculate_smart_threshold(self, query: str, search_results: List[Dict], 
                                domain: str = "general") -> float:
        """ML ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì„ê³„ê°’ ê³„ì‚°"""
        from config import Config
        base_threshold = Config.SIMILARITY_THRESHOLD
        
        # íŠ¹ì„± ë²¡í„° ìƒì„±
        features = self._extract_threshold_features(query, search_results, domain)
        
        # ê·œì¹™ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°
        smart_threshold = self._rule_based_threshold(features, base_threshold)
        
        return smart_threshold
    
    def _extract_threshold_features(self, query: str, search_results: List[Dict], 
                                  domain: str) -> Dict[str, float]:
        """ì„ê³„ê°’ ê²°ì •ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        # ì¿¼ë¦¬ íŠ¹ì„±
        features["query_length"] = len(query) / 100.0
        features["query_complexity"] = len(query.split()) / 20.0
        features["has_economic_terms"] = self._count_economic_terms(query) / 10.0
        
        # ë„ë©”ì¸ íŠ¹ì„±
        features["domain_economics"] = 1.0 if domain == "economics" else 0.0
        features["domain_finance"] = 1.0 if domain == "finance" else 0.0
        
        # ê²€ìƒ‰ ê²°ê³¼ íŠ¹ì„±
        if search_results:
            scores = [result.get("score", 0) for result in search_results[:10]]
            features["max_score"] = max(scores)
            features["mean_score"] = np.mean(scores)
            features["score_std"] = np.std(scores)
        else:
            features["max_score"] = 0.0
            features["mean_score"] = 0.0
            features["score_std"] = 0.0
        
        return features
    
    def _rule_based_threshold(self, features: Dict[str, float], base_threshold: float) -> float:
        """ê·œì¹™ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°"""
        threshold = base_threshold
        
        # ë„ë©”ì¸ë³„ ì¡°ì •
        if features["domain_economics"] > 0.5:
            threshold -= 0.05
        
        # ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
        if features["query_complexity"] > 0.5:
            threshold -= 0.03
        elif features["query_complexity"] < 0.2:
            threshold += 0.03
        
        # ê²½ì œ ìš©ì–´ ë°€ë„ì— ë”°ë¥¸ ì¡°ì •
        if features["has_economic_terms"] > 0.3:
            threshold -= 0.04
        
        # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆì— ë”°ë¥¸ ì¡°ì •
        if features["score_std"] > 0.15:
            threshold += 0.02
        
        # ì„ê³„ê°’ ë²”ìœ„ ì œí•œ
        threshold = max(0.2, min(0.7, threshold))
        
        return threshold
    
    def _count_economic_terms(self, query: str) -> int:
        """ê²½ì œ ìš©ì–´ ê°œìˆ˜ ê³„ì‚°"""
        query_lower = query.lower()
        return sum(1 for term in ECONOMIC_TERMS if term in query_lower)

class LocalGemmaQAGenerator:
    """Gemma-3-12b-it ê¸°ë°˜ ë¡œì»¬ í•©ì„± Q&A ìƒì„±ê¸°"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.domain_prompts = {
            "economics": {
                "system": "ë‹¹ì‹ ì€ ê²½ì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²½ì œ ì§€í‘œì™€ ì‹œì¥ ë™í–¥ì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": ["ì†Œë¹„ìë¬¼ê°€", "ê³ ìš©ë¥ ", "GDP", "ê¸ˆë¦¬", "ì¸í”Œë ˆì´ì…˜"]
            },
            "healthcare": {
                "system": "ë‹¹ì‹ ì€ ì˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê±´ê°•ê³¼ ì˜ë£Œì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": ["ì§ˆë³‘ì˜ˆë°©", "ê±´ê°•ê²€ì§„", "ì˜ì–‘ê´€ë¦¬", "ìš´ë™ìš”ë²•", "ìŠ¤íŠ¸ë ˆìŠ¤ê´€ë¦¬"]
            },
            "legal": {
                "system": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë²•ë¥ ê³¼ ê¶Œë¦¬ì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.",
                "topics": ["ê³„ì•½ë²•", "ë¯¼ë²•", "í˜•ë²•", "ë…¸ë™ë²•", "ë¶€ë™ì‚°ë²•"]
            }
        }
    
    def load_model_if_needed(self):
        """í•„ìš”ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if self.model is None:
            try:
                from transformers import AutoTokenizer, AutoModelForCausalLM
                import torch
                
                logger.info("Gemma ëª¨ë¸ ë¡œë”© ì¤‘...")
                model_name = "google/gemma-3-12b-it"
                
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                logger.info("âœ… Gemma ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"Gemma ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
    
    def generate_qa_pairs(self, domain: str, count: int = 5) -> List[Dict[str, str]]:
        """ë„ë©”ì¸ë³„ Q&A ìŒ ìƒì„±"""
        if domain not in self.domain_prompts:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„ë©”ì¸: {domain}, ê¸°ë³¸ ê²½ì œ ë„ë©”ì¸ ì‚¬ìš©")
            domain = "economics"
        
        self.load_model_if_needed()
        
        qa_pairs = []
        domain_config = self.domain_prompts[domain]
        
        for i in range(count):
            try:
                # ì£¼ì œ ëœë¤ ì„ íƒ
                import random
                topic = random.choice(domain_config["topics"])
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = f"""<start_of_turn>user
{domain_config['system']}

ì£¼ì œ: {topic}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ 1ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: [êµ¬ì²´ì ì¸ ì§ˆë¬¸]
ë‹µë³€: [ë„ì›€ì´ ë˜ëŠ” ë‹µë³€]
<end_of_turn>
<start_of_turn>model
"""
                
                # í…ìŠ¤íŠ¸ ìƒì„±
                generated_text = self._generate_text(prompt)
                
                # Q&A íŒŒì‹±
                qa_pair = self._parse_qa(generated_text, domain, topic)
                if qa_pair:
                    qa_pairs.append(qa_pair)
                    logger.debug(f"Q&A ìƒì„± {i+1}/{count}: {qa_pair['question'][:30]}...")
                
            except Exception as e:
                logger.error(f"Q&A ìƒì„± ì‹¤íŒ¨ {i+1}: {e}")
                continue
        
        logger.info(f"âœ… {domain} ë„ë©”ì¸ Q&A {len(qa_pairs)}ê°œ ìƒì„± ì™„ë£Œ")
        return qa_pairs
    
    def _generate_text(self, prompt: str) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=256,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            return generated_text.strip()
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def _parse_qa(self, text: str, domain: str, topic: str) -> Optional[Dict[str, str]]:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ Q&A íŒŒì‹±"""
        try:
            import re
            
            # ì§ˆë¬¸ ì¶”ì¶œ
            question_match = re.search(r'ì§ˆë¬¸:\s*(.+?)(?=ë‹µë³€:|$)', text, re.DOTALL)
            # ë‹µë³€ ì¶”ì¶œ
            answer_match = re.search(r'ë‹µë³€:\s*(.+?)(?=$)', text, re.DOTALL)
            
            if question_match and answer_match:
                question = question_match.group(1).strip()
                answer = answer_match.group(1).strip()
                
                if len(question) > 5 and len(answer) > 10:
                    return {
                        "question": question,
                        "answer": answer,
                        "domain": domain,
                        "topic": topic,
                        "generated_at": time.time()
                    }
            
            return None
            
        except Exception as e:
            logger.error(f"Q&A íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

class FlyWheelMetricsCollector:
    """í”Œë¼ì´íœ  ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.metrics = {
            "keyword_performance": [],
            "weight_performance": [],
            "threshold_performance": [],
            "overall_performance": []
        }
        
        # Gemma ê¸°ë°˜ Q&A ìƒì„±ê¸° ì¶”ê°€
        self.qa_generator = LocalGemmaQAGenerator()
    
    def record_query_performance(self, query: str, result: Dict[str, Any], 
                               user_feedback: float = None):
        """ì¿¼ë¦¬ ì„±ëŠ¥ ê¸°ë¡"""
        metric_record = {
            "timestamp": time.time(),
            "query": query,
            "confidence": result.get("confidence", 0.0),
            "sources_count": len(result.get("sources", [])),
            "answer_length": len(result.get("answer", "")),
            "user_feedback": user_feedback
        }
        
        self.metrics["overall_performance"].append(metric_record)
        
        # ë©”íŠ¸ë¦­ í¬ê¸° ì œí•œ
        for key in self.metrics:
            if len(self.metrics[key]) > 1000:
                self.metrics[key] = self.metrics[key][-800:]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        if not self.metrics["overall_performance"]:
            return {"message": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        recent_data = self.metrics["overall_performance"][-100:]
        
        summary = {
            "total_queries": len(self.metrics["overall_performance"]),
            "recent_avg_confidence": np.mean([r["confidence"] for r in recent_data]),
            "recent_avg_sources": np.mean([r["sources_count"] for r in recent_data]),
            "performance_trend": self._calculate_performance_trend()
        }
        
        return summary
    
    def _calculate_performance_trend(self) -> str:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(self.metrics["overall_performance"]) < 20:
            return "insufficient_data"
        
        recent_20 = self.metrics["overall_performance"][-20:]
        prev_20 = self.metrics["overall_performance"][-40:-20]
        
        recent_conf = np.mean([r["confidence"] for r in recent_20])
        prev_conf = np.mean([r["confidence"] for r in prev_20])
        
        if recent_conf > prev_conf + 0.05:
            return "improving"
        elif recent_conf < prev_conf - 0.05:
            return "declining"
        else:
            return "stable"


class ABTestManager:
    """A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì - ê¸°ì¡´ ì‹œìŠ¤í…œ vs ì ì‘í˜• ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ"""
    
    def __init__(self):
        self.test_data = {
            "variant_A": {"queries": [], "results": []},  # ê¸°ì¡´ ì‹œìŠ¤í…œ
            "variant_B": {"queries": [], "results": []}   # ì ì‘í˜• ì‹œìŠ¤í…œ
        }
        self.user_sessions = {}  # ì‚¬ìš©ìë³„ í• ë‹¹ëœ variant ì¶”ì 
        
        # A/B í…ŒìŠ¤íŠ¸ ì„¤ì • ë¡œë“œ
        try:
            from config import Config
            self.enable_ab_test = Config.ENABLE_AB_TESTING
            self.test_ratio = Config.AB_TEST_RATIO
            self.variants = Config.AB_TEST_VARIANTS
            logger.info(f"âœ… A/B í…ŒìŠ¤íŠ¸ í™œì„±í™”: {self.enable_ab_test}")
        except Exception as e:
            logger.warning(f"A/B í…ŒìŠ¤íŠ¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.enable_ab_test = False
    
    def assign_user_variant(self, user_id: str = None) -> str:
        """ì‚¬ìš©ìì—ê²Œ A/B í…ŒìŠ¤íŠ¸ variant í• ë‹¹"""
        if not self.enable_ab_test:
            return "B"  # A/B í…ŒìŠ¤íŠ¸ ë¹„í™œì„±í™”ì‹œ ì ì‘í˜• ì‚¬ìš©
        
        if user_id is None:
            user_id = str(uuid.uuid4())[:8]
        
        if user_id in self.user_sessions:
            return self.user_sessions[user_id]
        
        # 50:50 ë¶„í• 
        variant = "A" if random.random() < self.test_ratio else "B"
        self.user_sessions[user_id] = variant
        
        variant_name = self.variants.get(variant, {}).get("name", variant)
        logger.debug(f"ì‚¬ìš©ì {user_id} â†’ Variant {variant} ({variant_name})")
        return variant
    
    def record_test_result(self, user_id: str, query: str, variant: str, 
                          result: Dict[str, Any], user_feedback: float = None):
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡"""
        test_record = {
            "timestamp": time.time(),
            "user_id": user_id,
            "query": query,
            "variant": variant,
            "confidence": result.get("confidence", 0.0),
            "response_time": result.get("response_time", 0.0),
            "domain": result.get("domain", "unknown"),
            "answer_length": len(result.get("answer", "")),
            "sources_count": len(result.get("sources", [])),
            "user_feedback": user_feedback
        }
        
        if variant in self.test_data:
            self.test_data[variant]["results"].append(test_record)
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if len(self.test_data[variant]["results"]) > 1000:
                self.test_data[variant]["results"] = self.test_data[variant]["results"][-800:]
    
    def get_ab_test_report(self) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë³´ê³ ì„œ"""
        if not self.enable_ab_test:
            return {"status": "disabled"}
        
        variant_stats = {}
        
        for variant, data in self.test_data.items():
            if not data["results"]:
                variant_stats[variant] = {"status": "no_data"}
                continue
            
            results = data["results"]
            recent_results = results[-100:] if len(results) > 100 else results
            
            variant_stats[variant] = {
                "name": self.variants.get(variant, {}).get("name", variant),
                "total_queries": len(results),
                "avg_confidence": np.mean([r["confidence"] for r in recent_results]),
                "avg_response_time": np.mean([r["response_time"] for r in recent_results]),
                "user_satisfaction": self._calculate_user_satisfaction(recent_results)
            }
        
        # ìŠ¹ì ê²°ì •
        winner = self._determine_winner(variant_stats)
        
        return {
            "status": "active",
            "total_users": len(self.user_sessions),
            "variant_stats": variant_stats,
            "winner": winner,
            "recommendations": self._generate_recommendations(winner)
        }
    
    def _calculate_user_satisfaction(self, results: List[Dict]) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ ê³„ì‚°"""
        feedback_results = [r for r in results if r["user_feedback"] is not None]
        if not feedback_results:
            return 0.0
        return np.mean([r["user_feedback"] for r in feedback_results])
    
    def _determine_winner(self, variant_stats: Dict) -> Dict[str, Any]:
        """ìŠ¹ì ê²°ì •"""
        if "variant_A" not in variant_stats or "variant_B" not in variant_stats:
            return {"winner": None, "reason": "insufficient_data"}
        
        stats_a = variant_stats["variant_A"]
        stats_b = variant_stats["variant_B"]
        
        if stats_a.get("status") == "no_data" or stats_b.get("status") == "no_data":
            return {"winner": None, "reason": "no_data"}
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        score_a = (stats_a["avg_confidence"] * 0.5 + 
                  stats_a["user_satisfaction"] * 0.3 +
                  (1/max(stats_a["avg_response_time"], 0.1)) * 0.2)
        
        score_b = (stats_b["avg_confidence"] * 0.5 + 
                  stats_b["user_satisfaction"] * 0.3 +
                  (1/max(stats_b["avg_response_time"], 0.1)) * 0.2)
        
        # ìµœì†Œ 30ê°œ ìƒ˜í”Œ í•„ìš”
        if stats_a["total_queries"] < 30 or stats_b["total_queries"] < 30:
            return {"winner": None, "reason": "insufficient_samples"}
        
        improvement_threshold = 0.05
        if score_b > score_a + improvement_threshold:
            return {"winner": "B", "improvement": f"{((score_b-score_a)/score_a*100):.1f}%"}
        elif score_a > score_b + improvement_threshold:
            return {"winner": "A", "improvement": f"{((score_a-score_b)/score_b*100):.1f}%"}
        else:
            return {"winner": "tie", "reason": "no_significant_difference"}
    
    def _generate_recommendations(self, winner: Dict) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        if winner.get("winner") == "B":
            recommendations.append("âœ… ì ì‘í˜• ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜ ê¶Œì¥")
            recommendations.append("ğŸ’¡ í”Œë¼ì´íœ  ì›Œí¬í”Œë¡œìš° íš¨ê³¼ í™•ì¸ë¨")
        elif winner.get("winner") == "A":
            recommendations.append("âš ï¸ ì ì‘í˜• ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”")
        else:
            recommendations.append("ğŸ“Š ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”")
        
        return recommendations 